"""
æ¸¬è©¦è…³æœ¬ï¼šåˆ‡æ–·ã€è²¼æ¨™ã€è½‰å‘é‡æµç¨‹
æ•´åˆæ–‡æœ¬æ¸…ç†åŠŸèƒ½ï¼Œè™•ç†è¡¨æƒ…ç¬¦è™Ÿå’Œç‰¹æ®Šå­—å…ƒ
"""

import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
from datetime import datetime

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

from vector_pipeline.core import (
    MongoDBProcessor, MongoDocument,
    TextChunker, TextChunk,
    VectorProcessor,
    MilvusWriter
)
from vector_pipeline.utils.text_cleaner import TextCleaner
from vector_pipeline.pipeline_orchestrator import TagProcessorManager

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_chunk_tag_vector.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ChunkTagVectorTester:
    """åˆ‡æ–·ã€è²¼æ¨™ã€è½‰å‘é‡æ¸¬è©¦å™¨"""
    
    def __init__(self):
        # é…ç½®
        self.mongo_config = {
            'host': 'localhost',
            'port': 27017,
            'database': 'podwise',
            'collection': 'podcasts'
        }
        
        self.milvus_config = {
            'host': 'localhost',
            'port': 19530,
            'collection_name': 'podcast_chunks_test',
            'dim': 1024
        }
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.text_cleaner = TextCleaner()
        self.mongo_processor = MongoDBProcessor(self.mongo_config)
        self.text_chunker = TextChunker(max_chunk_size=1024, overlap=100)
        self.vector_processor = VectorProcessor(model_name="BAAI/bge-m3")
        self.milvus_writer = MilvusWriter(self.milvus_config)
        self.tag_manager = TagProcessorManager("TAG_info.csv")
        
        logger.info("æ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def test_text_cleaning(self, sample_texts: List[str]) -> None:
        """æ¸¬è©¦æ–‡æœ¬æ¸…ç†åŠŸèƒ½"""
        logger.info("=== æ¸¬è©¦æ–‡æœ¬æ¸…ç†åŠŸèƒ½ ===")
        
        for i, text in enumerate(sample_texts):
            logger.info(f"\nåŸå§‹æ–‡æœ¬ {i+1}: {text}")
            
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self.text_cleaner.clean_text(
                text, 
                remove_emojis=True,
                normalize_unicode=True,
                remove_special_chars=False
            )
            
            logger.info(f"æ¸…ç†å¾Œæ–‡æœ¬: {cleaned_text}")
            
            # æ¸¬è©¦æ¨™é¡Œæ¨™æº–åŒ–
            normalized_title = self.text_cleaner.normalize_title(text)
            logger.info(f"æ¨™æº–åŒ–æ¨™é¡Œ: {normalized_title}")
            
            # æ¸¬è©¦æœç´¢è®Šé«”
            search_variants = self.text_cleaner.create_search_variants(text)
            logger.info(f"æœç´¢è®Šé«”: {search_variants}")
    
    def test_chunking(self, long_text: str) -> List[TextChunk]:
        """æ¸¬è©¦æ–‡æœ¬åˆ‡åˆ†"""
        logger.info("=== æ¸¬è©¦æ–‡æœ¬åˆ‡åˆ† ===")
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self.text_cleaner.clean_text(long_text)
        logger.info(f"æ¸…ç†å¾Œæ–‡æœ¬é•·åº¦: {len(cleaned_text)}")
        
        # åˆ‡åˆ†æ–‡æœ¬
        chunks = self.text_chunker.chunk_text(cleaned_text)
        logger.info(f"åˆ‡åˆ†å‡º {len(chunks)} å€‹æ–‡æœ¬å¡Š")
        
        for i, chunk in enumerate(chunks):
            logger.info(f"æ–‡æœ¬å¡Š {i+1}: é•·åº¦={len(chunk.text)}, å…§å®¹é è¦½={chunk.text[:100]}...")
        
        return chunks
    
    def test_tagging(self, chunks: List[TextChunk]) -> List[Dict[str, Any]]:
        """æ¸¬è©¦æ¨™ç±¤æå–"""
        logger.info("=== æ¸¬è©¦æ¨™ç±¤æå– ===")
        
        tagged_chunks = []
        
        for i, chunk in enumerate(chunks):
            logger.info(f"\nè™•ç†æ–‡æœ¬å¡Š {i+1}:")
            logger.info(f"æ–‡æœ¬é è¦½: {chunk.text[:200]}...")
            
            # æå–æ¨™ç±¤
            tags = self.tag_manager.extract_tags(chunk.text)
            logger.info(f"æå–æ¨™ç±¤: {tags}")
            
            tagged_chunks.append({
                'chunk_index': i,
                'chunk_text': chunk.text,
                'tags': tags
            })
        
        return tagged_chunks
    
    def test_vectorization(self, tagged_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ¸¬è©¦å‘é‡åŒ–"""
        logger.info("=== æ¸¬è©¦å‘é‡åŒ– ===")
        
        vectorized_chunks = []
        
        for chunk_data in tagged_chunks:
            logger.info(f"\nå‘é‡åŒ–æ–‡æœ¬å¡Š {chunk_data['chunk_index'] + 1}:")
            
            # ç”Ÿæˆæ–‡æœ¬å‘é‡
            text_embedding = self.vector_processor.generate_embedding(chunk_data['chunk_text'])
            logger.info(f"æ–‡æœ¬å‘é‡ç¶­åº¦: {len(text_embedding)}")
            
            # ç”Ÿæˆæ¨™ç±¤å‘é‡
            tag_embeddings = []
            for tag in chunk_data['tags']:
                tag_embedding = self.vector_processor.generate_embedding(tag)
                tag_embeddings.append(tag_embedding)
            
            logger.info(f"æ¨™ç±¤å‘é‡æ•¸é‡: {len(tag_embeddings)}")
            
            vectorized_chunks.append({
                **chunk_data,
                'text_embedding': text_embedding,
                'tag_embeddings': tag_embeddings
            })
        
        return vectorized_chunks
    
    def test_milvus_storage(self, vectorized_chunks: List[Dict[str, Any]]) -> None:
        """æ¸¬è©¦ Milvus å„²å­˜"""
        logger.info("=== æ¸¬è©¦ Milvus å„²å­˜ ===")
        
        try:
            # æº–å‚™å„²å­˜è³‡æ–™
            storage_data = []
            
            for chunk_data in vectorized_chunks:
                # æº–å‚™æ¨™ç±¤å‘é‡ï¼ˆç¢ºä¿æœ‰3å€‹ï¼‰
                tag_vectors = chunk_data['tag_embeddings']
                while len(tag_vectors) < 3:
                    tag_vectors.append(chunk_data['text_embedding'])  # ç”¨æ–‡æœ¬å‘é‡å¡«å……
                
                storage_data.append({
                    'chunk_id': f"test_chunk_{chunk_data['chunk_index']}",
                    'chunk_index': chunk_data['chunk_index'],
                    'chunk_text': chunk_data['chunk_text'],
                    'chunk_length': len(chunk_data['chunk_text']),
                    'episode_id': 999,  # æ¸¬è©¦ç”¨
                    'podcast_id': 999,  # æ¸¬è©¦ç”¨
                    'episode_title': 'æ¸¬è©¦ç¯€ç›®',
                    'podcast_name': 'æ¸¬è©¦æ’­å®¢',
                    'author': 'æ¸¬è©¦ä½œè€…',
                    'category': 'æ¸¬è©¦åˆ†é¡',
                    'created_at': datetime.now().isoformat(),
                    'source_model': 'test_model',
                    'language': 'zh-TW',
                    'embedding': chunk_data['text_embedding'],
                    'tag_1': tag_vectors[0],
                    'tag_2': tag_vectors[1],
                    'tag_3': tag_vectors[2],
                    'tags': chunk_data['tags']
                })
            
            # å¯«å…¥ Milvus
            success_count = self.milvus_writer.write_chunks(storage_data)
            logger.info(f"æˆåŠŸå„²å­˜ {success_count} å€‹æ–‡æœ¬å¡Šåˆ° Milvus")
            
        except Exception as e:
            logger.error(f"Milvus å„²å­˜å¤±æ•—: {e}")
    
    def test_mongo_document_processing(self, limit: int = 3) -> None:
        """æ¸¬è©¦ MongoDB æ–‡ä»¶è™•ç†"""
        logger.info("=== æ¸¬è©¦ MongoDB æ–‡ä»¶è™•ç† ===")
        
        try:
            # ç²å–æ¸¬è©¦æ–‡ä»¶
            documents = self.mongo_processor.get_documents(limit=limit)
            logger.info(f"ç²å–åˆ° {len(documents)} å€‹æ¸¬è©¦æ–‡ä»¶")
            
            for i, doc in enumerate(documents):
                logger.info(f"\nè™•ç†æ–‡ä»¶ {i+1}:")
                logger.info(f"æ¨™é¡Œ: {doc.title}")
                logger.info(f"å…§å®¹é•·åº¦: {len(doc.content)}")
                
                # æ¸…ç†æ¨™é¡Œ
                cleaned_title = self.text_cleaner.clean_text(doc.title)
                logger.info(f"æ¸…ç†å¾Œæ¨™é¡Œ: {cleaned_title}")
                
                # æ¸…ç†å…§å®¹
                cleaned_content = self.text_cleaner.clean_text(doc.content)
                logger.info(f"æ¸…ç†å¾Œå…§å®¹é•·åº¦: {len(cleaned_content)}")
                
                # åˆ‡åˆ†
                chunks = self.text_chunker.chunk_text(cleaned_content)
                logger.info(f"åˆ‡åˆ†å‡º {len(chunks)} å€‹æ–‡æœ¬å¡Š")
                
                # æ¨™ç±¤æå–
                for j, chunk in enumerate(chunks[:2]):  # åªè™•ç†å‰2å€‹å¡Š
                    tags = self.tag_manager.extract_tags(chunk.text)
                    logger.info(f"æ–‡æœ¬å¡Š {j+1} æ¨™ç±¤: {tags}")
                
                break  # åªè™•ç†ç¬¬ä¸€å€‹æ–‡ä»¶ä½œç‚ºæ¸¬è©¦
        
        except Exception as e:
            logger.error(f"MongoDB è™•ç†å¤±æ•—: {e}")
    
    def run_full_test(self) -> None:
        """åŸ·è¡Œå®Œæ•´æ¸¬è©¦"""
        logger.info("é–‹å§‹åŸ·è¡Œå®Œæ•´æ¸¬è©¦æµç¨‹")
        
        # æ¸¬è©¦æ–‡æœ¬æ¸…ç†
        sample_texts = [
            "ğŸ§ ç§‘æŠ€æ–°çŸ¥ï¼šAI äººå·¥æ™ºæ…§æœ€æ–°ç™¼å±• ğŸ”¥",
            "å•†æ¥­ç®¡ç†ï¼šä¼æ¥­ç¶“ç‡Ÿç­–ç•¥èˆ‡é ˜å°åŠ› ğŸ’¼",
            "æ•™è‚²å­¸ç¿’ï¼šå¦‚ä½•æå‡å­¸ç¿’æ•ˆç‡ ğŸ“š",
            "å‰µæ¥­æ•…äº‹ï¼šå¾é›¶åˆ°ä¸€çš„å‰µæ¥­æ­·ç¨‹ ğŸš€",
            "å¥åº·ç”Ÿæ´»ï¼šç‡Ÿé¤Šèˆ‡é‹å‹•çš„å¹³è¡¡ âš–ï¸"
        ]
        self.test_text_cleaning(sample_texts)
        
        # æ¸¬è©¦é•·æ–‡æœ¬åˆ‡åˆ†
        long_text = """
        äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä¼åœ–äº†è§£æ™ºèƒ½çš„å¯¦è³ªï¼Œ
        ä¸¦ç”Ÿç”¢å‡ºä¸€ç¨®æ–°çš„èƒ½ä»¥äººé¡æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºåæ‡‰çš„æ™ºèƒ½æ©Ÿå™¨ã€‚è©²é ˜åŸŸçš„ç ”ç©¶åŒ…æ‹¬æ©Ÿå™¨äººã€
        èªè¨€è­˜åˆ¥ã€åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†å’Œå°ˆå®¶ç³»çµ±ç­‰ã€‚äººå·¥æ™ºæ…§å¾èª•ç”Ÿä»¥ä¾†ï¼Œç†è«–å’ŒæŠ€è¡“æ—¥ç›Šæˆç†Ÿï¼Œ
        æ‡‰ç”¨é ˜åŸŸä¹Ÿä¸æ–·æ“´å¤§ï¼Œå¯ä»¥è¨­æƒ³ï¼Œæœªä¾†äººå·¥æ™ºæ…§å¸¶ä¾†çš„ç§‘æŠ€ç”¢å“ï¼Œå°‡æœƒæ˜¯äººé¡æ™ºæ…§çš„ã€Œå®¹å™¨ã€ã€‚
        
        äººå·¥æ™ºæ…§å¯ä»¥å°äººçš„æ„è­˜ã€æ€ç¶­çš„ä¿¡æ¯éç¨‹çš„æ¨¡æ“¬ã€‚äººå·¥æ™ºæ…§ä¸æ˜¯äººçš„æ™ºèƒ½ï¼Œä½†èƒ½åƒäººé‚£æ¨£æ€è€ƒã€
        ä¹Ÿå¯èƒ½è¶…éäººçš„æ™ºèƒ½ã€‚äººå·¥æ™ºæ…§æ˜¯ä¸€é–€æ¥µå¯ŒæŒ‘æˆ°æ€§çš„ç§‘å­¸ï¼Œå¾äº‹äººå·¥æ™ºæ…§å·¥ä½œçš„äººå¿…é ˆæ‡‚å¾—é›»è…¦çŸ¥è­˜ï¼Œ
        å¿ƒç†å­¸å’Œå“²å­¸ã€‚äººå·¥æ™ºæ…§æ˜¯åŒ…æ‹¬ååˆ†å»£æ³›çš„ç§‘å­¸ï¼Œå®ƒç”±ä¸åŒçš„é ˜åŸŸçµ„æˆï¼Œå¦‚æ©Ÿå™¨å­¸ç¿’ï¼Œé›»è…¦è¦–è¦ºç­‰ç­‰ï¼Œ
        ç¸½çš„èªªä¾†ï¼Œäººå·¥æ™ºæ…§ç ”ç©¶çš„ä¸€å€‹ä¸»è¦ç›®æ¨™æ˜¯ä½¿æ©Ÿå™¨èƒ½å¤ å‹ä»»ä¸€äº›é€šå¸¸éœ€è¦äººé¡æ™ºèƒ½æ‰èƒ½å®Œæˆçš„è¤‡é›œå·¥ä½œã€‚
        
        åœ¨å•†æ¥­æ‡‰ç”¨æ–¹é¢ï¼Œäººå·¥æ™ºæ…§å·²ç¶“åœ¨è¨±å¤šé ˜åŸŸç™¼æ®é‡è¦ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨å®¢æˆ¶æœå‹™ä¸­ï¼ŒèŠå¤©æ©Ÿå™¨äººå¯ä»¥
        24å°æ™‚ä¸é–“æ–·åœ°ç‚ºå®¢æˆ¶æä¾›æœå‹™ï¼›åœ¨é‡‘èé ˜åŸŸï¼ŒAIå¯ä»¥å¹«åŠ©é€²è¡Œé¢¨éšªè©•ä¼°å’ŒæŠ•è³‡æ±ºç­–ï¼›åœ¨é†«ç™‚é ˜åŸŸï¼Œ
        AIå¯ä»¥å”åŠ©é†«ç”Ÿé€²è¡Œç–¾ç—…è¨ºæ–·å’Œæ²»ç™‚æ–¹æ¡ˆåˆ¶å®šã€‚é€™äº›æ‡‰ç”¨ä¸åƒ…æé«˜äº†æ•ˆç‡ï¼Œä¹Ÿç‚ºä¼æ¥­å‰µé€ äº†æ–°çš„åƒ¹å€¼ã€‚
        """
        
        chunks = self.test_chunking(long_text)
        tagged_chunks = self.test_tagging(chunks)
        vectorized_chunks = self.test_vectorization(tagged_chunks)
        self.test_milvus_storage(vectorized_chunks)
        
        # æ¸¬è©¦ MongoDB æ–‡ä»¶è™•ç†
        self.test_mongo_document_processing()
        
        logger.info("å®Œæ•´æ¸¬è©¦æµç¨‹å®Œæˆ")
    
    def cleanup(self) -> None:
        """æ¸…ç†è³‡æº"""
        try:
            self.mongo_processor.close()
            self.milvus_writer.close()
            logger.info("è³‡æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†è³‡æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    tester = ChunkTagVectorTester()
    
    try:
        tester.run_full_test()
    except Exception as e:
        logger.error(f"æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        tester.cleanup()


if __name__ == "__main__":
    main() 