"""
åµŒå…¥æœå‹™
æ•´åˆæ‰€æœ‰å‘é‡åŒ–å’Œ Milvus æ“ä½œåŠŸèƒ½
ç¬¦åˆ Google Clean Code åŸå‰‡
"""

import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import numpy as np

from pymilvus import connections, Collection, utility
from ..core.vector_processor import VectorProcessor
from ..config.settings import config

logger = logging.getLogger(__name__)


class EmbeddingService:
    """åµŒå…¥æœå‹™ - æ•´åˆå‘é‡åŒ–å’Œ Milvus æ“ä½œ"""
    
    def __init__(self, milvus_config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åµŒå…¥æœå‹™
        
        Args:
            milvus_config: Milvus é…ç½®ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é è¨­é…ç½®
        """
        self.milvus_config = milvus_config or {
            'host': config.milvus_host,
            'port': config.milvus_port
        }
        self.collection_name = config.collection_name
        self.vector_processor = VectorProcessor(
            embedding_model=config.embedding_model
        )
        self.connected = False
        
    def connect_milvus(self) -> bool:
        """é€£ç·šåˆ° Milvus"""
        try:
            connections.connect(
                alias="default",
                host=self.milvus_config['host'],
                port=self.milvus_config['port']
            )
            self.connected = True
            logger.info(f"âœ… æˆåŠŸé€£ç·šåˆ° Milvus: {self.milvus_config['host']}:{self.milvus_config['port']}")
            return True
        except Exception as e:
            logger.error(f"âŒ Milvus é€£ç·šå¤±æ•—: {e}")
            return False
    
    def disconnect_milvus(self) -> None:
        """æ–·é–‹ Milvus é€£ç·š"""
        if self.connected:
            connections.disconnect("default")
            self.connected = False
            logger.info("ğŸ”Œ Milvus é€£ç·šå·²æ–·é–‹")
    
    def check_collection_exists(self) -> bool:
        """æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨"""
        try:
            return utility.has_collection(self.collection_name)
        except Exception as e:
            logger.error(f"æª¢æŸ¥é›†åˆå­˜åœ¨æ€§å¤±æ•—: {e}")
            return False
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ç²å–é›†åˆçµ±è¨ˆè³‡è¨Š"""
        try:
            if not self.check_collection_exists():
                return {"error": "é›†åˆä¸å­˜åœ¨"}
            
            collection = Collection(self.collection_name)
            collection.load()
            
            stats = {
                "collection_name": self.collection_name,
                "num_entities": collection.num_entities,
                "schema": collection.schema.to_dict()
            }
            
            collection.release()
            return stats
            
        except Exception as e:
            logger.error(f"ç²å–é›†åˆçµ±è¨ˆå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def embed_stage3_data(self, stage3_dir: str = None) -> Dict[str, Any]:
        """
        å°‡ stage3_tagging è³‡æ–™åµŒå…¥åˆ° Milvus
        
        Args:
            stage3_dir: stage3 ç›®éŒ„è·¯å¾‘
            
        Returns:
            åµŒå…¥çµæœçµ±è¨ˆ
        """
        stage3_path = Path(stage3_dir or config.stage3_dir)
        
        if not stage3_path.exists():
            return {"error": f"ç›®éŒ„ä¸å­˜åœ¨: {stage3_path}"}
        
        # é€£ç·šåˆ° Milvus
        if not self.connect_milvus():
            return {"error": "Milvus é€£ç·šå¤±æ•—"}
        
        try:
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if not self.check_collection_exists():
                return {"error": "Milvus é›†åˆä¸å­˜åœ¨ï¼Œè«‹å…ˆå‰µå»ºé›†åˆ"}
            
            # è¼‰å…¥é›†åˆ
            collection = Collection(self.collection_name)
            collection.load()
            
            # è™•ç†æ‰€æœ‰æª”æ¡ˆ
            results = self._process_stage3_files(stage3_path, collection)
            
            # é‡‹æ”¾é›†åˆ
            collection.release()
            
            return results
            
        except Exception as e:
            logger.error(f"åµŒå…¥éç¨‹å¤±æ•—: {e}")
            return {"error": str(e)}
        finally:
            self.disconnect_milvus()
    
    def _process_stage3_files(self, stage3_path: Path, collection: Collection) -> Dict[str, Any]:
        """è™•ç† stage3 æª”æ¡ˆ"""
        total_files = 0
        successful_files = 0
        failed_files = 0
        total_chunks = 0
        failed_files_list = []
        
        # éæ­·æ‰€æœ‰å­è³‡æ–™å¤¾
        for subfolder in stage3_path.iterdir():
            if not subfolder.is_dir():
                continue
                
            logger.info(f"è™•ç†è³‡æ–™å¤¾: {subfolder.name}")
            
            # è™•ç†è©²è³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰ JSON æª”æ¡ˆ
            json_files = list(subfolder.glob("*.json"))
            
            for json_file in json_files:
                total_files += 1
                
                try:
                    # è®€å–æª”æ¡ˆ
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # è™•ç† chunks
                    chunks = data.get('chunks', [])
                    if not chunks:
                        failed_files += 1
                        failed_files_list.append(str(json_file))
                        continue
                    
                    # éæ¿¾æœ‰æ•ˆçš„ chunks
                    valid_chunks = []
                    for chunk in chunks:
                        chunk_text = chunk.get('chunk_text', '')
                        if chunk_text and chunk_text.strip() != '' and chunk_text.strip() != '!':
                            valid_chunks.append(chunk)
                    
                    if not valid_chunks:
                        failed_files += 1
                        failed_files_list.append(str(json_file))
                        continue
                    
                    # ç”ŸæˆåµŒå…¥å‘é‡
                    texts = [chunk['chunk_text'] for chunk in valid_chunks]
                    embeddings = self.vector_processor.generate_embeddings(texts)
                    
                    # æº–å‚™æ’å…¥è³‡æ–™
                    insert_data = self._prepare_insert_data(data, valid_chunks, embeddings)
                    
                    # æ’å…¥åˆ° Milvus
                    collection.insert(insert_data)
                    
                    successful_files += 1
                    total_chunks += len(valid_chunks)
                    
                    if total_files % 100 == 0:
                        logger.info(f"å·²è™•ç† {total_files} å€‹æª”æ¡ˆï¼ŒæˆåŠŸ {successful_files} å€‹")
                    
                except Exception as e:
                    logger.error(f"è™•ç†æª”æ¡ˆå¤±æ•— {json_file}: {e}")
                    failed_files += 1
                    failed_files_list.append(str(json_file))
        
        return {
            "total_files": total_files,
            "successful_files": successful_files,
            "failed_files": failed_files,
            "total_chunks": total_chunks,
            "failed_files_list": failed_files_list,
            "success_rate": successful_files / total_files * 100 if total_files > 0 else 0
        }
    
    def _prepare_insert_data(self, data: Dict[str, Any], chunks: List[Dict], 
                           embeddings: np.ndarray) -> Dict[str, List]:
        """æº–å‚™æ’å…¥è³‡æ–™"""
        insert_data = {
            "chunk_id": [],
            "chunk_index": [],
            "episode_id": [],
            "podcast_id": [],
            "podcast_name": [],
            "author": [],
            "category": [],
            "episode_title": [],
            "duration": [],
            "published_date": [],
            "apple_rating": [],
            "chunk_text": [],
            "embedding": [],
            "language": [],
            "created_at": [],
            "source_model": [],
            "tag": []
        }
        
        # ç²å– episode è³‡è¨Š
        episode_info = data.get('episode_info', {})
        
        for i, chunk in enumerate(chunks):
            insert_data["chunk_id"].append(chunk.get('chunk_id', ''))
            insert_data["chunk_index"].append(chunk.get('chunk_index', 0))
            insert_data["episode_id"].append(episode_info.get('episode_id', 0))
            insert_data["podcast_id"].append(episode_info.get('podcast_id', 0))
            insert_data["podcast_name"].append(episode_info.get('podcast_name', ''))
            insert_data["author"].append(episode_info.get('author', ''))
            insert_data["category"].append(episode_info.get('category', ''))
            insert_data["episode_title"].append(episode_info.get('episode_title', ''))
            insert_data["duration"].append(episode_info.get('duration', ''))
            insert_data["published_date"].append(episode_info.get('published_date', ''))
            insert_data["apple_rating"].append(episode_info.get('apple_rating', 0))
            insert_data["chunk_text"].append(chunk.get('chunk_text', ''))
            insert_data["embedding"].append(embeddings[i].tolist())
            insert_data["language"].append(episode_info.get('language', 'zh'))
            insert_data["created_at"].append(datetime.now().isoformat())
            insert_data["source_model"].append(config.embedding_model)
            insert_data["tag"].append(json.dumps(chunk.get('enhanced_tags', []), ensure_ascii=False))
        
        return insert_data
    
    def search_similar_chunks(self, query_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        æœå°‹ç›¸ä¼¼çš„ chunks
        
        Args:
            query_text: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›çµæœæ•¸é‡
            
        Returns:
            ç›¸ä¼¼ chunks åˆ—è¡¨
        """
        try:
            if not self.connect_milvus():
                return []
            
            # è¼‰å…¥é›†åˆ
            collection = Collection(self.collection_name)
            collection.load()
            
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self.vector_processor.generate_single_embedding(query_text)
            
            # åŸ·è¡Œæœå°‹
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": 10}
            }
            
            results = collection.search(
                data=[query_embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["chunk_id", "chunk_text", "episode_title", "podcast_name", "tag"]
            )
            
            # è™•ç†çµæœ
            search_results = []
            if results and len(results) > 0:
                hits = results[0]
                for hit in hits:
                    search_results.append({
                        "score": hit.score,
                        "chunk_id": hit.entity.get('chunk_id'),
                        "chunk_text": hit.entity.get('chunk_text'),
                        "episode_title": hit.entity.get('episode_title'),
                        "podcast_name": hit.entity.get('podcast_name'),
                        "tags": json.loads(hit.entity.get('tag', '[]'))
                    })
            
            collection.release()
            return search_results
            
        except Exception as e:
            logger.error(f"æœå°‹å¤±æ•—: {e}")
            return []
        finally:
            self.disconnect_milvus() 