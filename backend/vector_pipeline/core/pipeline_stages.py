#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡è™•ç†ç®¡ç·šçš„å››å€‹éšæ®µè™•ç†å™¨

Author: Podri Team
License: MIT
"""

import json
import logging
import uuid
import re
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime

import pymongo
from pymongo import MongoClient
from pymongo.database import Database
from pymongo.collection import Collection

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¼•ç”¨ data_cleaning çš„æª”åæ¸…ç†åŠŸèƒ½
try:
    from data_cleaning.core.mongo_cleaner import MongoCleaner
    mongo_cleaner = MongoCleaner()
    def clean_filename(filename: str) -> str:
        """ä½¿ç”¨ data_cleaning çš„æª”åæ¸…ç†åŠŸèƒ½"""
        return mongo_cleaner._clean_filename(filename)
except ImportError:
    logger.warning("ç„¡æ³•è¼‰å…¥ data_cleaning æ¨¡çµ„ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆæª”åæ¸…ç†")
    def clean_filename(filename: str) -> str:
        """ç°¡åŒ–ç‰ˆæª”åæ¸…ç†ï¼ˆfallbackï¼‰"""
        if not filename:
            return ""
        # ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿå’Œç‰¹æ®Šå­—å…ƒï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€åº•ç·šã€é€£å­—è™Ÿã€é»è™Ÿ
        cleaned = re.sub(r'[^\u4e00-\u9fff\w\s\-_\.]', '', filename)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        return cleaned


@dataclass
class ChunkData:
    """Chunk è³‡æ–™çµæ§‹"""
    chunk_id: str
    episode_id: str
    chunk_index: int
    content: str
    original_filename: str
    collection_name: str
    created_at: str


@dataclass
class ProcessedChunkData(ChunkData):
    """è™•ç†å¾Œçš„ Chunk è³‡æ–™çµæ§‹"""
    cleaned_content: str
    tags: List[str]
    tag_sources: Dict[str, List[str]]  # æ¨™ç±¤ä¾†æºåˆ†é¡


class BaseStageProcessor(ABC):
    """åŸºç¤éšæ®µè™•ç†å™¨"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.stage_name = self.__class__.__name__
    
    @abstractmethod
    def process(self, input_data: Any) -> Any:
        """è™•ç†è³‡æ–™çš„æŠ½è±¡æ–¹æ³•"""
        pass
    
    def save_results(self, results: Any, filename: str) -> str:
        """å„²å­˜çµæœåˆ°æª”æ¡ˆ"""
        output_path = self.data_dir / filename
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"{self.stage_name} çµæœå·²å„²å­˜åˆ°: {output_path}")
        return str(output_path)
    
    def load_results(self, filename: str) -> Any:
        """å¾æª”æ¡ˆè¼‰å…¥çµæœ"""
        input_path = self.data_dir / filename
        if not input_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {input_path}")
        
        with open(input_path, 'r', encoding='utf-8') as f:
            return json.load(f)


class Stage1TextChunker(BaseStageProcessor):
    """éšæ®µ 1ï¼šæ–‡æœ¬åˆ‡æ–·è™•ç†å™¨"""
    
    def __init__(self, output_dir: str = "data/stage1_chunking"):
        super().__init__(output_dir)
        self.stage_name = "æ–‡æœ¬åˆ‡æ–·"
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """å°‡æ–‡æœ¬åˆ‡æ–·æˆ chunks - ä½¿ç”¨çµ±ä¸€çš„ TextChunker"""
        from .text_chunker import TextChunker
        
        if not text or not text.strip():
            return []
        
        # ä½¿ç”¨çµ±ä¸€çš„ TextChunker
        chunker = TextChunker(max_chunk_size=1024, overlap_size=100)
        text_chunks = chunker.split_text_into_chunks(text, "temp_doc_id")
        
        # è½‰æ›ç‚ºç°¡å–®çš„å­—ç¬¦ä¸²åˆ—è¡¨
        return [chunk.chunk_text for chunk in text_chunks]
    
    def process_document(self, doc: Dict[str, Any], collection_name: str) -> Dict[str, Any]:
        """è™•ç†å–®å€‹æ–‡ä»¶"""
        # ç²å–æª”å
        filename = self._get_filename(doc)
        
        # ç²å–æ–‡æœ¬å…§å®¹
        text_content = doc.get('text', '')
        if not text_content:
            logger.warning(f"æ–‡ä»¶ {doc.get('_id')} æ²’æœ‰å…§å®¹")
            return {}
        
        # åˆ‡æ–·æ–‡æœ¬
        text_chunks = self.split_text_into_chunks(text_content)
        
        if not text_chunks:
            logger.warning(f"æ–‡ä»¶ {doc.get('_id')} åˆ‡æ–·å¾Œæ²’æœ‰ chunks")
            return {}
        
        # å»ºç«‹ chunk è³‡æ–™
        chunk_data = []
        for index, chunk_text in enumerate(text_chunks):
            chunk_info = {
                'chunk_text': chunk_text,
                'chunk_id': str(uuid.uuid4()),  # å”¯ä¸€ UUID
                'chunk_index': index,  # è©²æ®µåœ¨è©² episode çš„é †åº
                'episode_id': str(doc.get('_id')),  # ä¿ç•™åŸå§‹ episode ID
                'original_filename': filename,
                'collection_name': collection_name,
                'chunk_length': len(chunk_text)
            }
            chunk_data.append(chunk_info)
        
        return {
            'filename': filename,
            'episode_id': str(doc.get('_id')),
            'collection_name': collection_name,
            'total_chunks': len(chunk_data),
            'chunks': chunk_data,
            'original_text_length': len(text_content)
        }
    
    def _get_filename(self, doc: Dict[str, Any]) -> str:
        """ç²å–æª”åï¼Œå„ªå…ˆé †åºï¼š_file_metadata.filename > file > _id"""
        # å˜—è©¦å¾ _file_metadata.filename ç²å–
        file_metadata = doc.get('_file_metadata', {})
        if isinstance(file_metadata, dict) and 'filename' in file_metadata:
            filename = file_metadata['filename']
            if filename:
                return clean_filename(filename)
        
        # å˜—è©¦å¾ file æ¬„ä½ç²å–
        file_field = doc.get('file', '')
        if file_field:
            # ç§»é™¤ .mp3 å‰¯æª”å
            filename = file_field.replace('.mp3', '')
            return clean_filename(filename)
        
        # æœ€å¾Œä½¿ç”¨ _id
        return str(doc.get('_id', ''))
    
    def process(self, mongodb_config: Dict[str, str]) -> List[Dict[str, Any]]:
        """è™•ç† MongoDB è³‡æ–™ - å¯¦ä½œ BaseStageProcessor æŠ½è±¡æ–¹æ³•"""
        logger.info("=== é–‹å§‹éšæ®µ 1: æ–‡æœ¬åˆ‡æ–·è™•ç† ===")
        
        # é€™è£¡éœ€è¦å¯¦ä½œ MongoDB é€£æ¥å’Œè™•ç†é‚è¼¯
        # æš«æ™‚è¿”å›ç©ºåˆ—è¡¨ï¼Œå¯¦éš›å¯¦ä½œéœ€è¦æ ¹æ“šå…·é«”éœ€æ±‚
        logger.warning("Stage1TextChunker.process æ–¹æ³•éœ€è¦å¯¦ä½œ MongoDB è™•ç†é‚è¼¯")
        return []
    
    def save_individual_file(self, result: Dict[str, Any], collection_dir: Path):
        """å„²å­˜å€‹åˆ¥æª”æ¡ˆ"""
        if not result or 'filename' not in result:
            return
        
        filename = result['filename']
        safe_filename = self._make_filename_safe(filename)
        output_file = collection_dir / f"{safe_filename}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²å„²å­˜: {output_file}")
        except Exception as e:
            logger.error(f"å„²å­˜æª”æ¡ˆå¤±æ•— {output_file}: {e}")
    
    def save_results(self, results: List[Dict[str, Any]], filename: str):
        """å„²å­˜çµæœ"""
        output_file = self.data_dir / filename
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"çµæœå·²å„²å­˜: {output_file}")
        except Exception as e:
            logger.error(f"å„²å­˜çµæœå¤±æ•— {output_file}: {e}")
    
    def _make_filename_safe(self, filename: str) -> str:
        """ç¢ºä¿æª”åå®‰å…¨"""
        # ç§»é™¤æˆ–æ›¿æ›ä¸å®‰å…¨çš„å­—ç¬¦
        safe_filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # é™åˆ¶é•·åº¦
        if len(safe_filename) > 200:
            safe_filename = safe_filename[:200]
        return safe_filename.strip()


class Stage2TextCleaningProcessor(BaseStageProcessor):
    """éšæ®µ 2: æ–‡æœ¬æ¸…ç†è™•ç†å™¨"""
    
    def __init__(self, data_dir: str = "data/stage2_text_cleaning"):
        super().__init__(data_dir)
        self.stage_name = "Stage2TextCleaning"
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # åŸºæœ¬æ¸…ç†
        cleaned = text.strip()
        
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
        cleaned = ' '.join(cleaned.split())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€æ¨™é»ç¬¦è™Ÿï¼‰
        import re
        cleaned = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()\[\]{}"\'-]', '', cleaned)
        
        return cleaned
    
    def process(self, chunks_data: List[Dict]) -> List[Dict]:
        """è™•ç†æ–‡æœ¬æ¸…ç†"""
        logger.info("=== é–‹å§‹éšæ®µ 2: æ–‡æœ¬æ¸…ç†è™•ç† ===")
        
        processed_chunks = []
        
        for chunk_data in chunks_data:
            cleaned_content = self.clean_text(chunk_data['content'])
            
            processed_chunk = {
                **chunk_data,
                'cleaned_content': cleaned_content,
                'cleaned_at': datetime.now().isoformat()
            }
            processed_chunks.append(processed_chunk)
        
        # å„²å­˜çµæœ
        self.save_results(processed_chunks, "cleaned_chunks.json")
        
        logger.info(f"éšæ®µ 2 å®Œæˆ: æ¸…ç†äº† {len(processed_chunks)} å€‹ chunks")
        return processed_chunks


class Stage3TaggingProcessor(BaseStageProcessor):
    """éšæ®µ 3: æ¨™ç±¤è™•ç†å™¨"""
    
    def __init__(self, data_dir: str = "data/stage3_tagging"):
        super().__init__(data_dir)
        self.stage_name = "Stage3Tagging"
        self.tag_processor = None
        self.tag_mappings = {}
    
    def load_tag_processor(self, tag_csv_path: str):
        """è¼‰å…¥æ¨™ç±¤è™•ç†å™¨ - ä½¿ç”¨çµ±ä¸€çš„ UnifiedTagProcessor"""
        try:
            from .tag_processor import UnifiedTagProcessor
            
            self.tag_processor = UnifiedTagProcessor(tag_csv_path)
            logger.info("âœ… æˆåŠŸè¼‰å…¥çµ±ä¸€æ¨™ç±¤è™•ç†å™¨")
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥çµ±ä¸€æ¨™ç±¤è™•ç†å™¨å¤±æ•—: {e}")
            raise
    
    def extract_tags(self, text: str) -> Tuple[List[str], Dict[str, List[str]]]:
        """æå–æ¨™ç±¤"""
        if not self.tag_processor:
            raise RuntimeError("æ¨™ç±¤è™•ç†å™¨æœªè¼‰å…¥")
        
        # ä½¿ç”¨çµ±ä¸€æ¨™ç±¤è™•ç†å™¨æå–æ¨™ç±¤
        tags = self.tag_processor.extract_enhanced_tags(text)
        
        # åˆ†é¡æ¨™ç±¤ä¾†æºï¼ˆé€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›å¯ä»¥æ ¹æ“š TagProcessor çš„å¯¦ç¾ä¾†åˆ†é¡ï¼‰
        tag_sources = {
            'enhanced_processor': tags,  # å¢å¼·ç‰ˆè™•ç†å™¨
            'fallback_sources': []  # å‚™æ´ä¾†æº
        }
        
        return tags, tag_sources
    
    def process(self, cleaned_chunks: List[Dict]) -> List[Dict]:
        """è™•ç†æ¨™ç±¤æå–"""
        logger.info("=== é–‹å§‹éšæ®µ 3: æ¨™ç±¤è™•ç† ===")
        
        if not self.tag_processor:
            raise RuntimeError("è«‹å…ˆè¼‰å…¥æ¨™ç±¤è™•ç†å™¨")
        
        tagged_chunks = []
        
        for chunk_data in cleaned_chunks:
            text = chunk_data['cleaned_content']
            tags, tag_sources = self.extract_tags(text)
            
            tagged_chunk = {
                **chunk_data,
                'tags': tags,
                'tag_sources': tag_sources,
                'tagged_at': datetime.now().isoformat()
            }
            tagged_chunks.append(tagged_chunk)
        
        # å„²å­˜çµæœ
        self.save_results(tagged_chunks, "tagged_chunks.json")
        
        logger.info(f"éšæ®µ 3 å®Œæˆ: ç‚º {len(tagged_chunks)} å€‹ chunks æ·»åŠ æ¨™ç±¤")
        return tagged_chunks


class Stage4EmbeddingPrepProcessor(BaseStageProcessor):
    """éšæ®µ 4: Embedding æº–å‚™è™•ç†å™¨"""
    
    def __init__(self, data_dir: str = "data/stage4_embedding_prep"):
        super().__init__(data_dir)
        self.stage_name = "Stage4EmbeddingPrep"
    
    def prepare_for_embedding(self, tagged_chunk: Dict) -> Dict:
        """æº–å‚™ embedding è³‡æ–™"""
        # æº–å‚™ç¬¦åˆ Milvus æ ¼å¼çš„è³‡æ–™
        embedding_data = {
            'chunk_id': tagged_chunk['chunk_id'],
            'episode_id': tagged_chunk['episode_id'],
            'chunk_index': tagged_chunk['chunk_index'],
            'content': tagged_chunk['cleaned_content'],
            'tags': tagged_chunk['tags'],
            'tag_sources': tagged_chunk['tag_sources'],
            'original_filename': tagged_chunk['original_filename'],
            'collection_name': tagged_chunk['collection_name'],
            'created_at': tagged_chunk['created_at'],
            'prepared_at': datetime.now().isoformat()
        }
        
        return embedding_data
    
    def process(self, tagged_chunks: List[Dict]) -> List[Dict]:
        """è™•ç† embedding æº–å‚™"""
        logger.info("=== é–‹å§‹éšæ®µ 4: Embedding æº–å‚™è™•ç† ===")
        
        embedding_ready_data = []
        
        for tagged_chunk in tagged_chunks:
            embedding_data = self.prepare_for_embedding(tagged_chunk)
            embedding_ready_data.append(embedding_data)
        
        # å„²å­˜çµæœ
        self.save_results(embedding_ready_data, "embedding_ready_data.json")
        
        logger.info(f"éšæ®µ 4 å®Œæˆ: æº–å‚™äº† {len(embedding_ready_data)} ç­† embedding è³‡æ–™")
        return embedding_ready_data


class PipelineOrchestrator:
    """ç®¡ç·šå”èª¿å™¨"""
    
    def __init__(self, base_data_dir: str = "data"):
        self.base_data_dir = Path(base_data_dir)
        self.stage1 = Stage1TextChunker(f"{base_data_dir}/stage1_chunking")
        self.stage2 = Stage2TextCleaningProcessor(f"{base_data_dir}/stage2_text_cleaning")
        self.stage3 = Stage3TaggingProcessor(f"{base_data_dir}/stage3_tagging")
        self.stage4 = Stage4EmbeddingPrepProcessor(f"{base_data_dir}/stage4_embedding_prep")
    
    def run_stage1(self, mongodb_config: Dict[str, str]) -> List[Dict]:
        """åŸ·è¡Œéšæ®µ 1"""
        return self.stage1.process(mongodb_config)
    
    def run_stage2(self, chunks_data: List[Dict]) -> List[Dict]:
        """åŸ·è¡Œéšæ®µ 2"""
        return self.stage2.process(chunks_data)
    
    def run_stage3(self, cleaned_chunks: List[Dict], tag_csv_path: str) -> List[Dict]:
        """åŸ·è¡Œéšæ®µ 3"""
        self.stage3.load_tag_processor(tag_csv_path)
        return self.stage3.process(cleaned_chunks)
    
    def run_stage4(self, tagged_chunks: List[Dict]) -> List[Dict]:
        """åŸ·è¡Œéšæ®µ 4"""
        return self.stage4.process(tagged_chunks)
    
    def run_all_stages(self, mongodb_config: Dict[str, str], tag_csv_path: str) -> List[Dict]:
        """åŸ·è¡Œæ‰€æœ‰éšæ®µ"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´ç®¡ç·š")
        
        # éšæ®µ 1: æ–‡æœ¬åˆ‡æ–·
        chunks_data = self.run_stage1(mongodb_config)
        
        # éšæ®µ 2: æ–‡æœ¬æ¸…ç†
        cleaned_chunks = self.run_stage2(chunks_data)
        
        # éšæ®µ 3: æ¨™ç±¤è™•ç†
        tagged_chunks = self.run_stage3(cleaned_chunks, tag_csv_path)
        
        # éšæ®µ 4: Embedding æº–å‚™
        embedding_data = self.run_stage4(tagged_chunks)
        
        logger.info("ğŸ‰ å®Œæ•´ç®¡ç·šåŸ·è¡Œå®Œæˆ")
        return embedding_data 