#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ Milvus è³‡æ–™è½‰æ›åŠŸèƒ½
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from convert_to_milvus_format import MilvusDataConverter


def test_single_file_conversion():
    """æ¸¬è©¦å–®å€‹æª”æ¡ˆè½‰æ›"""
    print("ğŸ§ª æ¸¬è©¦å–®å€‹æª”æ¡ˆè½‰æ›...")
    
    # å»ºç«‹æ¸¬è©¦è³‡æ–™
    test_data = {
        "filename": "RSS_262026947_podcast_1304_3D printers.json",
        "episode_id": "6865015b66c1a4e8d1176616",
        "collection_name": "RSS_262026947",
        "total_chunks": 13,
        "chunks": [
            {
                "chunk_text": "This is a download from BBC Learning English. To find out more, visit our website.",
                "chunk_id": "be646042-981e-4558-ab38-da7cc3f0b9c8",
                "chunk_index": 0,
                "episode_id": "6865015b66c1a4e8d1176616",
                "original_filename": "RSS_262026947_podcast_1304_3D printers.json",
                "collection_name": "RSS_262026947",
                "chunk_length": 82,
                "enhanced_tags": ["BBC", "English", "Learning"],
                "tagged_at": "2025-07-10T14:45:08.823699"
            }
        ]
    }
    
    # å»ºç«‹æ¸¬è©¦æª”æ¡ˆ
    test_file = Path("test_input.json")
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    try:
        # å»ºç«‹è½‰æ›å™¨
        converter = MilvusDataConverter()
        
        # è½‰æ›æª”æ¡ˆ
        result = converter.convert_file_to_milvus_format(test_file)
        
        if result:
            print(f"âœ… è½‰æ›æˆåŠŸï¼å…± {len(result)} å€‹ chunks")
            
            # æª¢æŸ¥ç¬¬ä¸€å€‹ chunk çš„æ ¼å¼
            first_chunk = result[0]
            print("\nğŸ“‹ è½‰æ›å¾Œçš„ chunk æ ¼å¼:")
            for key, value in first_chunk.items():
                print(f"  {key}: {type(value).__name__} = {value}")
            
            # é©—è­‰å¿…è¦æ¬„ä½
            required_fields = [
                'chunk_id', 'chunk_index', 'episode_id', 'podcast_id',
                'podcast_name', 'author', 'category', 'episode_title',
                'duration', 'published_date', 'apple_rating', 'chunk_text',
                'embedding', 'language', 'created_at', 'source_model', 'tags'
            ]
            
            missing_fields = []
            for field in required_fields:
                if field not in first_chunk:
                    missing_fields.append(field)
            
            if missing_fields:
                print(f"âŒ ç¼ºå°‘æ¬„ä½: {missing_fields}")
            else:
                print("âœ… æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½å­˜åœ¨")
            
            # é©—è­‰è³‡æ–™å‹æ…‹
            type_checks = [
                ('chunk_id', str),
                ('chunk_index', int),
                ('episode_id', int),
                ('podcast_id', int),
                ('podcast_name', str),
                ('author', str),
                ('category', str),
                ('episode_title', str),
                ('chunk_text', str),
                ('tags', str)
            ]
            
            type_errors = []
            for field, expected_type in type_checks:
                if field in first_chunk:
                    actual_type = type(first_chunk[field])
                    if not isinstance(first_chunk[field], expected_type):
                        type_errors.append(f"{field}: æœŸæœ› {expected_type.__name__}, å¯¦éš› {actual_type.__name__}")
            
            if type_errors:
                print(f"âŒ å‹æ…‹éŒ¯èª¤: {type_errors}")
            else:
                print("âœ… æ‰€æœ‰æ¬„ä½å‹æ…‹æ­£ç¢º")
            
            return True
        else:
            print("âŒ è½‰æ›å¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False
    finally:
        # æ¸…ç†æ¸¬è©¦æª”æ¡ˆ
        if test_file.exists():
            test_file.unlink()


def test_filename_parsing():
    """æ¸¬è©¦æª”æ¡ˆåç¨±è§£æ"""
    print("\nğŸ§ª æ¸¬è©¦æª”æ¡ˆåç¨±è§£æ...")
    
    converter = MilvusDataConverter()
    
    test_cases = [
        "RSS_262026947_podcast_1304_3D printers.json",
        "RSS_1488295306_podcast_1321_æ—©æ™¨è²¡ç¶“é€Ÿè§£è®€.json",
        "RSS_1500839292_podcast_1234_è‚¡ç™ŒæŠ•è³‡ç†è²¡.json"
    ]
    
    for filename in test_cases:
        result = converter.parse_filename(filename)
        if result:
            print(f"âœ… {filename}")
            print(f"  podcast_id: {result['podcast_id']}")
            print(f"  episode_title: {result['episode_title']}")
        else:
            print(f"âŒ {filename} - è§£æå¤±æ•—")


def test_database_query():
    """æ¸¬è©¦è³‡æ–™åº«æŸ¥è©¢"""
    print("\nğŸ§ª æ¸¬è©¦è³‡æ–™åº«æŸ¥è©¢...")
    
    converter = MilvusDataConverter()
    
    # æ¸¬è©¦æŸ¥è©¢
    podcast_id = "262026947"
    episode_title = "3D printers"
    
    metadata = converter.get_episode_metadata_from_db(podcast_id, episode_title)
    
    if metadata:
        print("âœ… è³‡æ–™åº«æŸ¥è©¢æˆåŠŸ")
        print(f"  podcast_name: {metadata.get('podcast_name', 'N/A')}")
        print(f"  author: {metadata.get('author', 'N/A')}")
        print(f"  category: {metadata.get('category', 'N/A')}")
    else:
        print("âš ï¸ è³‡æ–™åº«æŸ¥è©¢å¤±æ•—ï¼Œå°‡ä½¿ç”¨å¿«å–è³‡æ–™")


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ Milvus è³‡æ–™è½‰æ›åŠŸèƒ½")
    
    # æ¸¬è©¦æª”æ¡ˆåç¨±è§£æ
    test_filename_parsing()
    
    # æ¸¬è©¦è³‡æ–™åº«æŸ¥è©¢
    test_database_query()
    
    # æ¸¬è©¦å–®å€‹æª”æ¡ˆè½‰æ›
    success = test_single_file_conversion()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼")
    else:
        print("\nâŒ æ¸¬è©¦å¤±æ•—")
        sys.exit(1)


if __name__ == "__main__":
    main() 