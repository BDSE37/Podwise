"""
Milvus Embedding è™•ç†å™¨
å°‡ stage4_embedding_prep ç›®éŒ„ä¸‹çš„è³‡æ–™é€²è¡Œ embedding ä¸¦å¯«å…¥ Milvus
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
from datetime import datetime
from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility
import sys
import os
import time
from tqdm import tqdm

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ç›´æ¥å®šç¾© VectorProcessor é¡åˆ¥ï¼Œé¿å…ä¾è³´å•é¡Œ
import logging
from typing import List, Optional, Dict, Any
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class VectorProcessor:
    """å‘é‡åŒ–è™•ç†å™¨"""
    
    def __init__(self, embedding_model: str = "BAAI/bge-m3", device: Optional[str] = None):
        """
        åˆå§‹åŒ–å‘é‡åŒ–è™•ç†å™¨
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹åç¨±
            device: è¨­å‚™ (cpu/cuda)
        """
        self.embedding_model = embedding_model
        self.device = device
        self.model: Optional[SentenceTransformer] = None
        
    def load_model(self) -> None:
        """è¼‰å…¥åµŒå…¥æ¨¡å‹"""
        if self.model is None:
            try:
                logging.info(f"è¼‰å…¥åµŒå…¥æ¨¡å‹: {self.embedding_model}")
                self.model = SentenceTransformer(self.embedding_model, device=self.device)
                logging.info("åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                logging.error(f"è¼‰å…¥åµŒå…¥æ¨¡å‹å¤±æ•—: {e}")
                raise
    
    def generate_embeddings(self, texts: List[str], normalize: bool = True, 
                          show_progress_bar: bool = True) -> np.ndarray:
        """
        ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦æ­£è¦åŒ–
            show_progress_bar: æ˜¯å¦é¡¯ç¤ºé€²åº¦æ¢
            
        Returns:
            åµŒå…¥å‘é‡é™£åˆ—
        """
        self.load_model()
        
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥")
        
        try:
            embeddings = self.model.encode(
                texts, 
                normalize_embeddings=normalize,
                show_progress_bar=show_progress_bar
            )
            logging.info(f"ç”Ÿæˆ {len(embeddings)} å€‹åµŒå…¥å‘é‡ï¼Œç¶­åº¦: {embeddings.shape[1]}")
            return embeddings
        except Exception as e:
            logging.error(f"ç”ŸæˆåµŒå…¥å‘é‡å¤±æ•—: {e}")
            raise
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ç²å–æ¨¡å‹è³‡è¨Š
        
        Returns:
            æ¨¡å‹è³‡è¨Šå­—å…¸
        """
        self.load_model()
        
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥")
        
        return {
            'model_name': self.embedding_model,
            'max_seq_length': self.model.max_seq_length,
            'embedding_dimension': self.model.get_sentence_embedding_dimension(),
            'device': str(self.model.device)
        }

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('embedding_process.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Milvus é€£ç·šè¨­å®š
MILVUS_CONFIG = {
    'host': '192.168.32.86',  # worker3 IP
    'port': '19530'
}

# é›†åˆåç¨±
COLLECTION_NAME = 'podwise_chunks'

# å‘é‡ç¶­åº¦ (BGE-M3 é è¨­ç‚º 1024)
VECTOR_DIM = 1024


class MilvusEmbeddingProcessor:
    """Milvus Embedding è™•ç†å™¨"""
    
    def __init__(self, milvus_config: Dict[str, str], collection_name: str = COLLECTION_NAME):
        """
        åˆå§‹åŒ– Milvus Embedding è™•ç†å™¨
        
        Args:
            milvus_config: Milvus é€£ç·šè¨­å®š
            collection_name: é›†åˆåç¨±
        """
        self.milvus_config = milvus_config
        self.collection_name = collection_name
        self.vector_processor = None
        self.model_loaded = False
        
    def initialize_vector_processor(self) -> None:
        """åˆå§‹åŒ–å‘é‡è™•ç†å™¨ä¸¦è¼‰å…¥æ¨¡å‹"""
        try:
            logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–å‘é‡è™•ç†å™¨...")
            self.vector_processor = VectorProcessor(embedding_model="BAAI/bge-m3")
            
            logger.info("ğŸ”„ æ­£åœ¨è¼‰å…¥ BGE-M3 æ¨¡å‹...")
            self.vector_processor.load_model()
            
            # ç²å–æ¨¡å‹è³‡è¨Š
            model_info = self.vector_processor.get_model_info()
            logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
            logger.info(f"   æ¨¡å‹åç¨±: {model_info['model_name']}")
            logger.info(f"   å‘é‡ç¶­åº¦: {model_info['embedding_dimension']}")
            logger.info(f"   æœ€å¤§åºåˆ—é•·åº¦: {model_info['max_seq_length']}")
            logger.info(f"   è¨­å‚™: {model_info['device']}")
            
            self.model_loaded = True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
        
    def connect_milvus(self) -> None:
        """é€£ç·šåˆ° Milvus"""
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨é€£ç·šåˆ° Milvus ({self.milvus_config['host']}:{self.milvus_config['port']})...")
            connections.connect(
                alias="default",
                host=self.milvus_config['host'],
                port=self.milvus_config['port']
            )
            logger.info("âœ… æˆåŠŸé€£ç·šåˆ° Milvus")
        except Exception as e:
            logger.error(f"âŒ é€£ç·š Milvus å¤±æ•—: {e}")
            raise
    
    def create_collection(self) -> None:
        """å»ºç«‹ Milvus é›†åˆ"""
        try:
            # æª¢æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if utility.has_collection(self.collection_name):
                logger.info(f"â„¹ï¸ é›†åˆ {self.collection_name} å·²å­˜åœ¨")
                return
            
            logger.info(f"ğŸ”„ æ­£åœ¨å»ºç«‹é›†åˆ {self.collection_name}...")
            
            # å®šç¾©æ¬„ä½çµæ§‹
            fields = [
                FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=64, is_primary=True),
                FieldSchema(name="chunk_index", dtype=DataType.INT64),
                FieldSchema(name="episode_id", dtype=DataType.INT64),
                FieldSchema(name="podcast_id", dtype=DataType.INT64),
                FieldSchema(name="podcast_name", dtype=DataType.VARCHAR, max_length=255),
                FieldSchema(name="author", dtype=DataType.VARCHAR, max_length=255),
                FieldSchema(name="category", dtype=DataType.VARCHAR, max_length=64),
                FieldSchema(name="episode_title", dtype=DataType.VARCHAR, max_length=255),
                FieldSchema(name="duration", dtype=DataType.VARCHAR, max_length=255),
                FieldSchema(name="published_date", dtype=DataType.VARCHAR, max_length=64),  # ä¿æŒ VARCHAR ä»¥å…¼å®¹ç¾æœ‰è³‡æ–™
                FieldSchema(name="apple_rating", dtype=DataType.FLOAT),
                FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=1024),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_DIM),
                FieldSchema(name="language", dtype=DataType.VARCHAR, max_length=16),
                FieldSchema(name="created_at", dtype=DataType.VARCHAR, max_length=64),  # ä¿æŒ VARCHAR ä»¥å…¼å®¹ç¾æœ‰è³‡æ–™
                FieldSchema(name="source_model", dtype=DataType.VARCHAR, max_length=64),
                FieldSchema(name="tags", dtype=DataType.VARCHAR, max_length=1024)
            ]
            
            schema = CollectionSchema(fields, description="Podcast chunks with embeddings")
            collection = Collection(self.collection_name, schema)
            
            # å»ºç«‹ç´¢å¼•
            logger.info("ğŸ”„ æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
            index_params = {
                "metric_type": "COSINE",
                "index_type": "IVF_FLAT",
                "params": {"nlist": 1024}
            }
            collection.create_index(field_name="embedding", index_params=index_params)
            
            logger.info(f"âœ… æˆåŠŸå»ºç«‹é›†åˆ {self.collection_name}")
            
        except Exception as e:
            logger.error(f"âŒ å»ºç«‹é›†åˆå¤±æ•—: {e}")
            raise
    
    def load_collection(self) -> Collection:
        """è¼‰å…¥é›†åˆ"""
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨è¼‰å…¥é›†åˆ {self.collection_name}...")
            collection = Collection(self.collection_name)
            collection.load()
            logger.info(f"âœ… æˆåŠŸè¼‰å…¥é›†åˆ {self.collection_name}")
            return collection
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥é›†åˆå¤±æ•—: {e}")
            raise
    
    def process_milvus_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        è™•ç†å–®å€‹ Milvus æ ¼å¼æª”æ¡ˆ
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è™•ç†å¾Œçš„è³‡æ–™åˆ—è¡¨
        """
        file_start_time = time.time()
        try:
            logger.info(f"ğŸ“„ é–‹å§‹è™•ç†æª”æ¡ˆ: {file_path.name}")
            
            # è®€å–æª”æ¡ˆ
            read_start_time = time.time()
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            read_time = time.time() - read_start_time
            logger.info(f"ğŸ“– æª”æ¡ˆè®€å–å®Œæˆï¼Œè€—æ™‚: {read_time:.2f}ç§’")
            
            chunks = data.get('chunks', [])
            if not chunks:
                logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path.name} æ²’æœ‰ chunks")
                return []
            
            logger.info(f"ğŸ“Š æª”æ¡ˆ {file_path.name}: æ‰¾åˆ° {len(chunks)} å€‹ chunks")
            
            # ç”Ÿæˆæ‰€æœ‰ chunk çš„åµŒå…¥å‘é‡
            chunk_texts = [chunk.get('chunk_text', '') for chunk in chunks]
            
            # éæ¿¾ç©ºæ–‡æœ¬
            valid_texts = []
            valid_indices = []
            for i, text in enumerate(chunk_texts):
                if text and text.strip():
                    valid_texts.append(text)
                    valid_indices.append(i)
            
            if not valid_texts:
                logger.warning(f"âš ï¸ æª”æ¡ˆ {file_path.name}: æ²’æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å…§å®¹")
                return []
            
            logger.info(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {len(valid_texts)} å€‹åµŒå…¥å‘é‡...")
            embedding_start_time = time.time()
            
            if self.vector_processor is None:
                raise RuntimeError("å‘é‡è™•ç†å™¨æœªåˆå§‹åŒ–")
                
            embeddings = self.vector_processor.generate_embeddings(
                valid_texts, 
                normalize=True, 
                show_progress_bar=True
            )
            
            embedding_time = time.time() - embedding_start_time
            logger.info(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {embedding_time:.2f}ç§’")
            
            # æº–å‚™æ’å…¥è³‡æ–™
            data_prep_start_time = time.time()
            logger.info(f"ğŸ”„ æ­£åœ¨æº–å‚™è³‡æ–™æ ¼å¼...")
            
            insert_data = []
            for i, chunk in enumerate(chunks):
                # è½‰æ› podcast_id ç‚ºæ•´æ•¸
                podcast_id_str = chunk.get('podcast_id', '0')
                if podcast_id_str is None or podcast_id_str == '':
                    podcast_id = 0
                else:
                    try:
                        podcast_id = int(podcast_id_str)
                    except (ValueError, TypeError):
                        podcast_id = 0
                
                # è½‰æ› episode_id ç‚ºæ•´æ•¸
                episode_id_str = chunk.get('episode_id', '0')
                if episode_id_str is None or episode_id_str == '':
                    episode_id = 0
                else:
                    try:
                        episode_id = int(episode_id_str)
                    except (ValueError, TypeError):
                        episode_id = 0
                
                # è™•ç†æ—¥æœŸæ ¼å¼
                published_date = chunk.get('published_date', '')
                if published_date is None or published_date == 'æœªçŸ¥':
                    published_date = ''
                
                # è™•ç† apple_ratingï¼Œç¢ºä¿ç‚º float å‹åˆ¥
                apple_rating = chunk.get('apple_rating', 0.0)
                try:
                    apple_rating = float(apple_rating)
                except (ValueError, TypeError):
                    apple_rating = 0.0
                
                # è™•ç†æ¨™ç±¤
                tags = chunk.get('tags', [])
                if isinstance(tags, str):
                    try:
                        tags = json.loads(tags)
                    except:
                        tags = []
                tags_str = json.dumps(tags, ensure_ascii=False) if tags else '[]'
                
                # ç²å–å°æ‡‰çš„åµŒå…¥å‘é‡
                if i in valid_indices:
                    embedding_idx = valid_indices.index(i)
                    embedding = embeddings[embedding_idx].tolist()
                else:
                    # å¦‚æœæ–‡æœ¬ç‚ºç©ºï¼Œä½¿ç”¨é›¶å‘é‡
                    embedding = [0.0] * VECTOR_DIM
                
                # ç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½æœ‰é è¨­å€¼
                insert_data.append({
                    'chunk_id': chunk.get('chunk_id', ''),
                    'chunk_index': chunk.get('chunk_index', 0),
                    'episode_id': episode_id,
                    'podcast_id': podcast_id,
                    'podcast_name': chunk.get('podcast_name', '') or '',
                    'author': chunk.get('author', '') or '',
                    'category': chunk.get('category', '') or '',
                    'episode_title': chunk.get('episode_title', '') or '',
                    'duration': chunk.get('duration', '') or '',
                    'published_date': published_date,
                    'apple_rating': apple_rating,
                    'chunk_text': (chunk.get('chunk_text', '') or '')[:1024],  # é™åˆ¶é•·åº¦
                    'embedding': embedding,
                    'language': chunk.get('language', 'zh') or 'zh',
                    'created_at': chunk.get('created_at', datetime.now().isoformat()) or datetime.now().isoformat(),
                    'source_model': chunk.get('source_model', 'BAAI/bge-m3') or 'BAAI/bge-m3',
                    'tags': tags_str
                })
            
            data_prep_time = time.time() - data_prep_start_time
            total_file_time = time.time() - file_start_time
            logger.info(f"âœ… è³‡æ–™æº–å‚™å®Œæˆï¼Œè€—æ™‚: {data_prep_time:.2f}ç§’")
            logger.info(f"âœ… æª”æ¡ˆ {file_path.name} ç¸½è™•ç†æ™‚é–“: {total_file_time:.2f}ç§’")
            logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ: è®€å– {read_time:.2f}s | åµŒå…¥ {embedding_time:.2f}s | æº–å‚™ {data_prep_time:.2f}s")
            
            return insert_data
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†æª”æ¡ˆ {file_path.name} å¤±æ•—: {e}")
            return []
    
    def insert_data_to_milvus(self, collection: Collection, data: List[Dict[str, Any]]) -> None:
        """
        å°‡è³‡æ–™æ’å…¥ Milvus
        
        Args:
            collection: Milvus é›†åˆ
            data: è¦æ’å…¥çš„è³‡æ–™
        """
        if not data:
            logger.warning("âš ï¸ æ²’æœ‰è³‡æ–™éœ€è¦æ’å…¥")
            return
        
        try:
            logger.info(f"ğŸ”„ æ­£åœ¨æ’å…¥ {len(data)} ç­†è³‡æ–™åˆ° Milvus...")
            insert_start_time = time.time()
            
            # æ‰¹æ¬¡æ’å…¥è³‡æ–™
            collection.insert(data)
            
            insert_time = time.time() - insert_start_time
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(data)} ç­†è³‡æ–™ï¼Œè€—æ™‚: {insert_time:.2f}ç§’")
            logger.info(f"ğŸ“Š å¹³å‡æ¯ç­†è³‡æ–™æ’å…¥æ™‚é–“: {insert_time/len(data):.3f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ æ’å…¥è³‡æ–™å¤±æ•—: {e}")
            logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {str(e)}")
            raise
    
    def process_stage4_embedding_prep(self, base_folder: str = "data/stage4_embedding_prep") -> None:
        """
        è™•ç† stage4_embedding_prep ç›®éŒ„ä¸‹çš„æ‰€æœ‰æª”æ¡ˆ
        
        Args:
            base_folder: åŸºç¤ç›®éŒ„è·¯å¾‘
        """
        logger.info("ğŸš€ é–‹å§‹ Milvus Embedding è™•ç†æµç¨‹")
        logger.info("=" * 60)
        
        # ä½¿ç”¨çµ•å°è·¯å¾‘
        script_dir = Path(__file__).parent
        base_path = script_dir.parent / base_folder
        if not base_path.exists():
            logger.error(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {base_folder}")
            return
        
        # åˆå§‹åŒ–å‘é‡è™•ç†å™¨
        self.initialize_vector_processor()
        
        # é€£ç·š Milvus
        self.connect_milvus()
        
        # å»ºç«‹é›†åˆ
        self.create_collection()
        
        # è¼‰å…¥é›†åˆ
        collection = self.load_collection()
        
        total_files = 0
        total_chunks = 0
        total_success = 0
        total_failed = 0
        total_embedding_time = 0
        total_insert_time = 0
        total_read_time = 0
        total_prep_time = 0
        
        # è™•ç†æ‰€æœ‰ JSON æª”æ¡ˆ
        json_files = [f for f in base_path.glob("*.json") if f.name != 'conversion_stats.json']
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆéœ€è¦è™•ç†")
        logger.info("=" * 60)
        
        overall_start_time = time.time()
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
        for json_file in tqdm(json_files, desc="è™•ç†æª”æ¡ˆ", unit="file"):
            file_start_time = time.time()
            total_files += 1
            
            try:
                logger.info(f"ğŸ“„ è™•ç†æª”æ¡ˆ {total_files}/{len(json_files)}: {json_file.name}")
                
                # è™•ç†æª”æ¡ˆ
                chunk_data = self.process_milvus_file(json_file)
                
                if chunk_data:
                    # æ’å…¥åˆ° Milvus
                    insert_start_time = time.time()
                    self.insert_data_to_milvus(collection, chunk_data)
                    insert_time = time.time() - insert_start_time
                    total_insert_time += insert_time
                    
                    total_chunks += len(chunk_data)
                    total_success += 1
                    
                    file_time = time.time() - file_start_time
                    logger.info(f"âœ… æˆåŠŸè™•ç† {json_file.name}: {len(chunk_data)} chunks")
                    logger.info(f"ğŸ“Š æª”æ¡ˆè™•ç†æ™‚é–“: {file_time:.2f}ç§’ (æ’å…¥: {insert_time:.2f}ç§’)")
                else:
                    total_failed += 1
                    logger.warning(f"âš ï¸ æª”æ¡ˆç„¡æ•ˆ: {json_file.name}")
                    
            except Exception as e:
                total_failed += 1
                logger.error(f"âŒ è™•ç†æª”æ¡ˆå¤±æ•— {json_file.name}: {e}")
        
        overall_time = time.time() - overall_start_time
        
        # è¼¸å‡ºçµ±è¨ˆçµæœ
        logger.info("=" * 60)
        logger.info("ğŸ‰ è™•ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š çµ±è¨ˆçµæœ:")
        logger.info(f"   ç¸½æª”æ¡ˆæ•¸: {total_files}")
        logger.info(f"   æˆåŠŸæª”æ¡ˆ: {total_success}")
        logger.info(f"   å¤±æ•—æª”æ¡ˆ: {total_failed}")
        logger.info(f"   ç¸½ chunk æ•¸: {total_chunks}")
        logger.info(f"   æˆåŠŸç‡: {(total_success/total_files*100):.1f}%")
        logger.info(f"   ç¸½è™•ç†æ™‚é–“: {overall_time:.2f}ç§’")
        logger.info(f"   å¹³å‡æ¯æª”æ¡ˆ: {overall_time/total_files:.2f}ç§’")
        logger.info(f"   å¹³å‡æ¯ chunk: {overall_time/total_chunks:.3f}ç§’")
        logger.info(f"   ç¸½æ’å…¥æ™‚é–“: {total_insert_time:.2f}ç§’")
        
        # é‡‹æ”¾é›†åˆ
        collection.release()
        logger.info("âœ… é›†åˆå·²é‡‹æ”¾")


def main():
    """ä¸»å‡½æ•¸"""
    try:
        processor = MilvusEmbeddingProcessor(MILVUS_CONFIG)
        processor.process_stage4_embedding_prep()
    except KeyboardInterrupt:
        logger.info("âš ï¸ ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
    except Exception as e:
        logger.error(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise


if __name__ == "__main__":
    main() 