#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°‡ stage3_tagging çš„ JSON æ ¼å¼è½‰æ›ç‚ºç¬¦åˆ Milvus æ’å…¥æ ¼å¼çš„è³‡æ–™çµæ§‹
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MilvusDataConverter:
    """Milvus è³‡æ–™è½‰æ›å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è½‰æ›å™¨"""
        self.db_config = {
            "host": os.getenv("POSTGRES_HOST", "10.233.50.117"),
            "port": int(os.getenv("POSTGRES_PORT", "5432")),
            "database": os.getenv("POSTGRES_DB", "podcast"),
            "user": os.getenv("POSTGRES_USER", "bdse37"),
            "password": os.getenv("POSTGRES_PASSWORD", "111111")
        }
        
        # é è¨­ podcast åç¨±å¿«å–
        self.podcast_name_cache = {
            "1488295306": "æ—©æ™¨è²¡ç¶“é€Ÿè§£è®€",
            "1500839292": "è‚¡ç™Œ",
            "1531106786": "å°å¹£æ¼²å¤ äº†å—",
            "1533645986": "ç†±è­°è¯çˆ¾è¡—",
            "1536242998": "å³æ·¡å¦‚",
            "1590806478": "å°å¹£æ”¶é—œ",
            "1626274583": "å‰µä½œæ­Œæ‰‹",
            "1707757888": "è²¡ç¶“Må¹³æ–¹",
            "1776077547": "FIREMAN",
            "1816898458": "Vè½‰æŠ•è³‡",
            "1452688611": "å·¥ä½œä¸­é‚£äº›äº‹",
            "1488718553": "å¹¸ç¦ç¿¹ç¿¹æ¿",
            "1500162537": "å“‡è³½å¿ƒè§€é»",
            "1513786617": "æ‹¿éŒ¢æ›å¿«æ¨‚",
            "1545511347": "ä¸å­¸æ–‡çš„è²¡ç¶“ä¸–ç•Œ",
            "1567737523": "ç”Ÿæ´»è‹±èªé€š",
            "1693352123": "å¤å…¸è©©è©",
            "262026947": "Climate Change"
        }
        
        # é è¨­ podcast ä½œè€…å¿«å–
        self.podcast_author_cache = {
            "1488295306": "è²¡ç¶“Må¹³æ–¹",
            "1500839292": "è¬å­Ÿæ­",
            "1531106786": "è²¡ç¶“Må¹³æ–¹",
            "1533645986": "è²¡ç¶“Må¹³æ–¹",
            "1536242998": "å³æ·¡å¦‚",
            "1590806478": "è²¡ç¶“Må¹³æ–¹",
            "1626274583": "è¬å­Ÿæ­",
            "1707757888": "è²¡ç¶“Må¹³æ–¹",
            "1776077547": "FIREMAN",
            "1816898458": "Vè½‰æŠ•è³‡",
            "1452688611": "è¬å­Ÿæ­",
            "1488718553": "è¬å­Ÿæ­",
            "1500162537": "è¬å­Ÿæ­",
            "1513786617": "è¬å­Ÿæ­",
            "1545511347": "ä¸å­¸æ–‡",
            "1567737523": "è¬å­Ÿæ­",
            "1693352123": "è¬å­Ÿæ­",
            "262026947": "BBC Learning English"
        }
        
        # é è¨­ podcast åˆ†é¡å¿«å–
        self.podcast_category_cache = {
            "1488295306": "business",
            "1500839292": "business",
            "1531106786": "business",
            "1533645986": "business",
            "1536242998": "business",
            "1590806478": "business",
            "1626274583": "business",
            "1707757888": "business",
            "1776077547": "business",
            "1816898458": "business",
            "1452688611": "education",
            "1488718553": "education",
            "1500162537": "education",
            "1513786617": "education",
            "1545511347": "education",
            "1567737523": "education",
            "1693352123": "education",
            "262026947": "education"
        }
    
    def get_db_connection(self):
        """ç²å–è³‡æ–™åº«é€£æ¥"""
        try:
            return psycopg2.connect(**self.db_config)
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            return None
    
    def parse_filename(self, filename: str) -> Optional[Dict[str, str]]:
        """è§£ææª”æ¡ˆåç¨±ç²å– podcast_id å’Œ episode_title"""
        try:
            # ç§»é™¤ .json å‰¯æª”å
            base_name = filename.replace('.json', '')
            
            # è§£ææ ¼å¼ï¼šRSS_{podcast_id}_podcast_{episode_id}_{episode_title}
            if not base_name.startswith('RSS_'):
                logger.warning(f"æª”æ¡ˆåç¨±æ ¼å¼éŒ¯èª¤ï¼Œä¸æ˜¯ RSS_ é–‹é ­: {filename}")
                return None
            
            # ç§»é™¤ "RSS_" å‰ç¶´
            base_name = base_name[4:]  # ç§»é™¤ "RSS_"
            
            # æ‰¾åˆ°ç¬¬ä¸€å€‹ä¸‹åŠƒç·šå¾Œçš„ä½ç½®ï¼ˆpodcast_idï¼‰
            first_underscore = base_name.find('_')
            if first_underscore == -1:
                logger.warning(f"æª”æ¡ˆåç¨±æ ¼å¼éŒ¯èª¤ï¼Œæ‰¾ä¸åˆ° podcast_id: {filename}")
                return None
            
            podcast_id = base_name[:first_underscore]
            
            # æ‰¾åˆ°æœ€å¾Œä¸€å€‹ä¸‹åŠƒç·šçš„ä½ç½®ï¼ˆepisode_title é–‹å§‹ä½ç½®ï¼‰
            last_underscore = base_name.rfind('_')
            if last_underscore == -1 or last_underscore <= first_underscore:
                logger.warning(f"æª”æ¡ˆåç¨±æ ¼å¼éŒ¯èª¤ï¼Œæ‰¾ä¸åˆ° episode_title: {filename}")
                return None
            
            episode_title = base_name[last_underscore + 1:]  # æœ€å¾Œä¸€å€‹_ä¹‹å¾Œçš„éƒ¨åˆ†
            
            return {
                'podcast_id': podcast_id,
                'episode_title': episode_title
            }
            
        except Exception as e:
            logger.error(f"è§£ææª”æ¡ˆåç¨±å¤±æ•—: {e}")
            return None

    def get_podcast_metadata_from_db(self, podcast_id: str) -> Optional[Dict[str, Any]]:
        """å¾è³‡æ–™åº«ç²å– podcast å®Œæ•´å…ƒè³‡æ–™"""
        try:
            conn = self.get_db_connection()
            if not conn:
                return None
            
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                query = """
                SELECT 
                    podcast_id,
                    podcast_name,
                    author,
                    category,
                    apple_rating,
                    rss_link,
                    languages,
                    created_at,
                    updated_at
                FROM podcasts
                WHERE podcast_id = %s
                LIMIT 1
                """
                
                cursor.execute(query, (podcast_id,))
                result = cursor.fetchone()
                
                if result:
                    return dict(result)
                
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢ podcast è³‡æ–™å¤±æ•—: {e}")
            return None
        finally:
            if conn:
                conn.close()

    def get_episode_metadata_from_db(self, podcast_id: str, episode_title: str) -> Optional[Dict[str, Any]]:
        """å¾è³‡æ–™åº«ç²å– episode å®Œæ•´å…ƒè³‡æ–™ï¼Œä½¿ç”¨æ¨¡ç³Šæ¯”å°"""
        try:
            conn = self.get_db_connection()
            if not conn:
                return None
            
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # ä½¿ç”¨æ¨¡ç³Šæ¯”å°æŸ¥è©¢ episode
                query = """
                SELECT 
                    e.episode_id,
                    e.podcast_id,
                    e.episode_title,
                    e.published_date,
                    e.duration,
                    e.description,
                    e.created_at,
                    p.podcast_name,
                    p.author,
                    p.category,
                    p.apple_rating,
                    p.rss_link,
                    p.languages
                FROM episodes e
                JOIN podcasts p ON e.podcast_id = p.podcast_id
                WHERE e.podcast_id = %s 
                AND e.episode_title ILIKE %s
                ORDER BY e.created_at DESC
                LIMIT 1
                """
                
                # ä½¿ç”¨æ¨¡ç³Šæ¯”å°ï¼ŒåŒ…å« episode_title
                search_pattern = f"%{episode_title}%"
                cursor.execute(query, (podcast_id, search_pattern))
                result = cursor.fetchone()
                
                if result:
                    return dict(result)
                
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢è³‡æ–™åº«å¤±æ•—: {e}")
            return None
        finally:
            if conn:
                conn.close()

    def convert_chunk_to_milvus_format(self, chunk: Dict[str, Any], episode_metadata: Optional[Dict[str, Any]] = None, podcast_metadata: Optional[Dict[str, Any]] = None, parsed_info: Optional[Dict[str, str]] = None) -> Optional[Dict[str, Any]]:
        """å°‡å–®å€‹ chunk è½‰æ›ç‚º Milvus æ ¼å¼ï¼Œç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½æœ‰æ­£ç¢ºå€¼"""
        try:
            # å¾æª”åè§£æçš„ podcast_id
            podcast_id = 0
            if parsed_info:
                podcast_id = int(parsed_info.get('podcast_id', '0'))
            
            # åŸºæœ¬æ¬„ä½
            milvus_data = {
                'chunk_id': chunk.get('chunk_id', ''),
                'chunk_index': chunk.get('chunk_index', 0),
                'chunk_text': chunk.get('chunk_text', ''),
                'language': chunk.get('language', 'zh'),
                'created_at': chunk.get('created_at', datetime.now().isoformat()),
                'source_model': chunk.get('source_model', 'bge-m3'),
                'embedding': chunk.get('embedding', []),
                'podcast_id': podcast_id
            }
            
            # è™•ç† episode_id
            episode_id_str = chunk.get('episode_id', '')
            if episode_id_str:
                try:
                    if isinstance(episode_id_str, str) and len(episode_id_str) > 10:
                        episode_id = abs(hash(episode_id_str)) % (2**63)
                    else:
                        episode_id = int(episode_id_str)
                except (ValueError, TypeError):
                    episode_id = abs(hash(str(episode_id_str))) % (2**63)
            else:
                episode_id = 0
            milvus_data['episode_id'] = episode_id
            
            # å„ªå…ˆä½¿ç”¨ episode_metadataï¼Œå…¶æ¬¡ä½¿ç”¨ podcast_metadataï¼Œæœ€å¾Œä½¿ç”¨å¿«å–
            if episode_metadata:
                # ä½¿ç”¨ episode æŸ¥è©¢çµæœï¼ˆåŒ…å« podcast è³‡è¨Šï¼‰
                # è™•ç† Decimal å‹æ…‹
                apple_rating = episode_metadata.get('apple_rating', 0)
                if hasattr(apple_rating, '__float__'):
                    apple_rating = float(apple_rating)
                else:
                    apple_rating = int(apple_rating) if apple_rating else 0
                
                # è™•ç† duration
                duration = episode_metadata.get('duration', '')
                if duration:
                    duration = str(duration)
                else:
                    duration = 'Unknown'
                
                # è™•ç† published_date
                published_date = episode_metadata.get('published_date', '')
                if published_date:
                    published_date = published_date.isoformat()
                else:
                    published_date = 'Unknown'
                
                milvus_data.update({
                    'podcast_name': episode_metadata.get('podcast_name', f"Podcast_{podcast_id}"),
                    'author': episode_metadata.get('author', 'Unknown'),
                    'category': episode_metadata.get('category', 'business'),
                    'episode_title': episode_metadata.get('episode_title', parsed_info.get('episode_title', '') if parsed_info else ''),
                    'duration': duration,
                    'published_date': published_date,
                    'apple_rating': apple_rating
                })
            elif podcast_metadata:
                # ä½¿ç”¨ podcast æŸ¥è©¢çµæœ
                # è™•ç† Decimal å‹æ…‹
                apple_rating = podcast_metadata.get('apple_rating', 0)
                if hasattr(apple_rating, '__float__'):
                    apple_rating = float(apple_rating)
                else:
                    apple_rating = int(apple_rating) if apple_rating else 0
                
                milvus_data.update({
                    'podcast_name': podcast_metadata.get('podcast_name', f"Podcast_{podcast_id}"),
                    'author': podcast_metadata.get('author', 'Unknown'),
                    'category': podcast_metadata.get('category', 'business'),
                    'episode_title': parsed_info.get('episode_title', '') if parsed_info else '',
                    'duration': 'Unknown',
                    'published_date': 'Unknown',
                    'apple_rating': apple_rating
                })
            else:
                # ä½¿ç”¨å¿«å–è³‡æ–™
                podcast_id_str = str(podcast_id)
                milvus_data.update({
                    'podcast_name': self.podcast_name_cache.get(podcast_id_str, f"Podcast_{podcast_id}"),
                    'author': self.podcast_author_cache.get(podcast_id_str, 'Unknown'),
                    'category': self.podcast_category_cache.get(podcast_id_str, 'business'),
                    'episode_title': parsed_info.get('episode_title', '') if parsed_info else '',
                    'duration': 'Unknown',
                    'published_date': 'Unknown',
                    'apple_rating': int(chunk.get('apple_rating', 0)) if chunk.get('apple_rating') else 0
                })
            
            # è™•ç† tags
            tags = chunk.get('enhanced_tags', [])
            if isinstance(tags, str):
                try:
                    tags = json.loads(tags)
                except json.JSONDecodeError:
                    tags = []
            elif not isinstance(tags, list):
                tags = []
            milvus_data['tags'] = json.dumps(tags, ensure_ascii=False)
            
            return milvus_data
            
        except Exception as e:
            logger.error(f"è½‰æ› chunk å¤±æ•—: {e}")
            return None

    def convert_file_to_milvus_format(self, file_path: Path) -> Optional[List[Dict[str, Any]]]:
        """å°‡å–®å€‹æª”æ¡ˆè½‰æ›ç‚º Milvus æ ¼å¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            filename = data.get('filename', '')
            chunks = data.get('chunks', [])
            
            if not chunks:
                logger.warning(f"æª”æ¡ˆ {filename} æ²’æœ‰ chunks")
                return None
            
            # è§£ææª”æ¡ˆåç¨±
            parsed_info = self.parse_filename(filename)
            if not parsed_info:
                logger.warning(f"ç„¡æ³•è§£ææª”æ¡ˆåç¨±: {filename}")
                return None
            
            podcast_id = parsed_info['podcast_id']
            episode_title = parsed_info['episode_title']
            
            # å¾è³‡æ–™åº«ç²å– episode å’Œ podcast å…ƒè³‡æ–™
            episode_metadata = self.get_episode_metadata_from_db(podcast_id, episode_title)
            podcast_metadata = self.get_podcast_metadata_from_db(podcast_id)
            
            # è½‰æ›æ‰€æœ‰ chunksï¼Œç¢ºä¿åŒä¸€æª”æ¡ˆå…§æ‰€æœ‰ chunks çš„ episode_titleã€durationã€published_date éƒ½ç›¸åŒ
            milvus_chunks = []
            for chunk in chunks:
                milvus_chunk = self.convert_chunk_to_milvus_format(chunk, episode_metadata, podcast_metadata, parsed_info)
                if milvus_chunk:
                    milvus_chunks.append(milvus_chunk)
            
            return milvus_chunks if milvus_chunks else None
            
        except Exception as e:
            logger.error(f"è½‰æ›æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
            return None
    
    def convert_stage3_to_milvus_format(self, stage3_dir: str = "data/stage3_tagging", 
                                      output_dir: str = "data/stage4_embedding_prep") -> Dict[str, Any]:
        """å°‡ stage3_tagging ç›®éŒ„ä¸‹çš„æ‰€æœ‰æª”æ¡ˆè½‰æ›ç‚º Milvus æ ¼å¼"""
        try:
            stage3_path = Path(stage3_dir)
            output_path = Path(output_dir)
            
            if not stage3_path.exists():
                return {"error": f"ç›®éŒ„ä¸å­˜åœ¨: {stage3_dir}"}
            
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_path.mkdir(parents=True, exist_ok=True)
            
            # è™•ç†æ‰€æœ‰ JSON æª”æ¡ˆ
            json_files = list(stage3_path.rglob("*.json"))
            
            total_files = len(json_files)
            successful_files = 0
            failed_files = 0
            total_chunks = 0
            failed_files_list = []
            
            logger.info(f"é–‹å§‹è™•ç† {total_files} å€‹æª”æ¡ˆ...")
            
            for json_file in json_files:
                try:
                    logger.info(f"è™•ç†æª”æ¡ˆ: {json_file.name}")
                    
                    # è½‰æ›æª”æ¡ˆ
                    milvus_chunks = self.convert_file_to_milvus_format(json_file)
                    
                    if milvus_chunks:
                        # å„²å­˜è½‰æ›å¾Œçš„è³‡æ–™
                        output_file = output_path / f"{json_file.stem}_milvus.json"
                        
                        output_data = {
                            "filename": json_file.name,
                            "total_chunks": len(milvus_chunks),
                            "converted_at": datetime.now().isoformat(),
                            "chunks": milvus_chunks
                        }
                        
                        with open(output_file, 'w', encoding='utf-8') as f:
                            json.dump(output_data, f, ensure_ascii=False, indent=2)
                        
                        successful_files += 1
                        total_chunks += len(milvus_chunks)
                        
                        logger.info(f"âœ… æˆåŠŸè½‰æ› {json_file.name}: {len(milvus_chunks)} chunks")
                    else:
                        failed_files += 1
                        failed_files_list.append(str(json_file))
                        logger.warning(f"âŒ è½‰æ›å¤±æ•—: {json_file.name}")
                    
                except Exception as e:
                    failed_files += 1
                    failed_files_list.append(str(json_file))
                    logger.error(f"è™•ç†æª”æ¡ˆå¤±æ•— {json_file.name}: {e}")
            
            # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            stats = {
                "total_files": total_files,
                "successful_files": successful_files,
                "failed_files": failed_files,
                "total_chunks": total_chunks,
                "success_rate": successful_files / total_files * 100 if total_files > 0 else 0,
                "failed_files_list": failed_files_list
            }
            
            # å„²å­˜çµ±è¨ˆå ±å‘Š
            stats_file = output_path / "conversion_stats.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ‰ è½‰æ›å®Œæˆï¼æˆåŠŸ: {successful_files}/{total_files}, ç¸½ chunks: {total_chunks}")
            return stats
            
        except Exception as e:
            logger.error(f"è½‰æ›éç¨‹å¤±æ•—: {e}")
            return {"error": str(e)}


def main():
    """ä¸»å‡½æ•¸"""
    try:
        converter = MilvusDataConverter()
        
        # è½‰æ› stage3_tagging åˆ° stage4_embedding_prep
        result = converter.convert_stage3_to_milvus_format()
        
        if "error" in result:
            logger.error(f"è½‰æ›å¤±æ•—: {result['error']}")
            sys.exit(1)
        else:
            logger.info("âœ… è½‰æ›æˆåŠŸå®Œæˆ")
            print(f"çµ±è¨ˆçµæœ: {result}")
            
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 