#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹æ¬¡è³‡æ–™è£œé½Šè™•ç†è…³æœ¬
å¾ stage3_tagging è®€å–æª”æ¡ˆï¼Œè£œé½Šå¿…è¦æ¬„ä½å¾Œè¼¸å‡ºåˆ° stage4_embedding_prep
"""

import os
import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import psycopg2
from psycopg2.extras import RealDictCursor

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

import psycopg2
from psycopg2.extras import RealDictCursor

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_complete_data.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DataCompleter:
    """è³‡æ–™è£œé½Šå™¨ - å¾ PostgreSQL è£œé½Šå¿…è¦æ¬„ä½"""
    
    def __init__(self):
        """åˆå§‹åŒ–è³‡æ–™è£œé½Šå™¨"""
        self.db_config = {
            "host": os.getenv("POSTGRES_HOST", "10.233.50.117"),
            "port": int(os.getenv("POSTGRES_PORT", "5432")),
            "database": os.getenv("POSTGRES_DB", "podcast"),
            "user": os.getenv("POSTGRES_USER", "bdse37"),
            "password": os.getenv("POSTGRES_PASSWORD", "111111")
        }
        
        # é è¨­ podcast åç¨±å¿«å–
        self.podcast_name_cache = {
            "1488295306": "æ—©æ™¨è²¡ç¶“é€Ÿè§£è®€",
            "1500839292": "è‚¡ç™Œ",
            "1531106786": "å°å¹£æ¼²å¤ äº†å—",
            "1533645986": "ç†±è­°è¯çˆ¾è¡—",
            "1536242998": "å³æ·¡å¦‚",
            "1590806478": "å°å¹£æ”¶é—œ",
            "1626274583": "å‰µä½œæ­Œæ‰‹",
            "1707757888": "è²¡ç¶“Må¹³æ–¹",
            "1776077547": "FIREMAN",
            "1816898458": "Vè½‰æŠ•è³‡",
            "1452688611": "å·¥ä½œä¸­é‚£äº›äº‹",
            "1488718553": "å¹¸ç¦ç¿¹ç¿¹æ¿",
            "1500162537": "å“‡è³½å¿ƒè§€é»",
            "1513786617": "æ‹¿éŒ¢æ›å¿«æ¨‚",
            "1545511347": "ä¸å­¸æ–‡çš„è²¡ç¶“ä¸–ç•Œ",
            "1567737523": "ç”Ÿæ´»è‹±èªé€š",
            "1693352123": "å¤å…¸è©©è©",
            "262026947": "Climate Change"
        }
        
        # é è¨­ TAGS å¿«å–
        self.podcast_tags_cache = {
            "1488295306": ["è²¡ç¶“åˆ†æ", "æŠ•è³‡ç†è²¡", "è‚¡å¸‚è¶¨å‹¢", "ç¶“æ¿Ÿæ”¿ç­–"],
            "1500839292": ["æŠ•è³‡ç†è²¡", "è‚¡ç¥¨åˆ†æ", "ç¶“æ¿Ÿåˆ†æ", "è²¡å‹™è¦åŠƒ"],
            "1531106786": ["è²¡ç¶“åˆ†æ", "å°å¹£åŒ¯ç‡", "å°ç©é›»", "æŠ•è³‡ç­–ç•¥"],
            "1533645986": ["å€å¡Šéˆ", "åŠ å¯†è²¨å¹£", "ç©©å®šå¹£", "é‡‘èç§‘æŠ€"],
            "1536242998": ["å€‹äººæˆé•·", "è·æ¶¯ç™¼å±•", "å­¸ç¿’æ–¹æ³•", "è‡ªæˆ‘æå‡"],
            "1590806478": ["è²¡ç¶“åˆ†æ", "å°å¹£åŒ¯ç‡", "æŠ•è³‡ç­–ç•¥", "å¸‚å ´è¶¨å‹¢"],
            "1626274583": ["éŸ³æ¨‚å‰µä½œ", "è·æ¶¯ç™¼å±•", "å‰µæ¥­æ•…äº‹", "è—è¡“æ–‡åŒ–"],
            "1707757888": ["è²¡ç¶“åˆ†æ", "æŠ•è³‡ç†è²¡", "è·æ¶¯è½‰æ›", "è·¨é ˜åŸŸå­¸ç¿’"],
            "1776077547": ["æŠ•è³‡ç†è²¡", "FIREç†è²¡", "è²¡å‹™è¦åŠƒ", "ç†è²¡æ•™è‚²"],
            "1816898458": ["æŠ•è³‡ç†è²¡", "è²¡ç¶“åˆ†æ", "æŠ•è³‡ç­–ç•¥", "å¸‚å ´åˆ†æ"],
            "1452688611": ["è·æ¶¯ç™¼å±•", "å·¥ä½œå¿ƒæ…‹", "è·å ´æŠ€èƒ½", "å€‹äººæˆé•·"],
            "1488718553": ["æ•™è‚²ç†å¿µ", "é«”åˆ¶å¤–æ•™è‚²", "å­¸ç¿’æ–¹æ³•", "æ•™è‚²å‰µæ–°"],
            "1500162537": ["æŠ•è³‡ç†è²¡", "ç†è²¡è§€å¿µ", "ç”Ÿæ´»æ…‹åº¦", "è²¡å‹™è¦åŠƒ"],
            "1513786617": ["å¿ƒç†å­¸", "å¿«æ¨‚å“²å­¸", "äººç”Ÿè§€", "è‡ªæˆ‘æå‡"],
            "1545511347": ["åœ‹éš›è²¡ç¶“", "ç¶“æ¿Ÿåˆ†æ", "å…¨çƒè¶¨å‹¢", "è²¡ç¶“æ–°è"],
            "1567737523": ["è‹±èªå­¸ç¿’", "èªè¨€æ•™è‚²", "ç”Ÿæ´»è‹±èª", "æ•™è‚²å…§å®¹"],
            "1693352123": ["å¤å…¸æ–‡å­¸", "è©©è©æ¬£è³", "æ–‡åŒ–æ•™è‚²", "æ–‡å­¸è—è¡“"],
            "262026947": ["æ°£å€™è®Šé·", "ç’°å¢ƒè­°é¡Œ", "å¿ƒç†å¥åº·", "ç¤¾æœƒè­°é¡Œ"]
        }
    
    def get_db_connection(self):
        """ç²å–è³‡æ–™åº«é€£æ¥"""
        try:
            return psycopg2.connect(**self.db_config)
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            return None
    
    def get_episode_data_from_db(self, podcast_id: str, episode_title: str) -> Optional[Dict[str, Any]]:
        """å¾è³‡æ–™åº«ç²å– episode è³‡æ–™"""
        try:
            conn = self.get_db_connection()
            if not conn:
                return None
            
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                # ä½¿ç”¨ JOIN æŸ¥è©¢ episode å’Œ podcast è³‡æ–™
                query = """
                SELECT 
                    e.podcast_id, e.episode_title, p.podcast_name, 
                    e.audio_url, p.images_300 as image_url, p.category
                FROM episodes e
                JOIN podcasts p ON e.podcast_id = p.podcast_id
                WHERE e.podcast_id = %s 
                AND LOWER(REPLACE(e.episode_title, ' ', '')) = LOWER(REPLACE(%s, ' ', ''))
                LIMIT 1
                """
                
                cursor.execute(query, (podcast_id, episode_title))
                result = cursor.fetchone()
                
                if result:
                    return dict(result)
                
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢è³‡æ–™åº«å¤±æ•—: {e}")
            return None
        finally:
            if conn:
                conn.close()
    
    def get_podcast_data_from_db(self, podcast_id: str) -> Optional[Dict[str, Any]]:
        """å¾è³‡æ–™åº«ç²å– podcast è³‡æ–™"""
        try:
            conn = self.get_db_connection()
            if not conn:
                return None
            
            with conn.cursor(cursor_factory=RealDictCursor) as cursor:
                query = """
                SELECT 
                    podcast_id, podcast_name, images_300 as image_url, category
                FROM podcasts 
                WHERE podcast_id = %s
                LIMIT 1
                """
                
                cursor.execute(query, (podcast_id,))
                result = cursor.fetchone()
                
                if result:
                    return dict(result)
                
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢ podcast è³‡æ–™å¤±æ•—: {e}")
            return None
        finally:
            if conn:
                conn.close()
    
    def complete_data(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è£œé½Šè³‡æ–™æ¬„ä½"""
        try:
            # è¤‡è£½åŸå§‹è³‡æ–™
            completed_data = data.copy()
            
            # å¾æª”æ¡ˆåç¨±è§£æ podcast_id å’Œ episode_title
            filename = data.get('filename', '')
            if not filename:
                logger.warning("ç¼ºå°‘ filename æ¬„ä½")
                return None
            
            # è§£ææª”æ¡ˆåç¨±æ ¼å¼ï¼šRSS_{podcast_id}_podcast_{episode_id}_{episode_title}.json
            parts = filename.replace('.json', '').split('_')
            if len(parts) < 4:
                logger.warning(f"æª”æ¡ˆåç¨±æ ¼å¼éŒ¯èª¤: {filename}")
                return None
            
            podcast_id = parts[1]  # RSS_1488295306_podcast_1321_...
            episode_title = '_'.join(parts[4:])  # å‰©é¤˜éƒ¨åˆ†ä½œç‚º episode_title
            
            # å¾è³‡æ–™åº«ç²å– episode è³‡æ–™
            db_episode_data = self.get_episode_data_from_db(podcast_id, episode_title)
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ° episode è³‡æ–™ï¼Œå˜—è©¦ç²å– podcast è³‡æ–™
            if not db_episode_data:
                db_podcast_data = self.get_podcast_data_from_db(podcast_id)
            else:
                db_podcast_data = None
            
            # è£œé½Šå¿…è¦æ¬„ä½
            completed_data.update({
                'podcast_id': podcast_id,
                'episode_id': parts[3] if len(parts) > 3 else 'unknown',
                'episode_title': episode_title,
                'podcast_name': (db_episode_data.get('podcast_name') if db_episode_data 
                               else db_podcast_data.get('podcast_name') if db_podcast_data
                               else self.podcast_name_cache.get(podcast_id, f"Podcast_{podcast_id}")),
                'audio_url': (db_episode_data.get('audio_url') if db_episode_data 
                            else f"http://192.168.32.66:30090/business-one-min-audio/RSS_{podcast_id}_{episode_title}.mp3"),
                'image_url': (db_episode_data.get('image_url') if db_episode_data 
                            else db_podcast_data.get('image_url') if db_podcast_data
                            else f"http://192.168.32.66:30090/podcast-images/RSS_{podcast_id}.jpg"),
                'category': (db_episode_data.get('category') if db_episode_data 
                           else db_podcast_data.get('category') if db_podcast_data
                           else 'business'),
                'rss_id': f"RSS_{podcast_id}",
                'tags': (db_episode_data.get('tags') if db_episode_data 
                        else self.podcast_tags_cache.get(podcast_id, []))
            })
            
            # ç¢ºä¿ tags æ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(completed_data['tags'], str):
                completed_data['tags'] = [completed_data['tags']]
            elif not isinstance(completed_data['tags'], list):
                completed_data['tags'] = []
            
            # ç¢ºä¿å…¶ä»–æ¬„ä½å‹æ…‹æ­£ç¢º
            completed_data['chunk_id'] = str(completed_data.get('chunk_id', ''))
            completed_data['chunk_index'] = int(completed_data.get('chunk_index', 0))
            completed_data['start_time'] = float(completed_data.get('start_time', 0.0))
            completed_data['end_time'] = float(completed_data.get('end_time', 0.0))
            completed_data['duration'] = float(completed_data.get('duration', 0.0))
            
            # è™•ç† content æ¬„ä½ - å„ªå…ˆä½¿ç”¨ chunk_text
            content = completed_data.get('content', '')
            if not content and 'chunk_text' in completed_data:
                content = completed_data['chunk_text']
            completed_data['content'] = str(content).strip()
            
            # ç¢ºä¿æ‰€æœ‰å¿…è¦æ¬„ä½éƒ½æœ‰å€¼
            required_fields = [
                'chunk_id', 'podcast_id', 'episode_id', 'episode_title',
                'podcast_name', 'content', 'tags', 'chunk_index',
                'start_time', 'end_time', 'duration', 'audio_url',
                'image_url', 'rss_id', 'category'
            ]
            
            for field in required_fields:
                if field not in completed_data or completed_data[field] is None:
                    # ç‚ºç¼ºå¤±çš„æ¬„ä½æä¾›é è¨­å€¼
                    if field == 'chunk_id':
                        completed_data[field] = str(completed_data.get('chunk_id', ''))
                    elif field == 'podcast_id':
                        completed_data[field] = podcast_id
                    elif field == 'episode_id':
                        completed_data[field] = parts[3] if len(parts) > 3 else 'unknown'
                    elif field == 'episode_title':
                        completed_data[field] = episode_title
                    elif field == 'podcast_name':
                        completed_data[field] = self.podcast_name_cache.get(podcast_id, f"Podcast_{podcast_id}")
                    elif field == 'content':
                        completed_data[field] = str(completed_data.get('chunk_text', '')).strip()
                    elif field == 'tags':
                        completed_data[field] = self.podcast_tags_cache.get(podcast_id, [])
                    elif field == 'chunk_index':
                        completed_data[field] = int(completed_data.get('chunk_index', 0))
                    elif field == 'start_time':
                        completed_data[field] = float(completed_data.get('start_time', 0.0))
                    elif field == 'end_time':
                        completed_data[field] = float(completed_data.get('end_time', 0.0))
                    elif field == 'duration':
                        completed_data[field] = float(completed_data.get('duration', 0.0))
                    elif field == 'audio_url':
                        completed_data[field] = f"http://192.168.32.66:30090/business-one-min-audio/RSS_{podcast_id}_{episode_title}.mp3"
                    elif field == 'image_url':
                        completed_data[field] = f"http://192.168.32.66:30090/podcast-images/RSS_{podcast_id}.jpg"
                    elif field == 'rss_id':
                        completed_data[field] = f"RSS_{podcast_id}"
                    elif field == 'category':
                        completed_data[field] = 'business'
            
            return completed_data
            
        except Exception as e:
            logger.error(f"è³‡æ–™è£œé½Šå¤±æ•—: {e}")
            return None


class BatchDataCompleter:
    """æ‰¹æ¬¡è³‡æ–™è£œé½Šè™•ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰¹æ¬¡è™•ç†å™¨"""
        self.data_completer = DataCompleter()
        self.stage3_dir = Path("backend/vector_pipeline/data/stage3_tagging")
        self.stage4_dir = Path("backend/vector_pipeline/data/stage4_embedding_prep")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.stage4_dir.mkdir(parents=True, exist_ok=True)
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "success_files": 0,
            "error_files": 0,
            "skipped_files": 0,
            "error_details": []  # è¨˜éŒ„è©³ç´°éŒ¯èª¤è³‡è¨Š
        }
    
    def get_all_json_files(self) -> List[Path]:
        """ç²å–æ‰€æœ‰éœ€è¦è™•ç†çš„ JSON æª”æ¡ˆ"""
        json_files = []
        
        if not self.stage3_dir.exists():
            logger.error(f"ç›®éŒ„ä¸å­˜åœ¨: {self.stage3_dir}")
            return json_files
        
        # éæ­·æ‰€æœ‰å­ç›®éŒ„
        for rss_dir in self.stage3_dir.iterdir():
            if not rss_dir.is_dir():
                continue
            
            logger.info(f"æƒæç›®éŒ„: {rss_dir.name}")
            
            # ç²å–è©²ç›®éŒ„ä¸‹çš„æ‰€æœ‰ JSON æª”æ¡ˆ
            for json_file in rss_dir.glob("*.json"):
                json_files.append(json_file)
        
        logger.info(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
        return json_files
    
    def process_single_file(self, json_file: Path) -> bool:
        """è™•ç†å–®å€‹æª”æ¡ˆ"""
        try:
            logger.info(f"è™•ç†æª”æ¡ˆ: {json_file}")
            
            # è®€å–åŸå§‹è³‡æ–™
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ chunks æ¬„ä½
            if 'chunks' not in data or not data['chunks']:
                logger.warning(f"æª”æ¡ˆæ²’æœ‰ chunks æ¬„ä½æˆ–ç‚ºç©º: {json_file}")
                return False
            
            # è™•ç†æ¯å€‹ chunk
            completed_chunks = []
            for chunk in data['chunks']:
                # å°‡ chunk è³‡æ–™èˆ‡æª”æ¡ˆç´šåˆ¥çš„è³‡æ–™åˆä½µ
                chunk_data = {
                    **chunk,
                    'filename': data.get('filename', ''),
                    'episode_id': data.get('episode_id', ''),
                    'collection_name': data.get('collection_name', ''),
                    'total_chunks': data.get('total_chunks', 0)
                }
                
                # è£œé½Š chunk è³‡æ–™
                completed_chunk = self.data_completer.complete_data(chunk_data)
                if completed_chunk:
                    completed_chunks.append(completed_chunk)
            
            if not completed_chunks:
                logger.warning(f"æ²’æœ‰æˆåŠŸè£œé½Šçš„ chunks: {json_file}")
                return False
            
            # å»ºç«‹è¼¸å‡ºç›®éŒ„çµæ§‹
            rss_dir_name = json_file.parent.name
            output_dir = self.stage4_dir / rss_dir_name
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # è¼¸å‡ºè£œé½Šå¾Œçš„è³‡æ–™
            output_file = output_dir / json_file.name
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(completed_chunks, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æˆåŠŸè™•ç†: {json_file.name} ({len(completed_chunks)} chunks)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è™•ç†å¤±æ•— {json_file}: {e}")
            return False
    
    def validate_completed_data(self, json_file: Path) -> Dict[str, Any]:
        """é©—è­‰è£œé½Šå¾Œçš„è³‡æ–™"""
        result = {
            "valid": True,
            "errors": [],
            "warnings": []
        }
        
        try:
            # è®€å–è£œé½Šå¾Œçš„è³‡æ–™
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé™£åˆ—
            if not isinstance(data, list):
                result["errors"].append(f"è³‡æ–™ä¸æ˜¯é™£åˆ—æ ¼å¼")
                result["valid"] = False
                return result
            
            if not data:
                result["errors"].append(f"è³‡æ–™é™£åˆ—ç‚ºç©º")
                result["valid"] = False
                return result
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_fields = [
                'chunk_id', 'podcast_id', 'episode_id', 'episode_title',
                'podcast_name', 'content', 'tags', 'chunk_index',
                'start_time', 'end_time', 'duration', 'audio_url',
                'image_url', 'rss_id', 'category'
            ]
            
            # æª¢æŸ¥ç¬¬ä¸€å€‹ chunk çš„æ¬„ä½
            first_chunk = data[0]
            missing_fields = []
            for field in required_fields:
                if field not in first_chunk or first_chunk[field] is None:
                    missing_fields.append(field)
            
            if missing_fields:
                result["errors"].append(f"ç¼ºå°‘æ¬„ä½: {missing_fields}")
                result["valid"] = False
            
            # æª¢æŸ¥è³‡æ–™å‹æ…‹
            if not isinstance(first_chunk.get('tags', []), list):
                result["errors"].append(f"tags æ¬„ä½å‹æ…‹éŒ¯èª¤: {type(first_chunk.get('tags'))}")
                result["valid"] = False
            
            if not isinstance(first_chunk.get('content', ''), str) or len(first_chunk.get('content', '').strip()) == 0:
                result["errors"].append(f"content æ¬„ä½ç‚ºç©º")
                result["valid"] = False
            
            # æª¢æŸ¥æ‰€æœ‰ chunks éƒ½æœ‰ content
            empty_content_count = 0
            for chunk in data:
                if not isinstance(chunk.get('content', ''), str) or len(chunk.get('content', '').strip()) == 0:
                    empty_content_count += 1
            
            if empty_content_count > 0:
                result["warnings"].append(f"æœ‰ {empty_content_count} å€‹ chunks çš„ content æ¬„ä½ç‚ºç©º")
            
            return result
            
        except Exception as e:
            result["errors"].append(f"é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            result["valid"] = False
            return result
    
    def run_batch_processing(self) -> None:
        """åŸ·è¡Œæ‰¹æ¬¡è™•ç†"""
        logger.info("ğŸš€ é–‹å§‹æ‰¹æ¬¡è³‡æ–™è£œé½Šè™•ç†")
        
        # ç²å–æ‰€æœ‰æª”æ¡ˆ
        json_files = self.get_all_json_files()
        self.stats["total_files"] = len(json_files)
        
        if not json_files:
            logger.warning("æ²’æœ‰æ‰¾åˆ°éœ€è¦è™•ç†çš„æª”æ¡ˆ")
            return
        
        # æ‰¹æ¬¡è™•ç†
        for i, json_file in enumerate(json_files, 1):
            logger.info(f"é€²åº¦: {i}/{len(json_files)} - {json_file.name}")
            
            try:
                # æª¢æŸ¥æ˜¯å¦å·²ç¶“è™•ç†é
                rss_dir_name = json_file.parent.name
                output_file = self.stage4_dir / rss_dir_name / json_file.name
                
                if output_file.exists():
                    logger.info(f"æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³é: {json_file.name}")
                    self.stats["skipped_files"] += 1
                    continue
                
                # è™•ç†æª”æ¡ˆ
                success = self.process_single_file(json_file)
                self.stats["processed_files"] += 1
                
                if success:
                    # é©—è­‰è™•ç†çµæœ
                    validation_result = self.validate_completed_data(output_file)
                    if validation_result["valid"]:
                        self.stats["success_files"] += 1
                        logger.info(f"âœ… é©—è­‰é€šé: {json_file.name}")
                    else:
                        self.stats["error_files"] += 1
                        error_info = {
                            "file": str(json_file),
                            "errors": validation_result["errors"],
                            "warnings": validation_result["warnings"]
                        }
                        self.stats["error_details"].append(error_info)
                        logger.error(f"âŒ é©—è­‰å¤±æ•—: {json_file.name} - {validation_result['errors']}")
                else:
                    self.stats["error_files"] += 1
                    error_info = {
                        "file": str(json_file),
                        "errors": ["è™•ç†å¤±æ•—"],
                        "warnings": []
                    }
                    self.stats["error_details"].append(error_info)
                
            except Exception as e:
                logger.error(f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ {json_file}: {e}")
                self.stats["error_files"] += 1
        
        # è¼¸å‡ºçµ±è¨ˆçµæœ
        self.print_statistics()
    
    def print_statistics(self) -> None:
        """è¼¸å‡ºçµ±è¨ˆçµæœ"""
        logger.info("=" * 50)
        logger.info("ğŸ“Š æ‰¹æ¬¡è™•ç†çµ±è¨ˆçµæœ")
        logger.info("=" * 50)
        logger.info(f"ç¸½æª”æ¡ˆæ•¸: {self.stats['total_files']}")
        logger.info(f"å·²è™•ç†æª”æ¡ˆ: {self.stats['processed_files']}")
        logger.info(f"æˆåŠŸè™•ç†: {self.stats['success_files']}")
        logger.info(f"è™•ç†å¤±æ•—: {self.stats['error_files']}")
        logger.info(f"è·³éæª”æ¡ˆ: {self.stats['skipped_files']}")
        
        if self.stats['total_files'] > 0:
            success_rate = (self.stats['success_files'] / self.stats['total_files']) * 100
            logger.info(f"æˆåŠŸç‡: {success_rate:.2f}%")
        
        # è¨˜éŒ„éŒ¯èª¤è©³æƒ…åˆ°æª”æ¡ˆ
        if self.stats['error_details']:
            error_log_file = "batch_processing_errors.json"
            with open(error_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats['error_details'], f, ensure_ascii=False, indent=2)
            logger.info(f"éŒ¯èª¤è©³æƒ…å·²è¨˜éŒ„åˆ°: {error_log_file}")
            logger.info(f"éŒ¯èª¤æª”æ¡ˆæ•¸é‡: {len(self.stats['error_details'])}")
        
        logger.info("=" * 50)


def main():
    """ä¸»å‡½æ•¸"""
    try:
        # åˆå§‹åŒ–æ‰¹æ¬¡è™•ç†å™¨
        batch_processor = BatchDataCompleter()
        
        # åŸ·è¡Œæ‰¹æ¬¡è™•ç†
        batch_processor.run_batch_processing()
        
        logger.info("ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 