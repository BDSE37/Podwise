#!/usr/bin/env python3
"""
RSS è™•ç†å™¨
å°ˆé–€è™•ç† MongoDB RSS collectionsï¼ŒåŒ…å« RSS_1500839292 çš„ä¾‹å¤–è™•ç†

åŠŸèƒ½ï¼š
1. å¾ MongoDB å–å¾— RSS_XXXXXX collections
2. ä½¿ç”¨ data_cleaning æ¨¡çµ„é€²è¡Œè³‡æ–™æ¸…ç†
3. å…§å®¹ä»¥ç©ºç™½æˆ–æ›è¡Œåˆ‡æ–·
4. ç‚ºæ¯å€‹ chunk æä¾› 1-3 å€‹ TAG
5. ä½¿ç”¨ bge-m3 é€²è¡Œ embedding
6. å­˜å…¥ Milvus
"""

import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from vector_pipeline.pipeline_orchestrator import PipelineOrchestrator
from vector_pipeline.core import UnifiedTagManager
from vector_pipeline.error_logger import ErrorLogger

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RSSProcessor:
    """RSS è™•ç†å™¨ - æ•´åˆ data_cleaning æ¸…ç†åŠŸèƒ½"""
    
    def __init__(self, 
                 mongo_config: Dict[str, Any],
                 postgres_config: Dict[str, Any],
                 milvus_config: Dict[str, Any],
                 tag_csv_path: str = "../rag_pipeline/scripts/csv/TAG_info.csv"):
        """
        åˆå§‹åŒ– RSS è™•ç†å™¨
        
        Args:
            mongo_config: MongoDB é…ç½®
            postgres_config: PostgreSQL é…ç½®
            milvus_config: Milvus é…ç½®
            tag_csv_path: TAG_info.csv æª”æ¡ˆè·¯å¾‘
        """
        self.mongo_config = mongo_config
        self.postgres_config = postgres_config
        self.milvus_config = milvus_config
        self.tag_csv_path = tag_csv_path
        
        # åˆå§‹åŒ–çµ±ä¸€æ¨™ç±¤ç®¡ç†å™¨
        self.tag_manager = UnifiedTagManager(tag_csv_path)
        
        # åˆå§‹åŒ– Pipeline Orchestrator
        self.orchestrator = PipelineOrchestrator(
            mongo_config=mongo_config,
            postgres_config=postgres_config,
            milvus_config=milvus_config,
            tag_csv_path=tag_csv_path,
            embedding_model="BAAI/bge-m3",
            max_chunk_size=1024,
            batch_size=50
        )
        
        logger.info("RSS è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_rss_collections(self) -> List[str]:
        """
        ç²å–æ‰€æœ‰ RSS collections
        
        Returns:
            RSS collections åˆ—è¡¨
        """
        try:
            collections = self.orchestrator.mongo_processor.get_collections()
            rss_collections = [col for col in collections if col.startswith('RSS_')]
            
            logger.info(f"æ‰¾åˆ° {len(rss_collections)} å€‹ RSS collections: {rss_collections}")
            return rss_collections
            
        except Exception as e:
            logger.error(f"ç²å– RSS collections å¤±æ•—: {e}")
            return []
    
    def process_rss_collection(self, collection_name: str, limit: Optional[int] = None) -> Dict[str, Any]:
        """
        è™•ç†å–®å€‹ RSS collection
        
        Args:
            collection_name: Collection åç¨±
            limit: é™åˆ¶è™•ç†æ–‡æª”æ•¸é‡
            
        Returns:
            è™•ç†çµæœçµ±è¨ˆ
        """
        try:
            logger.info(f"é–‹å§‹è™•ç† RSS collection: {collection_name}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º RSS_1500839292ï¼ˆéœ€è¦ä¾‹å¤–è™•ç†ï¼‰
            is_special_collection = collection_name == "RSS_1500839292"
            
            if is_special_collection:
                logger.info("æª¢æ¸¬åˆ° RSS_1500839292ï¼Œä½¿ç”¨è‚¡ç™Œå°ˆç”¨æ¸…ç†å™¨")
                return self._process_special_collection(collection_name, limit)
            else:
                logger.info(f"ä½¿ç”¨æ¨™æº–è™•ç†é‚è¼¯è™•ç† {collection_name}")
                return self._process_standard_collection(collection_name, limit)
                
        except Exception as e:
            logger.error(f"è™•ç† RSS collection {collection_name} å¤±æ•—: {e}")
            return {
                "collection_name": collection_name,
                "status": "failed",
                "error": str(e),
                "processed_documents": 0,
                "processed_chunks": 0
            }
    
    def _process_standard_collection(self, collection_name: str, limit: Optional[int] = None) -> Dict[str, Any]:
        """è™•ç†æ¨™æº– RSS collection"""
        try:
            # ä½¿ç”¨æ¨™æº–çš„ Pipeline Orchestrator è™•ç†
            # MongoDBProcessor æœƒè‡ªå‹•ä½¿ç”¨é©ç•¶çš„æ¸…ç†å™¨
            stats = self.orchestrator.process_collection(
                mongo_collection=collection_name,
                milvus_collection="podcast_chunks",
                limit=limit
            )
            
            return {
                "collection_name": collection_name,
                "status": "success",
                "processed_documents": stats.get("processed_documents", 0),
                "processed_chunks": stats.get("processed_chunks", 0),
                "embedding_model": "BAAI/bge-m3",
                "processing_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"æ¨™æº–è™•ç†å¤±æ•—: {e}")
            raise
    
    def _process_special_collection(self, collection_name: str, limit: Optional[int] = None) -> Dict[str, Any]:
        """è™•ç† RSS_1500839292 ç‰¹æ®Š collection"""
        try:
            logger.info("é–‹å§‹ç‰¹æ®Šè™•ç† RSS_1500839292")
            
            # ç²å–æ–‡æª”ï¼ˆMongoDBProcessor æœƒè‡ªå‹•ä½¿ç”¨è‚¡ç™Œæ¸…ç†å™¨ï¼‰
            documents = self.orchestrator.mongo_processor.process_collection(
                collection_name, 
                limit=limit
            )
            
            processed_documents = 0
            processed_chunks = 0
            
            for doc in documents:
                try:
                    # è™•ç†æ–‡æª”ï¼ˆæ¸…ç†å·²åœ¨ process_collection ä¸­å®Œæˆï¼‰
                    chunk_count = self.orchestrator.process_single_document(doc)
                    
                    if chunk_count > 0:
                        processed_documents += 1
                        processed_chunks += chunk_count
                        logger.info(f"è™•ç†æ–‡æª” {doc.episode_id}: {chunk_count} chunks")
                    
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡æª” {doc.episode_id} å¤±æ•—: {e}")
                    continue
            
            return {
                "collection_name": collection_name,
                "status": "success",
                "processed_documents": processed_documents,
                "processed_chunks": processed_chunks,
                "embedding_model": "BAAI/bge-m3",
                "processing_time": datetime.now().isoformat(),
                "special_processing": True
            }
            
        except Exception as e:
            logger.error(f"ç‰¹æ®Šè™•ç†å¤±æ•—: {e}")
            raise
    
    def process_all_rss_collections(self, limit_per_collection: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è™•ç†æ‰€æœ‰ RSS collections
        
        Args:
            limit_per_collection: æ¯å€‹ collection çš„é™åˆ¶æ•¸é‡
            
        Returns:
            æ‰€æœ‰è™•ç†çµæœ
        """
        try:
            rss_collections = self.get_rss_collections()
            
            if not rss_collections:
                logger.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½• RSS collections")
                return []
            
            all_results = []
            error_logger = ErrorLogger()
            
            for collection_name in rss_collections:
                try:
                    logger.info(f"é–‹å§‹è™•ç† collection: {collection_name}")
                    result = self.process_rss_collection(collection_name, limit_per_collection)
                    all_results.append(result)
                    
                    logger.info(f"Collection {collection_name} è™•ç†å®Œæˆ: {result}")
                    
                except Exception as e:
                    logger.error(f"è™•ç† collection {collection_name} å¤±æ•—: {e}")
                    
                    # æå– RSS ID
                    rss_id = self._extract_rss_id_from_collection(collection_name)
                    
                    # è¨˜éŒ„éŒ¯èª¤
                    error_logger.add_error(
                        collection_id=collection_name,
                        rss_id=rss_id,
                        title=f"Collection: {collection_name}",
                        error_type="collection_processing_error",
                        error_message=str(e),
                        error_details=f"Collection: {collection_name}, RSS_ID: {rss_id}",
                        processing_stage="collection_processing"
                    )
                    
                    # æ·»åŠ å¤±æ•—çµæœä½†ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹
                    all_results.append({
                        "collection_name": collection_name,
                        "status": "failed",
                        "error": str(e),
                        "processed_documents": 0,
                        "processed_chunks": 0,
                        "rss_id": rss_id
                    })
            
            # å„²å­˜éŒ¯èª¤å ±å‘Š
            if error_logger.errors:
                error_reports = error_logger.save_errors()
                csv_report = error_logger.save_csv_report()
                logger.warning(f"è™•ç†éç¨‹ä¸­ç™¼ç¾ {len(error_logger.errors)} å€‹éŒ¯èª¤")
                logger.warning(f"JSON éŒ¯èª¤å ±å‘Š: {error_reports}")
                logger.warning(f"CSV éŒ¯èª¤å ±å‘Š: {csv_report}")
                
                # è¼¸å‡ºéŒ¯èª¤æ‘˜è¦
                self._print_error_summary(error_logger)
            
            return all_results
            
        except Exception as e:
            logger.error(f"è™•ç†æ‰€æœ‰ RSS collections å¤±æ•—: {e}")
            return []
    
    def _extract_rss_id_from_collection(self, collection_name: str) -> str:
        """å¾ collection åç¨±æå– RSS ID"""
        import re
        match = re.search(r'RSS_(\d+)', collection_name)
        return match.group(1) if match else ""
    
    def _print_error_summary(self, error_logger: ErrorLogger) -> None:
        """è¼¸å‡ºéŒ¯èª¤æ‘˜è¦"""
        try:
            summary = error_logger.get_error_summary()
            
            print("\n" + "="*80)
            print("ğŸš¨ éŒ¯èª¤è™•ç†æ‘˜è¦")
            print("="*80)
            print(f"ğŸ“Š ç¸½éŒ¯èª¤æ•¸: {summary['total_errors']}")
            print(f"ğŸ“ å—å½±éŸ¿çš„ Collections: {summary['collections_affected']}")
            print(f"ğŸ”§ è™•ç†éšæ®µ: {', '.join(summary['processing_stages'])}")
            
            if summary['error_types']:
                print("\nğŸ“‹ éŒ¯èª¤é¡å‹çµ±è¨ˆ:")
                for error_type, count in summary['error_types'].items():
                    print(f"   â€¢ {error_type}: {count} å€‹")
            
            # é¡¯ç¤ºå‰ 10 å€‹éŒ¯èª¤çš„ RSS_ID å’Œæ¨™é¡Œ
            if error_logger.errors:
                print("\nğŸ“ éŒ¯èª¤æª”æ¡ˆæ¸…å–® (å‰ 10 å€‹):")
                for i, error in enumerate(error_logger.errors[:10]):
                    print(f"   {i+1}. RSS_{error.rss_id} - {error.title}")
                
                if len(error_logger.errors) > 10:
                    print(f"   ... é‚„æœ‰ {len(error_logger.errors) - 10} å€‹éŒ¯èª¤")
            
            print("="*80)
            
        except Exception as e:
            logger.error(f"è¼¸å‡ºéŒ¯èª¤æ‘˜è¦å¤±æ•—: {e}")
    
    def get_error_report(self) -> Dict[str, Any]:
        """ç²å–éŒ¯èª¤å ±å‘Š"""
        try:
            error_logger = ErrorLogger()
            summary = error_logger.get_error_summary()
            
            # ç²å–éŒ¯èª¤æª”æ¡ˆæ¸…å–®
            error_files = []
            for error in error_logger.errors:
                error_files.append({
                    "rss_id": error.rss_id,
                    "title": error.title,
                    "collection_id": error.collection_id,
                    "error_type": error.error_type,
                    "processing_stage": error.processing_stage,
                    "timestamp": error.timestamp
                })
            
            return {
                "summary": summary,
                "error_files": error_files,
                "total_error_files": len(error_files)
            }
            
        except Exception as e:
            logger.error(f"ç²å–éŒ¯èª¤å ±å‘Šå¤±æ•—: {e}")
            return {}
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ç²å–è™•ç†çµ±è¨ˆè³‡è¨Š"""
        try:
            rss_collections = self.get_rss_collections()
            
            stats = {
                "total_rss_collections": len(rss_collections),
                "rss_collections": rss_collections,
                "tag_manager_status": self.tag_manager.get_extractor_status(),
                "embedding_model": "BAAI/bge-m3",
                "max_chunk_size": 1024,
                "special_collections": ["RSS_1500839292"],
                "data_cleaning_integration": True
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"ç²å–çµ±è¨ˆè³‡è¨Šå¤±æ•—: {e}")
            return {}
    
    def close(self) -> None:
        """é—œé–‰é€£æ¥"""
        try:
            if self.orchestrator:
                self.orchestrator.close()
            logger.info("RSS è™•ç†å™¨å·²é—œé–‰")
        except Exception as e:
            logger.error(f"é—œé–‰ RSS è™•ç†å™¨å¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    # é…ç½®
    mongo_config = {
        "host": "192.168.32.86",
        "port": 30017,
        "username": "bdse37",
        "password": "111111",
        "database": "podcast"
    }
    
    postgres_config = {
        "host": "192.168.32.56",
        "port": 32432,
        "database": "podcast",
        "user": "bdse37",
        "password": "111111"
    }
    
    milvus_config = {
        "host": "192.168.32.86",
        "port": "19530",
        "collection_name": "podcast_chunks",
        "dim": 1024,
        "index_type": "IVF_FLAT",
        "metric_type": "L2",
        "params": {"nlist": 1024}
    }
    
    try:
        # å‰µå»º RSS è™•ç†å™¨
        processor = RSSProcessor(mongo_config, postgres_config, milvus_config)
        
        # ç²å–çµ±è¨ˆè³‡è¨Š
        stats = processor.get_processing_statistics()
        logger.info(f"RSS è™•ç†çµ±è¨ˆ: {stats}")
        
        # è™•ç†æ‰€æœ‰ RSS collections
        results = processor.process_all_rss_collections(limit_per_collection=None)
        
        # è¼¸å‡ºçµæœ
        print("\n" + "="*80)
        print("RSS Collections è™•ç†çµæœ")
        print("="*80)
        
        total_processed = 0
        total_chunks = 0
        
        for result in results:
            print(f"\nğŸ“‹ Collection: {result['collection_name']}")
            print(f"   Status: {result['status']}")
            
            if result['status'] == 'success':
                print(f"   ğŸ“„ è™•ç†æ–‡æª”: {result.get('processed_documents', 0)}")
                print(f"   ğŸ§© è™•ç† Chunks: {result.get('processed_chunks', 0)}")
                print(f"   ğŸ”§ ç‰¹æ®Šè™•ç†: {result.get('special_processing', False)}")
                
                total_processed += result.get('processed_documents', 0)
                total_chunks += result.get('processed_chunks', 0)
            else:
                print(f"   âŒ éŒ¯èª¤: {result.get('error', 'Unknown error')}")
        
        print(f"\nğŸ“Š ç¸½è¨ˆ:")
        print(f"   ğŸ“„ ç¸½è™•ç†æ–‡æª”: {total_processed}")
        print(f"   ğŸ§© ç¸½è™•ç† Chunks: {total_chunks}")
        print(f"   ğŸ—ï¸  åµŒå…¥æ¨¡å‹: BAAI/bge-m3")
        print(f"   ğŸ§¹ è³‡æ–™æ¸…ç†: æ•´åˆ data_cleaning æ¨¡çµ„")
        print("="*80)
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºåŸ·è¡Œå¤±æ•—: {e}")
        raise
    finally:
        # é—œé–‰è™•ç†å™¨
        if 'processor' in locals():
            processor.close()


if __name__ == "__main__":
    main() 