#!/usr/bin/env python3
"""
è‡ªå‹•åŒ–æ‰¹æ¬¡è™•ç†è…³æœ¬
è™•ç†æ‰€æœ‰ MongoDB collectionsï¼Œåˆ‡åˆ†ç‚º chunksï¼Œè²¼æ¨™ç±¤ï¼Œå‘é‡åŒ–å¾Œå­˜å…¥ podcast_chunks
åŒ…å«ç•°å¸¸è™•ç†ã€é‡è¤‡è³‡æ–™æª¢æŸ¥ã€é€²åº¦è¨˜éŒ„

éµå¾ª Google Clean Code Style å’Œ OOP åŸå‰‡
"""

import logging
import sys
import time
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from tqdm import tqdm
from abc import ABC, abstractmethod

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_process.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class ProcessingStats:
    """è™•ç†çµ±è¨ˆè³‡æ–™"""
    collection_name: str
    total_documents: int
    processed_documents: int
    failed_documents: int
    total_chunks: int
    processed_chunks: int
    failed_chunks: int
    start_time: datetime
    end_time: Optional[datetime] = None
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
    
    @property
    def duration(self) -> float:
        """è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰"""
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return (datetime.now() - self.start_time).total_seconds()
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_documents == 0:
            return 0.0
        return (self.processed_documents / self.total_documents) * 100


class ProgressManager(ABC):
    """é€²åº¦ç®¡ç†å™¨æŠ½è±¡é¡åˆ¥"""
    
    @abstractmethod
    def load_progress(self) -> Dict[str, Any]:
        """è¼‰å…¥é€²åº¦è¨˜éŒ„"""
        pass
    
    @abstractmethod
    def save_progress(self, **kwargs) -> None:
        """å„²å­˜é€²åº¦è¨˜éŒ„"""
        pass
    
    @abstractmethod
    def mark_collection_completed(self, collection_name: str) -> None:
        """æ¨™è¨˜ collection å®Œæˆ"""
        pass


class FileProgressManager(ProgressManager):
    """æª”æ¡ˆå‹é€²åº¦ç®¡ç†å™¨"""
    
    def __init__(self, progress_file: str = "batch_progress.json"):
        self.progress_file = progress_file
        self.progress = self.load_progress()
    
    def load_progress(self) -> Dict[str, Any]:
        """è¼‰å…¥é€²åº¦è¨˜éŒ„"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    logger.info(f"ğŸ“‹ è¼‰å…¥é€²åº¦è¨˜éŒ„: {progress}")
                    return progress
            except Exception as e:
                logger.warning(f"è¼‰å…¥é€²åº¦è¨˜éŒ„å¤±æ•—: {e}")
        
        # é è¨­é€²åº¦
        return {
            "last_processed_collection": None,
            "last_processed_doc": None,
            "completed_collections": [],
            "total_processed_chunks": 0,
            "start_time": datetime.now().isoformat(),
            "last_update": datetime.now().isoformat(),
            "cycle_count": 0,
            "current_cycle": 0
        }
    
    def save_progress(self, **kwargs) -> None:
        """å„²å­˜é€²åº¦è¨˜éŒ„"""
        try:
            for key, value in kwargs.items():
                if key in self.progress:
                    self.progress[key] = value
            
            self.progress["last_update"] = datetime.now().isoformat()
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ’¾ é€²åº¦å·²å„²å­˜: {self.progress}")
        except Exception as e:
            logger.error(f"å„²å­˜é€²åº¦å¤±æ•—: {e}")
    
    def mark_collection_completed(self, collection_name: str) -> None:
        """æ¨™è¨˜ collection å®Œæˆ"""
        if collection_name not in self.progress["completed_collections"]:
            self.progress["completed_collections"].append(collection_name)
            self.save_progress()
            logger.info(f"âœ… Collection {collection_name} å·²æ¨™è¨˜ç‚ºå®Œæˆ")
    
    def should_skip_collection(self, collection_name: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²è·³é collection"""
        return collection_name in self.progress["completed_collections"]
    
    def get_current_cycle(self) -> int:
        """ç²å–ç•¶å‰å¾ªç’°æ•¸"""
        return self.progress.get("current_cycle", 0)
    
    def increment_cycle(self) -> None:
        """å¢åŠ å¾ªç’°æ•¸"""
        self.progress["current_cycle"] = self.progress.get("current_cycle", 0) + 1
        self.progress["cycle_count"] = self.progress.get("cycle_count", 0) + 1
        self.save_progress()


class MetadataValidator:
    """Metadata é©—è­‰å™¨"""
    
    @staticmethod
    def is_metadata_complete(metadata) -> bool:
        """æª¢æŸ¥ metadata å®Œæ•´æ€§"""
        try:
            # æª¢æŸ¥å¿…éœ€æ¬„ä½ï¼ˆé™¤äº† tag æ¬„ä½å¤–ï¼‰
            required_fields = [
                'episode_id',
                'podcast_id', 
                'episode_title',
                'podcast_name',
                'author',
                'category'
            ]
            
            for field in required_fields:
                value = getattr(metadata, field, None)
                if value is None or (isinstance(value, str) and not value.strip()):
                    logger.warning(f"metadata æ¬„ä½ {field} ç‚ºç©ºæˆ– None")
                    return False
            
            # æª¢æŸ¥ episode_id å’Œ podcast_id ä¸ç‚º 0
            if metadata.episode_id == 0 or metadata.podcast_id == 0:
                logger.warning(f"episode_id æˆ– podcast_id ç‚º 0")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥ metadata å®Œæ•´æ€§å¤±æ•—: {e}")
            return False


class CollectionProcessor:
    """Collection è™•ç†å™¨"""
    
    def __init__(self, orchestrator, progress_manager: ProgressManager):
        self.orchestrator = orchestrator
        self.progress_manager = progress_manager
        self.processed_chunks = set()
        self.metadata_validator = MetadataValidator()
    
    def process_collection(self, collection_name: str, limit: Optional[int] = None) -> ProcessingStats:
        """è™•ç†å–®å€‹ collection"""
        logger.info(f"ğŸ”„ é–‹å§‹è™•ç† collection: {collection_name}")
        
        # æª¢æŸ¥æ˜¯å¦å·²å®Œæˆ
        if self.progress_manager.should_skip_collection(collection_name):
            logger.info(f"â­ï¸ è·³éå·²å®Œæˆçš„ collection: {collection_name}")
            return ProcessingStats(
                collection_name=collection_name,
                total_documents=0,
                processed_documents=0,
                failed_documents=0,
                total_chunks=0,
                processed_chunks=0,
                failed_chunks=0,
                start_time=datetime.now()
            )
        
        # ç²å–ç¸½æ–‡æª”æ•¸
        total_docs = self.orchestrator.mongo_processor.get_document_count(collection_name)
        if limit:
            total_docs = min(total_docs, limit)
        
        logger.info(f"ğŸ“Š Collection {collection_name} ç¸½æ–‡æª”æ•¸: {total_docs}")
        
        # å»ºç«‹çµ±è¨ˆç‰©ä»¶
        stats = ProcessingStats(
            collection_name=collection_name,
            total_documents=total_docs,
            processed_documents=0,
            failed_documents=0,
            total_chunks=0,
            processed_chunks=0,
            failed_chunks=0,
            start_time=datetime.now()
        )
        
        batch_size = 50  # æ‰¹æ¬¡å¤§å°
        
        try:
            # å»ºç«‹é€²åº¦æ¢
            with tqdm(total=total_docs, desc=f"è™•ç† {collection_name}", unit="æ–‡æª”") as pbar:
                for offset in range(0, total_docs, batch_size):
                    current_batch_size = min(batch_size, total_docs - offset)
                    
                    try:
                        # ç²å–æ‰¹æ¬¡æ–‡æª”
                        docs = self.orchestrator.mongo_processor.fetch_documents(
                            collection_name, 
                            limit=current_batch_size
                        )
                        
                        # è™•ç†æ¯å€‹æ–‡æª”
                        for doc in docs:
                            self._process_single_document(doc, stats, pbar)
                        
                        # æ‰¹æ¬¡é–“éš”
                        time.sleep(1)
                        
                    except Exception as e:
                        error_msg = f"æ‰¹æ¬¡è™•ç†ç•°å¸¸: {str(e)}"
                        stats.errors.append(error_msg)
                        logger.error(f"âŒ {error_msg}")
                        
                        # å„²å­˜é€²åº¦
                        self.progress_manager.save_progress(
                            last_processed_collection=collection_name
                        )
                        
                        # å¦‚æœæ˜¯åš´é‡éŒ¯èª¤ï¼Œä¸­æ­¢è™•ç†
                        if "metadata" in str(e).lower():
                            raise e
            
            # æ¨™è¨˜ collection å®Œæˆ
            self.progress_manager.mark_collection_completed(collection_name)
            logger.info(f"âœ… Collection {collection_name} è™•ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ è™•ç† collection {collection_name} æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            # å„²å­˜é€²åº¦
            self.progress_manager.save_progress(
                last_processed_collection=collection_name
            )
            raise
        
        return stats
    
    def _process_single_document(self, doc, stats: ProcessingStats, pbar: tqdm) -> None:
        """è™•ç†å–®å€‹æ–‡æª”"""
        try:
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
            if doc.file in self.processed_chunks:
                logger.debug(f"â­ï¸ è·³éå·²è™•ç†æ–‡æª”: {doc.file}")
                pbar.update(1)
                return
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ metadataï¼ˆæ²’æœ‰å‰‡ä¸­æ­¢ç¨‹å¼ï¼‰
            metadata = self.orchestrator._get_episode_metadata(doc)
            if not metadata:
                error_msg = f"âŒ æ–‡æª”ç„¡ metadataï¼Œç¨‹å¼ä¸­æ­¢: {doc.file}"
                logger.error(error_msg)
                stats.errors.append(error_msg)
                raise Exception(error_msg)
            
            # æª¢æŸ¥ metadata å®Œæ•´æ€§ï¼ˆé™¤äº† tag æ¬„ä½å¤–ï¼Œå…¶ä»–æ¬„ä½éƒ½è¦æœ‰å€¼ï¼‰
            if not self.metadata_validator.is_metadata_complete(metadata):
                error_msg = f"âŒ metadata ä¸å®Œæ•´ï¼Œç¨‹å¼ä¸­æ­¢: {doc.file}"
                logger.error(error_msg)
                stats.errors.append(error_msg)
                raise Exception(error_msg)
            
            # è™•ç†æ–‡æª”ï¼ˆåˆ‡åˆ†ã€æ¨™ç±¤ã€å‘é‡åŒ–ã€å¯«å…¥ï¼‰
            chunk_count = self.orchestrator.process_single_document(doc)
            
            if chunk_count > 0:
                # æ›´æ–°çµ±è¨ˆ
                stats.processed_documents += 1
                stats.processed_chunks += chunk_count
                
                # æ¨™è¨˜æ–‡æª”å·²è™•ç†
                self.processed_chunks.add(doc.file)
                
                # å„²å­˜é€²åº¦ï¼ˆæ¯å€‹æ–‡æª”è™•ç†å®Œå¾Œç«‹å³å„²å­˜ï¼‰
                self.progress_manager.save_progress(
                    last_processed_collection=stats.collection_name,
                    last_processed_doc=doc.file,
                    total_processed_chunks=self.progress_manager.progress["total_processed_chunks"] + chunk_count
                )
                
                logger.info(f"âœ… æ–‡æª”è™•ç†å®Œæˆ: {doc.file} -> {chunk_count} chunks")
            else:
                stats.failed_documents += 1
                error_msg = f"æ–‡æª”è™•ç†å¤±æ•—ï¼Œç„¡ chunks ç”¢ç”Ÿ: {doc.file}"
                stats.errors.append(error_msg)
                logger.warning(f"âš ï¸ {error_msg}")
            
            # æ›´æ–°é€²åº¦æ¢
            pbar.set_postfix({
                'æˆåŠŸ': stats.processed_documents,
                'å¤±æ•—': stats.failed_documents,
                'chunks': stats.processed_chunks
            })
            pbar.update(1)
            
        except Exception as e:
            error_msg = f"è™•ç†æ–‡æª” {doc.file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            stats.errors.append(error_msg)
            stats.failed_documents += 1
            logger.error(f"âŒ {error_msg}")
            
            # å„²å­˜é€²åº¦ï¼ˆå³ä½¿å¤±æ•—ä¹Ÿè¦å„²å­˜ï¼‰
            self.progress_manager.save_progress(
                last_processed_collection=stats.collection_name,
                last_processed_doc=doc.file
            )
            
            pbar.set_postfix({
                'æˆåŠŸ': stats.processed_documents,
                'å¤±æ•—': stats.failed_documents,
                'chunks': stats.processed_chunks
            })
            pbar.update(1)
            
            # å¦‚æœæ˜¯åš´é‡éŒ¯èª¤ï¼ˆå¦‚ metadata å•é¡Œï¼‰ï¼Œä¸­æ­¢è™•ç†
            if "metadata" in str(e).lower():
                raise e


class BatchProcessor:
    """æ‰¹æ¬¡è™•ç†å™¨ - éµå¾ª Google Clean Code Style"""
    
    def __init__(self, mongo_config: Dict[str, Any], 
                 postgres_config: Dict[str, Any], 
                 milvus_config: Dict[str, Any],
                 collections_per_cycle: int = 5):
        """
        åˆå§‹åŒ–æ‰¹æ¬¡è™•ç†å™¨
        
        Args:
            mongo_config: MongoDB é…ç½®
            postgres_config: PostgreSQL é…ç½®  
            milvus_config: Milvus é…ç½®
            collections_per_cycle: æ¯å€‹å¾ªç’°è™•ç†çš„ collections æ•¸é‡
        """
        self.mongo_config = mongo_config
        self.postgres_config = postgres_config
        self.milvus_config = milvus_config
        self.collections_per_cycle = collections_per_cycle
        
        self.orchestrator = None
        self.progress_manager = FileProgressManager()
        self.collection_processor = None
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶"""
        try:
            from vector_pipeline.pipeline_orchestrator import PipelineOrchestrator
            
            self.orchestrator = PipelineOrchestrator(
                mongo_config=self.mongo_config,
                postgres_config=self.postgres_config,
                milvus_config=self.milvus_config,
                max_chunk_size=500,
                batch_size=50
            )
            
            self.collection_processor = CollectionProcessor(
                self.orchestrator, 
                self.progress_manager
            )
            
            logger.info("âœ… æ‰¹æ¬¡è™•ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡è™•ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    def process_collections_in_cycles(self, limit_per_collection: Optional[int] = None) -> List[ProcessingStats]:
        """æŒ‰å¾ªç’°è™•ç† collections"""
        if not self.initialize():
            return []
        
        try:
            # ç²å–æ‰€æœ‰ collections
            collections = self.orchestrator.mongo_processor.get_collection_names()
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(collections)} å€‹ collections: {collections}")
            
            all_stats = []
            current_cycle = self.progress_manager.get_current_cycle()
            
            # è¨ˆç®—ç•¶å‰å¾ªç’°æ‡‰è©²è™•ç†çš„ collections
            start_index = current_cycle * self.collections_per_cycle
            end_index = min(start_index + self.collections_per_cycle, len(collections))
            
            current_collections = collections[start_index:end_index]
            
            logger.info(f"ğŸ”„ é–‹å§‹è™•ç†å¾ªç’° {current_cycle + 1}")
            logger.info(f"ğŸ“Š æœ¬å¾ªç’°è™•ç† collections: {current_collections}")
            
            # å»ºç«‹æ•´é«”é€²åº¦æ¢
            with tqdm(total=len(current_collections), desc=f"å¾ªç’° {current_cycle + 1}", unit="collection") as pbar:
                for i, collection_name in enumerate(current_collections, 1):
                    logger.info(f"ğŸ”„ è™•ç†é€²åº¦: {i}/{len(current_collections)} - {collection_name}")
                    
                    stats = self.collection_processor.process_collection(collection_name, limit_per_collection)
                    all_stats.append(stats)
                    
                    # è¼¸å‡ºçµ±è¨ˆè³‡è¨Š
                    self.print_stats(stats)
                    
                    # æ›´æ–°é€²åº¦æ¢
                    pbar.update(1)
                    pbar.set_postfix({
                        'ç•¶å‰': collection_name,
                        'æˆåŠŸæ–‡æª”': sum(s.processed_documents for s in all_stats),
                        'ç¸½chunks': sum(s.processed_chunks for s in all_stats)
                    })
                    
                    # é›†åˆé–“éš”
                    if i < len(current_collections):
                        logger.info("â³ ç­‰å¾… 3 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹ collection...")
                        time.sleep(3)
            
            # å¢åŠ å¾ªç’°è¨ˆæ•¸
            self.progress_manager.increment_cycle()
            
            # è¼¸å‡ºç¸½é«”çµ±è¨ˆ
            self.print_cycle_stats(all_stats, current_cycle + 1)
            
            return all_stats
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
            return []
        
        finally:
            if self.orchestrator:
                self.orchestrator.close()
    
    def process_all_collections(self, limit_per_collection: Optional[int] = None) -> List[ProcessingStats]:
        """è™•ç†æ‰€æœ‰ collectionsï¼ˆä¸€æ¬¡æ€§è™•ç†ï¼‰"""
        if not self.initialize():
            return []
        
        try:
            # ç²å–æ‰€æœ‰ collections
            collections = self.orchestrator.mongo_processor.get_collection_names()
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(collections)} å€‹ collections: {collections}")
            
            all_stats = []
            
            # å»ºç«‹æ•´é«”é€²åº¦æ¢
            with tqdm(total=len(collections), desc="è™•ç† Collections", unit="collection") as pbar:
                for i, collection_name in enumerate(collections, 1):
                    logger.info(f"ğŸ”„ è™•ç†é€²åº¦: {i}/{len(collections)} - {collection_name}")
                    
                    stats = self.collection_processor.process_collection(collection_name, limit_per_collection)
                    all_stats.append(stats)
                    
                    # è¼¸å‡ºçµ±è¨ˆè³‡è¨Š
                    self.print_stats(stats)
                    
                    # æ›´æ–°é€²åº¦æ¢
                    pbar.update(1)
                    pbar.set_postfix({
                        'ç•¶å‰': collection_name,
                        'æˆåŠŸæ–‡æª”': sum(s.processed_documents for s in all_stats),
                        'ç¸½chunks': sum(s.processed_chunks for s in all_stats)
                    })
                    
                    # é›†åˆé–“éš”
                    if i < len(collections):
                        logger.info("â³ ç­‰å¾… 5 ç§’å¾Œè™•ç†ä¸‹ä¸€å€‹ collection...")
                        time.sleep(5)
            
            # è¼¸å‡ºç¸½é«”çµ±è¨ˆ
            self.print_overall_stats(all_stats)
            
            return all_stats
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
            return []
        
        finally:
            if self.orchestrator:
                self.orchestrator.close()
    
    def print_stats(self, stats: ProcessingStats) -> None:
        """è¼¸å‡ºå–®å€‹ collection çµ±è¨ˆ"""
        print(f"\nğŸ“Š Collection: {stats.collection_name}")
        print(f"  ğŸ“„ æ–‡æª”: {stats.processed_documents}/{stats.total_documents} ({stats.success_rate:.1f}%)")
        print(f"  ğŸ§© Chunks: {stats.processed_chunks}/{stats.total_chunks}")
        print(f"  â±ï¸ æ™‚é–“: {stats.duration:.1f} ç§’")
        if stats.errors:
            print(f"  âŒ éŒ¯èª¤: {len(stats.errors)} å€‹")
    
    def print_cycle_stats(self, all_stats: List[ProcessingStats], cycle_number: int) -> None:
        """è¼¸å‡ºå¾ªç’°çµ±è¨ˆ"""
        print(f"\n" + "=" * 80)
        print(f"ğŸ¯ å¾ªç’° {cycle_number} çµ±è¨ˆ")
        print("=" * 80)
        
        total_docs = sum(s.total_documents for s in all_stats)
        processed_docs = sum(s.processed_documents for s in all_stats)
        total_chunks = sum(s.total_chunks for s in all_stats)
        processed_chunks = sum(s.processed_chunks for s in all_stats)
        total_errors = sum(len(s.errors) for s in all_stats)
        total_time = sum(s.duration for s in all_stats)
        
        print(f"ğŸ“‹ Collections: {len(all_stats)}")
        print(f"ğŸ“„ ç¸½æ–‡æª”: {processed_docs}/{total_docs} ({(processed_docs/total_docs*100):.1f}%)")
        print(f"ğŸ§© ç¸½ Chunks: {processed_chunks}/{total_chunks}")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time:.1f} ç§’")
        print(f"âŒ ç¸½éŒ¯èª¤: {total_errors}")
        
        # ä¿å­˜çµ±è¨ˆåˆ°æª”æ¡ˆ
        self.save_stats_to_file(all_stats, f"cycle_{cycle_number}")
    
    def print_overall_stats(self, all_stats: List[ProcessingStats]) -> None:
        """è¼¸å‡ºç¸½é«”çµ±è¨ˆ"""
        print("\n" + "=" * 80)
        print("ğŸ¯ ç¸½é«”è™•ç†çµ±è¨ˆ")
        print("=" * 80)
        
        total_docs = sum(s.total_documents for s in all_stats)
        processed_docs = sum(s.processed_documents for s in all_stats)
        total_chunks = sum(s.total_chunks for s in all_stats)
        processed_chunks = sum(s.processed_chunks for s in all_stats)
        total_errors = sum(len(s.errors) for s in all_stats)
        total_time = sum(s.duration for s in all_stats)
        
        print(f"ğŸ“‹ Collections: {len(all_stats)}")
        print(f"ğŸ“„ ç¸½æ–‡æª”: {processed_docs}/{total_docs} ({(processed_docs/total_docs*100):.1f}%)")
        print(f"ğŸ§© ç¸½ Chunks: {processed_chunks}/{total_chunks}")
        print(f"â±ï¸ ç¸½æ™‚é–“: {total_time:.1f} ç§’")
        print(f"âŒ ç¸½éŒ¯èª¤: {total_errors}")
        
        # ä¿å­˜çµ±è¨ˆåˆ°æª”æ¡ˆ
        self.save_stats_to_file(all_stats, "overall")
    
    def save_stats_to_file(self, all_stats: List[ProcessingStats], suffix: str = "") -> None:
        """ä¿å­˜çµ±è¨ˆåˆ°æª”æ¡ˆ"""
        try:
            stats_data = {
                "timestamp": datetime.now().isoformat(),
                "collections": [asdict(stats) for stats in all_stats]
            }
            
            filename = f"batch_process_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{suffix}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(stats_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ“ çµ±è¨ˆå·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜çµ±è¨ˆå¤±æ•—: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹è‡ªå‹•åŒ–æ‰¹æ¬¡è™•ç† MongoDB collectionsï¼ˆå¾ªç’°æ¨¡å¼ + æ–·é»çºŒå‚³ï¼‰")
    print("=" * 80)
    print("ğŸ’¡ æç¤ºï¼šæŒ‰ Ctrl+C å¯éš¨æ™‚åœæ­¢åŸ·è¡Œï¼Œä¸‹æ¬¡æœƒå¾ä¸­æ–·è™•ç¹¼çºŒ")
    print("ğŸ’¡ æ¯ 5 å€‹ collections ç‚ºä¸€å€‹å¾ªç’°ï¼Œè‡ªå‹•å„²å­˜é€²åº¦")
    print("=" * 80)
    
    # é…ç½®
    mongo_config = {
        "host": "192.168.32.86",
        "port": 30017,
        "username": "bdse37",
        "password": "111111",
        "database": "podcast"
    }
    
    postgres_config = {
        "host": "192.168.32.56",
        "port": 32432,
        "database": "podcast",
        "user": "bdse37",
        "password": "111111"
    }
    
    milvus_config = {
        "host": "192.168.32.86",
        "port": "19530",
        "collection_name": "podcast_chunks",
        "dim": 1024,
        "index_type": "IVF_FLAT",
        "metric_type": "L2",
        "params": {"nlist": 1024}
    }
    
    # å‰µå»ºæ‰¹æ¬¡è™•ç†å™¨
    processor = BatchProcessor(mongo_config, postgres_config, milvus_config, collections_per_cycle=5)
    
    if processor.initialize():
        collections = processor.orchestrator.mongo_processor.get_collection_names()
        if collections:
            print(f"\nğŸ“‹ ç¸½å…±æœ‰ {len(collections)} å€‹ collections")
            print(f"ğŸ“‹ Collections: {collections}")
            
            # é¡¯ç¤ºé€²åº¦è³‡è¨Š
            progress = processor.progress_manager.progress
            if progress["completed_collections"]:
                print(f"\nğŸ“Š é€²åº¦è³‡è¨Š:")
                print(f"   âœ… å·²å®Œæˆ collections: {len(progress['completed_collections'])}")
                print(f"   ğŸ“„ å·²è™•ç† chunks: {progress['total_processed_chunks']:,}")
                print(f"   ğŸ”„ å·²å®Œæˆå¾ªç’°: {progress.get('cycle_count', 0)}")
                print(f"   ğŸ• é–‹å§‹æ™‚é–“: {progress['start_time']}")
                print(f"   ğŸ”„ ä¸Šæ¬¡æ›´æ–°: {progress['last_update']}")
                
                if progress["last_processed_collection"]:
                    print(f"   ğŸ“ ä¸Šæ¬¡è™•ç†: {progress['last_processed_collection']}")
                
                # è©¢å•æ˜¯å¦å¾é ­é–‹å§‹
                print(f"\nâ“ æ˜¯å¦è¦å¾é ­é–‹å§‹é‡æ–°è™•ç†ï¼Ÿ(y/N): ", end="")
                try:
                    choice = input().strip().lower()
                    if choice == 'y':
                        print("ğŸ§¹ æ¸…ç©ºé€²åº¦è¨˜éŒ„ï¼Œå¾é ­é–‹å§‹...")
                        processor.progress_manager.progress = {
                            "last_processed_collection": None,
                            "last_processed_doc": None,
                            "completed_collections": [],
                            "total_processed_chunks": 0,
                            "start_time": datetime.now().isoformat(),
                            "last_update": datetime.now().isoformat(),
                            "cycle_count": 0,
                            "current_cycle": 0
                        }
                        processor.progress_manager.save_progress()
                        
                        # æ¸…ç©º Milvus é›†åˆ
                        print(f"ğŸ§¹ æ¸…ç©º Milvus é›†åˆ podcast_chunks...")
                        try:
                            processor.orchestrator.milvus_writer.clear_collection("podcast_chunks")
                            print("âœ… Milvus é›†åˆå·²æ¸…ç©º")
                        except Exception as e:
                            print(f"âš ï¸  æ¸…ç©ºé›†åˆå¤±æ•—ï¼ˆå¯èƒ½é›†åˆä¸å­˜åœ¨ï¼‰: {e}")
                            try:
                                processor.orchestrator.milvus_writer.create_collection("podcast_chunks")
                                print("âœ… é‡æ–°å‰µå»º Milvus é›†åˆ")
                            except Exception as e2:
                                print(f"âŒ å‰µå»ºé›†åˆå¤±æ•—: {e2}")
                                return
                    else:
                        print("ğŸ”„ å¾ä¸Šæ¬¡ä¸­æ–·è™•ç¹¼çºŒåŸ·è¡Œ...")
                except KeyboardInterrupt:
                    print("\nâ¹ï¸  ä½¿ç”¨è€…å–æ¶ˆ")
                    return
            else:
                # ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼Œæ¸…ç©º Milvus é›†åˆ
                print(f"\nğŸ§¹ æ¸…ç©º Milvus é›†åˆ podcast_chunks...")
                try:
                    processor.orchestrator.milvus_writer.clear_collection("podcast_chunks")
                    print("âœ… Milvus é›†åˆå·²æ¸…ç©º")
                except Exception as e:
                    print(f"âš ï¸  æ¸…ç©ºé›†åˆå¤±æ•—ï¼ˆå¯èƒ½é›†åˆä¸å­˜åœ¨ï¼‰: {e}")
                    try:
                        processor.orchestrator.milvus_writer.create_collection("podcast_chunks")
                        print("âœ… é‡æ–°å‰µå»º Milvus é›†åˆ")
                    except Exception as e2:
                        print(f"âŒ å‰µå»ºé›†åˆå¤±æ•—: {e2}")
                        return
            
            print(f"\nğŸš€ é–‹å§‹å¾ªç’°è™•ç† collections...")
            
            try:
                # è©¢å•è™•ç†æ¨¡å¼
                print(f"\nâ“ é¸æ“‡è™•ç†æ¨¡å¼:")
                print(f"   1. å¾ªç’°æ¨¡å¼ï¼ˆæ¯ 5 å€‹ collections ä¸€å€‹å¾ªç’°ï¼‰")
                print(f"   2. ä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰ collections")
                print(f"è«‹é¸æ“‡ (1/2): ", end="")
                
                choice = input().strip()
                
                if choice == "1":
                    print("ğŸ”„ ä½¿ç”¨å¾ªç’°æ¨¡å¼...")
                    all_stats = processor.process_collections_in_cycles(limit=None)
                else:
                    print("ğŸš€ ä½¿ç”¨ä¸€æ¬¡æ€§è™•ç†æ¨¡å¼...")
                    all_stats = processor.process_all_collections(limit=None)
                
                # è¼¸å‡ºç¸½é«”çµ±è¨ˆ
                if all_stats:
                    processor.print_overall_stats(all_stats)
                    
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸  ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
                print(f"ğŸ’¾ é€²åº¦å·²å„²å­˜ï¼Œä¸‹æ¬¡å¯å¾ä¸­æ–·è™•ç¹¼çºŒ")
        else:
            print("âŒ æ‰¾ä¸åˆ°ä»»ä½• collection")
    else:
        print("âŒ æ‰¹æ¬¡è™•ç†å™¨åˆå§‹åŒ–å¤±æ•—ï¼")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")


if __name__ == "__main__":
    main() 