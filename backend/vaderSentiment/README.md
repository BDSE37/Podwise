# ç¹é«”ä¸­æ–‡æƒ…æ„Ÿåˆ†æå¼•æ“

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![OOP](https://img.shields.io/badge/Architecture-OOP-orange.svg)](https://en.wikipedia.org/wiki/Object-oriented_programming)

## ğŸ“‹ æ¦‚è¿°

é€™æ˜¯ä¸€å€‹å°ˆç‚ºç¹é«”ä¸­æ–‡è¨­è¨ˆçš„æƒ…æ„Ÿåˆ†æå¼•æ“ï¼ŒåŸºæ–¼ **NLTK** å’Œè‡ªå®šç¾©ä¸­æ–‡æƒ…æ„Ÿè©å…¸ï¼Œæ¡ç”¨ **OOP æ¶æ§‹** å’Œ **Google Clean Code åŸå‰‡** é–‹ç™¼ã€‚å°ˆæ¡ˆæ•´åˆäº† [ç¹é«”ä¸­æ–‡ NLP å¾ word2vec åˆ°æƒ…æ„Ÿåˆ†æ](https://studentcodebank.wordpress.com/2019/02/22/%E7%B9%81%E9%AB%94%E4%B8%AD%E6%96%87-nlp-%E5%BE%9Eword2vec%E5%88%B0-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/) çš„å®Œæ•´é‚è¼¯ï¼Œæä¾›é«˜æº–ç¢ºåº¦çš„ä¸­æ–‡æƒ…æ„Ÿåˆ†ææœå‹™ã€‚

## ğŸ¯ åŠŸèƒ½ç‰¹è‰²

- **ç¹é«”ä¸­æ–‡å„ªåŒ–**: å°ˆé–€é‡å°å°ç£ç¹é«”ä¸­æ–‡èªå¢ƒè¨­è¨ˆ
- **å¤šç¶­åº¦åˆ†æ**: æä¾› Compoundã€æ­£é¢ã€è² é¢ã€ä¸­æ€§åˆ†æ•¸
- **æ™ºèƒ½æ–‡æœ¬è™•ç†**: ä½¿ç”¨ jieba åˆ†è© + åœç”¨è©éæ¿¾
- **æ‰¹æ¬¡è™•ç†**: æ”¯æ´å¤§é‡ JSON æª”æ¡ˆæ‰¹æ¬¡åˆ†æ
- **OOP æ¶æ§‹**: ç¬¦åˆ Google Clean Code åŸå‰‡
- **å¯æ“´å±•æ€§**: æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œæ˜“æ–¼ç¶­è­·å’Œæ“´å±•

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```
vaderSentiment/
â”œâ”€â”€ core/                          # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py               # æ¨¡çµ„åˆå§‹åŒ–
â”‚   â”œâ”€â”€ text_preprocessor.py      # æ–‡æœ¬é è™•ç†å™¨
â”‚   â”œâ”€â”€ lexicon_manager.py        # è©å…¸ç®¡ç†å™¨
â”‚   â”œâ”€â”€ sentiment_analyzer.py     # æƒ…æ„Ÿåˆ†æå™¨
â”‚   â””â”€â”€ data_processor.py         # è³‡æ–™è™•ç†å™¨
â”œâ”€â”€ scripts/                       # å·¥å…·è…³æœ¬
â”‚   â”œâ”€â”€ fix_json_format.py        # JSON æ ¼å¼ä¿®æ­£
â”‚   â”œâ”€â”€ clean_filenames.py        # æª”åæ¸…ç†
â”‚   â”œâ”€â”€ convert_to_json.py        # æ ¼å¼è½‰æ›
â”‚   â”œâ”€â”€ convert_to_traditional.py # ç°¡ç¹è½‰æ›
â”‚   â””â”€â”€ filename_rename_table.csv # æª”åå°ç…§è¡¨
â”œâ”€â”€ comment_data/                  # åŸå§‹è³‡æ–™
â”‚   â””â”€â”€ *.json                    # JSON æª”æ¡ˆ
â”œâ”€â”€ main.py                       # ä¸»ç¨‹å¼å…¥å£
â”œâ”€â”€ apple_podcast_analyzer.py     # Apple Podcast åˆ†æå™¨
â”œâ”€â”€ sentiment_analysis_engine.py  # èˆŠç‰ˆåˆ†æå¼•æ“ï¼ˆä¿ç•™ï¼‰
â”œâ”€â”€ vader_lexicon.txt             # è‹±æ–‡æƒ…æ„Ÿè©å…¸
â”œâ”€â”€ vader_lexicon2.txt            # æ“´å±•è©å…¸ï¼ˆç°¡é«”ä¸­æ–‡ï¼‰
â”œâ”€â”€ emoji_utf8_lexicon.txt        # è¡¨æƒ…ç¬¦è™Ÿè©å…¸
â””â”€â”€ README.md                     # å°ˆæ¡ˆèªªæ˜
```

## ğŸ§  æƒ…æ„Ÿåˆ†æé‚è¼¯

### A. æ–‡æœ¬é è™•ç† (TextPreprocessor)

```python
# 1. æ–‡æœ¬æ¸…ç†
text = re.sub(r'[^\u4e00-\u9fff\w\s]', ' ', text)  # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—
text = re.sub(r'\s+', ' ', text).strip()            # ç§»é™¤å¤šé¤˜ç©ºæ ¼

# 2. ä¸­æ–‡åˆ†è©
tokens = jieba.lcut(text)                           # jieba åˆ†è©
tokens = [token for token in tokens if token not in stop_words and len(token) > 1]

# 3. åœç”¨è©éæ¿¾
stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', ...}
```

### B. ä¸­æ–‡æƒ…æ„Ÿè©å…¸ (LexiconManager)

#### 1. æ­£é¢è©å½™ (120+ å€‹)
```python
positive_words = {
    'å¥½', 'æ£’', 'è®š', 'å„ªç§€', 'å®Œç¾', 'å–œæ­¡', 'æ„›', 'é–‹å¿ƒ', 'å¿«æ¨‚', 'é«˜èˆˆ', 
    'èˆˆå¥®', 'æ»¿æ„', 'æˆåŠŸ', 'å²å®³', 'å¼·å¤§', 'ç¾å¥½', 'ç²¾å½©', 'æœ‰è¶£', 'å¥½ç©',
    'èˆ’é©', 'æ–¹ä¾¿', 'å¿«é€Ÿ', 'ä¾¿å®œ', 'åˆ’ç®—', 'å€¼å¾—', 'æ¨è–¦', 'æ”¯æŒ', 'é¼“å‹µ',
    'å¹«åŠ©', 'æº«æš–', 'å‹å–„', 'è¦ªåˆ‡', 'å°ˆæ¥­', 'èªçœŸ', 'è² è²¬', 'èª å¯¦', 'å¯é ',
    'ç©©å®š', 'å®‰å…¨', 'å¥åº·', 'æ–°é®®', 'ç¾å‘³', 'æ¼‚äº®', 'å¸¥æ°£', 'å¯æ„›', 'è°æ˜',
    'æ™ºæ…§', 'å‰µæ„', 'å‰µæ–°', 'é€²æ­¥', 'ç™¼å±•', 'æˆé•·', 'å­¸ç¿’', 'æå‡', 'æ”¹å–„',
    'è§£æ±º', 'å®Œæˆ', 'å¯¦ç¾', 'å¤¢æƒ³', 'å¸Œæœ›', 'æœªä¾†', 'æ©Ÿæœƒ', 'æˆåŠŸ', 'å‹åˆ©',
    'å† è»', 'ç¬¬ä¸€', 'æœ€å¥½', 'æœ€æ£’', 'æœ€è®š', 'è¶…æ£’', 'è¶…è®š', 'ç„¡æ•µ', 'å®Œç¾',
    'ç†æƒ³', 'å„ªè³ª', 'é«˜å“è³ª', 'é ‚ç´š', 'ä¸€æµ', 'å‚‘å‡º', 'å“è¶Š', 'éå‡¡', 'å‡ºè‰²',
    'å„ªé›…', 'ç²¾ç·»', 'ç´°ç·»', 'ç”¨å¿ƒ', 'è²¼å¿ƒ', 'å‘¨åˆ°', 'ç´°å¿ƒ', 'è€å¿ƒ', 'æ†å¿ƒ',
    'æ±ºå¿ƒ', 'ä¿¡å¿ƒ', 'å‹‡æ°£', 'å …å¼·', 'å‹‡æ•¢', 'æ¨‚è§€', 'ç©æ¥µ', 'æ­£é¢', 'é™½å…‰',
    'æ´»åŠ›', 'èƒ½é‡', 'å‹•åŠ›', 'ç†±æƒ…', 'æ¿€æƒ…', 'ç†±æ„›', 'ç†±è¡·', 'å°ˆæ³¨', 'æŠ•å…¥',
    'å°ˆå¿ƒ', 'å°ˆæ³¨', 'å°ˆä¸€', 'å°ˆç²¾', 'ç²¾é€š', 'ç†Ÿç·´'
}
```

#### 2. è² é¢è©å½™ (150+ å€‹) - å·²æ¸…ç†é‡è¤‡
```python
negative_words = {
    # åŸºç¤è² é¢æƒ…æ„Ÿè©å½™
    'å£', 'ç³Ÿ', 'çˆ›', 'å·®', 'è¨å­', 'æ¨', 'ç”Ÿæ°£', 'æ†¤æ€’', 'å‚·å¿ƒ', 'é›£é',
    'ç—›è‹¦', 'å¤±æœ›', 'å¤±æ•—', 'ç„¡èŠ', 'ç…©', 'ç´¯', 'ç–²æ†Š', 'å›°é›£', 'éº»ç…©',
    'è¤‡é›œ', 'æ˜‚è²´', 'è²´', 'æµªè²»', 'ä¸å€¼å¾—', 'ä¸æ¨è–¦', 'åå°', 'æ‰¹è©•',
    'æŠ±æ€¨', 'æŒ‡è²¬', 'å†·æ¼ ', 'ç„¡æƒ…', 'ä¸å‹å–„', 'ä¸å°ˆæ¥­', 'ä¸èªçœŸ', 'ä¸è² è²¬',
    'ä¸èª å¯¦', 'ä¸å¯é ', 'ä¸ç©©å®š', 'ä¸å®‰å…¨', 'ä¸å¥åº·', 'ä¸æ–°é®®', 'é›£åƒ',
    'é†œ', 'é›£çœ‹', 'ç¬¨', 'æ„šè ¢', 'ç„¡å‰µæ„', 'é€€æ­¥', 'é€€ç¸®', 'åœæ»¯',
    'å•é¡Œ', 'éšœç¤™', 'æŒ«æŠ˜', 'çµ•æœ›', 'ç„¡æœ›', 'å¤±å»', 'éŒ¯é', 'è¼¸',
    'å–®èª¿', 'æ¯ç‡¥', 'ä¹å‘³', 'æ²‰æ‚¶', 'å£“æŠ‘', 'æ²‰é‡', 'æŠ˜ç£¨', 'ç…ç†¬',
    'ææ‡¼', 'å®³æ€•', 'é©šæ', 'ææ…Œ',
    
    # å“è³ªç›¸é—œè² é¢è©å½™
    'ä¸å…¬å¹³', 'ä¸æ­£ç¾©', 'ä¸èˆ’æœ', 'é«’', 'äº‚', 'é‚ªæƒ¡', 'èªªè¬Š', 
    'è†½å°', 'è»Ÿå¼±', 'æ‚²è§€', 'æ¶ˆæ¥µ', 'è¢«å‹•', 'ä¾è³´', 'ä¸å¹³ç­‰', 'å°ˆåˆ¶', 
    'æˆ°çˆ­', 'åˆ†è£‚', 'å®ˆèˆŠ', 'åµ', 'å±éšª', 'å‚³çµ±', 'ä¿å®ˆ', 'é™åˆ¶', 'ç«¶çˆ­',
    
    # å¦å®šè©çµ„åˆï¼ˆé¿å…é‡è¤‡ï¼‰
    'ä¸å–œæ­¡', 'ä¸æ„›', 'ä¸é–‹å¿ƒ', 'ä¸é«˜èˆˆ', 'ä¸å¿«æ¨‚', 'ä¸æ»¿æ„', 'ä¸æˆåŠŸ',
    'ä¸å²å®³', 'ä¸å¼·å¤§', 'ä¸ç¾å¥½', 'ä¸ç²¾å½©', 'ä¸æœ‰è¶£', 'ä¸å¥½ç©', 'ä¸èˆ’é©',
    'ä¸æ–¹ä¾¿', 'ä¸å¿«é€Ÿ', 'ä¸ä¾¿å®œ', 'ä¸åˆ’ç®—', 'ä¸æ”¯æŒ', 'ä¸é¼“å‹µ', 'ä¸å¹«åŠ©',
    'ä¸æº«æš–', 'ä¸è¦ªåˆ‡', 'ä¸ç¾å‘³', 'ä¸æ¼‚äº®', 'ä¸å¸¥æ°£', 'ä¸å¯æ„›', 'ä¸è°æ˜',
    'ä¸æ™ºæ…§', 'ä¸èƒ½', 'ä¸éœ€è¦', 'ä¸æƒ³è¦', 'ä¸çŸ¥é“', 'æ“”å¿ƒ', 'ä¸ç´¯', 'ä¸é¤“', 'ä¸æ¸´'
}
```

#### 3. ç¨‹åº¦è© (50+ å€‹)
```python
intensifiers = {
    'éå¸¸': 2.0, 'å¾ˆ': 1.5, 'ç‰¹åˆ¥': 1.8, 'æ¥µå…¶': 2.0, 'ååˆ†': 1.8,
    'ç›¸ç•¶': 1.3, 'æ¯”è¼ƒ': 1.2, 'æœ‰é»': 0.8, 'ç¨å¾®': 0.6, 'ä¸€é»': 0.5,
    'è¶…ç´š': 2.5, 'ç„¡æ•µ': 2.5, 'è¶…': 1.8, 'å¤ª': 1.5, 'çœŸ': 1.3,
    'ç¢ºå¯¦': 1.2, 'çœŸçš„': 1.3, 'å¯¦åœ¨': 1.2, 'ç°¡ç›´': 1.5, 'å®Œå…¨': 1.8,
    'çµ•å°': 2.0, 'æ ¹æœ¬': 1.5, 'å¾ä¾†': 1.3, 'å¾ä¾†æ²’æœ‰': 1.5, 'å¾ä¾†ä¸': 1.5,
    # å¾åœç”¨è©ä¸­ç§»å‡ºçš„ç¨‹åº¦è©
    'ç†±': 1.2, 'å†·': 1.2, 'æš–': 1.1, 'æ¶¼': 1.1, 'äº®': 1.1, 'æš—': 1.2,
    'å®‰éœ': 1.1, 'åµ': 1.3, 'æ•´é½Š': 1.1, 'äº‚': 1.3, 'ç°¡å–®': 1.1, 'è¤‡é›œ': 1.3,
    'å®¹æ˜“': 1.1, 'å›°é›£': 1.3, 'ä¾¿å®œ': 1.1, 'è²´': 1.3, 'å…è²»': 1.1, 'ä»˜è²»': 1.2,
    'å…¬é–‹': 1.1, 'ç§äºº': 1.1, 'æ­£å¼': 1.1, 'éæ­£å¼': 1.1, 'é‡è¦': 1.2,
    'ä¸é‡è¦': 1.2, 'ç·Šæ€¥': 1.3, 'ä¸ç·Šæ€¥': 1.1, 'å±éšª': 1.5, 'å®‰å…¨': 1.1
}
```

#### 4. å¦å®šè© (80+ å€‹)
```python
negations = {
    'ä¸', 'æ²’', 'ç„¡', 'åˆ¥', 'è«', 'å‹¿', 'æœª', 'é', 'å¦', 'ä¸æ˜¯',
    'æ²’æœ‰', 'ç„¡é—œ', 'ç„¡æ‰€è¬‚', 'ä¸åœ¨ä¹', 'ä¸é—œå¿ƒ', 'ä¸å–œæ­¡', 'ä¸æ„›',
    'ä¸é–‹å¿ƒ', 'ä¸å¿«æ¨‚', 'ä¸é«˜èˆˆ', 'ä¸æ»¿æ„', 'ä¸æˆåŠŸ', 'ä¸å²å®³',
    'ä¸å¼·å¤§', 'ä¸ç¾å¥½', 'ä¸ç²¾å½©', 'ä¸æœ‰è¶£', 'ä¸å¥½ç©', 'ä¸èˆ’é©',
    'ä¸æ–¹ä¾¿', 'ä¸å¿«é€Ÿ', 'ä¸ä¾¿å®œ', 'ä¸åˆ’ç®—', 'ä¸å€¼å¾—', 'ä¸æ¨è–¦',
    'ä¸æ”¯æŒ', 'ä¸é¼“å‹µ', 'ä¸å¹«åŠ©', 'ä¸æº«æš–', 'ä¸å‹å–„', 'ä¸è¦ªåˆ‡',
    'ä¸å°ˆæ¥­', 'ä¸èªçœŸ', 'ä¸è² è²¬', 'ä¸èª å¯¦', 'ä¸å¯é ', 'ä¸ç©©å®š',
    'ä¸å®‰å…¨', 'ä¸å¥åº·', 'ä¸æ–°é®®', 'ä¸ç¾å‘³', 'ä¸æ¼‚äº®', 'ä¸å¸¥æ°£',
    'ä¸å¯æ„›', 'ä¸è°æ˜', 'ä¸æ™ºæ…§', 'ä¸å‰µæ„', 'ä¸å‰µæ–°', 'ä¸é€€æ­¥',
    'ä¸é€€ç¸®', 'ä¸åœæ»¯', 'æ²’å•é¡Œ', 'æ²’å›°é›£', 'æ²’éšœç¤™', 'æ²’å¤±æ•—',
    'æ²’æŒ«æŠ˜', 'æ²’çµ•æœ›', 'æ²’ç„¡æœ›', 'æ²’éå»', 'æ²’å¤±å»', 'æ²’éŒ¯é',
    'æ²’å¤±æ•—', 'æ²’è¼¸', 'æ²’æœ€å¾Œ', 'æ²’æœ€å·®', 'æ²’æœ€ç³Ÿ', 'æ²’æœ€çˆ›'
}
```

### C. æƒ…æ„Ÿåˆ†æ•¸è¨ˆç®—é‚è¼¯

#### 1. è©å½™åŒ¹é…èˆ‡æ¬Šé‡è¨ˆç®—
```python
def _calculate_sentiment_scores(self, tokens: List[str]) -> Dict[str, float]:
    positive_score = 0.0
    negative_score = 0.0
    
    positive_words = self.lexicon.get_positive_words()
    negative_words = self.lexicon.get_negative_words()
    intensifiers = self.lexicon.get_intensifiers()
    negations = self.lexicon.get_negations()
    
    for i, token in enumerate(tokens):
        # æª¢æŸ¥ç¨‹åº¦è©
        intensity = 1.0
        if token in intensifiers:
            intensity = intensifiers[token]
        
        # æª¢æŸ¥å¦å®šè©
        negation_factor = 1.0
        if i > 0 and tokens[i-1] in negations:
            negation_factor = -1.0
        
        # è¨ˆç®—åˆ†æ•¸
        if token in positive_words:
            positive_score += intensity * negation_factor
        elif token in negative_words:
            negative_score += intensity * negation_factor
    
    return {
        'positive': positive_score,
        'negative': negative_score
    }
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from vaderSentiment import get_sentiment_analysis

# ç²å–æƒ…æ„Ÿåˆ†æå¯¦ä¾‹
sentiment_analyzer = get_sentiment_analysis()

# åˆ†æè©•è«–æƒ…æ„Ÿ
result = sentiment_analyzer.analyze_text(
    text="é€™å€‹ Podcast çœŸçš„å¾ˆæ£’ï¼",
    analyzer_type="chinese"
)

print(f"æƒ…æ„Ÿæ¨™ç±¤: {result.label}")
print(f"ä¿¡å¿ƒåº¦: {result.confidence}")
```

## ğŸ¯ è¨­è¨ˆåŸå‰‡

### OOP è¨­è¨ˆåŸå‰‡
- **å–®ä¸€è·è²¬åŸå‰‡ (SRP)**: æ¯å€‹é¡åˆ¥å°ˆè²¬ç‰¹å®šåŠŸèƒ½
- **é–‹æ”¾å°é–‰åŸå‰‡ (OCP)**: å¯æ“´å±•æ–°çš„åˆ†æå™¨
- **ä¾è³´åè½‰åŸå‰‡ (DIP)**: é«˜å±¤æ¨¡çµ„ä¾è³´æŠ½è±¡ä»‹é¢
- **ä»‹é¢éš”é›¢åŸå‰‡ (ISP)**: æ˜ç¢ºçš„è·è²¬åˆ†å·¥
- **é‡Œæ°æ›¿æ›åŸå‰‡ (LSP)**: å­é¡åˆ¥å¯æ›¿æ›çˆ¶é¡åˆ¥

## ğŸ› ï¸ ä¾è³´é …ç›®

- nltk
- jieba
- pandas
- numpy
- scikit-learn 