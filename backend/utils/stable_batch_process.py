#!/usr/bin/env python3
"""
ç©©å®šçš„æ‰¹æ¬¡è™•ç†è…³æœ¬
- åˆ†æ®µè™•ç†ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º
- è©³ç´°çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒ
- é€²åº¦ä¿å­˜å’Œæ¢å¾©åŠŸèƒ½
- è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
"""

import os
import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple
import logging
import psycopg2
from datetime import datetime, timedelta
from dotenv import load_dotenv
from pymilvus import connections, Collection, utility
import numpy as np
from tqdm import tqdm
import traceback
import signal
import pickle

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv('backend/.env')

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_process.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GracefulExit:
    """å„ªé›…é€€å‡ºè™•ç†"""
    def __init__(self):
        self.kill_now = False
        signal.signal(signal.SIGINT, self.exit_gracefully)
        signal.signal(signal.SIGTERM, self.exit_gracefully)
    
    def exit_gracefully(self, signum, frame):
        logger.info("æ”¶åˆ°é€€å‡ºä¿¡è™Ÿï¼Œæ­£åœ¨å„ªé›…é€€å‡º...")
        self.kill_now = True

class BGE_M3_Embedding:
    """BGE-M3 embedding ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.model = None
        self.device = 'cuda'
        self.load_model()
    
    def load_model(self):
        """è¼‰å…¥ BGE-M3 æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"å˜—è©¦è¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° {self.device}...")
            
            # è¨­å®šè¼ƒé•·çš„è¶…æ™‚æ™‚é–“
            self.model = SentenceTransformer('BAAI/bge-m3', device=self.device)
            
            # æ¸¬è©¦æ¨¡å‹
            test_text = "æ¸¬è©¦æ–‡æœ¬"
            test_embedding = self.model.encode(test_text, normalize_embeddings=True)
            
            if len(test_embedding) == 1024 and not np.allclose(test_embedding, 0):
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° {self.device}")
                return True
            else:
                raise Exception("æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼šembedding ç‚ºé›¶å‘é‡")
                
        except Exception as e:
            logger.warning(f"GPU è¼‰å…¥å¤±æ•—: {str(e)}")
            
            try:
                logger.info("å˜—è©¦è¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° CPU...")
                self.device = 'cpu'
                self.model = SentenceTransformer('BAAI/bge-m3', device=self.device)
                
                test_text = "æ¸¬è©¦æ–‡æœ¬"
                test_embedding = self.model.encode(test_text, normalize_embeddings=True)
                
                if len(test_embedding) == 1024 and not np.allclose(test_embedding, 0):
                    logger.info("âœ… æˆåŠŸè¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° CPU")
                    return True
                else:
                    raise Exception("æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼šembedding ç‚ºé›¶å‘é‡")
                    
            except Exception as e2:
                logger.error(f"CPU è¼‰å…¥ä¹Ÿå¤±æ•—: {str(e2)}")
                return False
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆ BGE-M3 embedding"""
        if not self.model:
            logger.error("BGE-M3 æ¨¡å‹æœªè¼‰å…¥")
            return [0.0] * 1024
        
        try:
            embedding = self.model.encode(text, normalize_embeddings=True)
            embedding_list = embedding.tolist()
            
            if np.allclose(embedding, 0):
                logger.warning(f"âš ï¸ è­¦å‘Šï¼šæ–‡æœ¬ '{text[:50]}...' ç”¢ç”Ÿé›¶å‘é‡ embedding")
            
            return embedding_list
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ embedding å¤±æ•—: {str(e)}")
            return [0.0] * 1024

class MilvusConnector:
    """Milvus é€£ç·šå™¨"""
    
    def __init__(self):
        self.host = "192.168.32.86"
        self.user = "bdse37"
        self.password = "111111"
        self.collection_name = "podcast_chunks"
        self.connected = False
        self.collection = None
    
    def _safe_cast_field(self, value: Any, field_name: str, field_type: str, 
                        default_value: Any, max_length: Optional[int] = None) -> Any:
        """å®‰å…¨çš„æ¬„ä½å‹æ…‹è½‰æ›"""
        try:
            if value is None:
                return default_value
            
            if field_type == 'str':
                if isinstance(value, list):
                    if len(value) > 0:
                        first_item = value[0]
                        if isinstance(first_item, (list, dict)):
                            import json
                            result = json.dumps(first_item, ensure_ascii=False)
                        else:
                            result = str(first_item)
                    else:
                        result = str(default_value)
                elif isinstance(value, dict):
                    import json
                    result = json.dumps(value, ensure_ascii=False)
                else:
                    result = str(value)
                
                result = str(result)
                
                if max_length and len(result) > max_length:
                    result = result[:max_length]
                
                return result
                
            elif field_type == 'int':
                if isinstance(value, (list, dict)):
                    return default_value
                try:
                    return int(value)
                except (ValueError, TypeError):
                    return default_value
                    
            elif field_type == 'float':
                if isinstance(value, (list, dict)):
                    return default_value
                try:
                    return float(value)
                except (ValueError, TypeError):
                    return default_value
                    
            elif field_type == 'list':
                if isinstance(value, list):
                    return value
                else:
                    return default_value
                    
            else:
                return default_value
                
        except Exception as e:
            logger.warning(f"æ¬„ä½ {field_name} è½‰æ›å¤±æ•—: {str(e)}ï¼Œä½¿ç”¨é è¨­å€¼")
            return default_value
    
    def connect(self) -> bool:
        """é€£ç·šåˆ° Milvus"""
        try:
            ports = [19530, 9091, 9000]
            
            for port in ports:
                try:
                    logger.info(f"å˜—è©¦é€£ç·š Milvus: {self.host}:{port}")
                    connections.connect(
                        alias="default",
                        host=self.host,
                        port=port,
                        user=self.user,
                        password=self.password
                    )
                    
                    utility.list_collections()
                    logger.info(f"âœ… æˆåŠŸé€£ç·š Milvus: {self.host}:{port}")
                    
                    if utility.has_collection(self.collection_name):
                        self.collection = Collection(self.collection_name)
                        self.collection.load()
                        logger.info(f"âœ… æˆåŠŸè¼‰å…¥ collection: {self.collection_name}")
                        self.connected = True
                        return True
                    else:
                        logger.error(f"âŒ Collection '{self.collection_name}' ä¸å­˜åœ¨")
                        return False
                        
                except Exception as e:
                    logger.warning(f"Port {port} é€£ç·šå¤±æ•—: {str(e)}")
                    continue
            
            logger.error("âŒ æ‰€æœ‰ port éƒ½ç„¡æ³•é€£ç·š Milvus")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Milvus é€£ç·šå¤±æ•—: {str(e)}")
            return False
    
    def insert_data(self, processed_chunks: List[Dict]) -> bool:
        """æ’å…¥è³‡æ–™åˆ° Milvus"""
        if not self.connected or not self.collection:
            logger.error("âŒ Milvus æœªé€£ç·š")
            return False
        
        try:
            for chunk in processed_chunks:
                raw_chunk_id = chunk.get('chunk_id')
                processed_chunk_id = self._safe_cast_field(raw_chunk_id, 'chunk_id', 'str', '', 64)
                
                single_chunk_data = {
                    'chunk_id': processed_chunk_id,
                    'chunk_index': self._safe_cast_field(chunk.get('chunk_index'), 'chunk_index', 'int', 0),
                    'episode_id': self._safe_cast_field(chunk.get('episode_id'), 'episode_id', 'int', 0),
                    'podcast_id': self._safe_cast_field(chunk.get('podcast_id'), 'podcast_id', 'int', 0),
                    'podcast_name': self._safe_cast_field(chunk.get('podcast_name'), 'podcast_name', 'æœªçŸ¥æ’­å®¢', 255),
                    'author': self._safe_cast_field(chunk.get('author'), 'author', 'str', 'æœªçŸ¥ä½œè€…', 255),
                    'category': self._safe_cast_field(chunk.get('category'), 'category', 'str', 'æœªçŸ¥åˆ†é¡', 64),
                    'episode_title': self._safe_cast_field(chunk.get('episode_title'), 'episode_title', 'str', 'æœªçŸ¥æ¨™é¡Œ', 255),
                    'duration': self._safe_cast_field(chunk.get('duration'), 'duration', 'str', 'æœªçŸ¥æ™‚é•·', 255),
                    'published_date': self._safe_cast_field(chunk.get('published_date'), 'published_date', 'str', 'æœªçŸ¥æ—¥æœŸ', 64),
                    'apple_rating': self._safe_cast_field(chunk.get('apple_rating'), 'apple_rating', 'int', 0),
                    'chunk_text': self._safe_cast_field(chunk.get('chunk_text'), 'chunk_text', 'str', 'ç„¡å…§å®¹', 1024),
                    'language': self._safe_cast_field(chunk.get('language'), 'language', 'str', 'zh', 16),
                    'created_at': self._safe_cast_field(chunk.get('created_at'), 'created_at', 'str', datetime.now().isoformat(), 64),
                    'source_model': self._safe_cast_field(chunk.get('source_model'), 'source_model', 'str', 'bge-m3', 64),
                    'tags': self._safe_cast_field(chunk.get('tags'), 'tags', 'str', 'ç„¡æ¨™ç±¤', 1024)
                }
                
                embedding = chunk.get('embedding', [])
                if isinstance(embedding, list) and len(embedding) == 1024:
                    single_chunk_data['embedding'] = embedding
                else:
                    single_chunk_data['embedding'] = [0.0] * 1024
                
                self.collection.insert([single_chunk_data])
            
            self.collection.flush()
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(processed_chunks)} å€‹ chunks åˆ° Milvus")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ’å…¥ Milvus å¤±æ•—: {str(e)}")
            return False

class StableBatchProcessor:
    """ç©©å®šçš„æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(self):
        current_dir = Path(__file__).parent
        project_root = current_dir.parent.parent
        self.stage3_path = project_root / "backend/vector_pipeline/data/stage3_tagging"
        self.output_path = project_root / "backend/vector_pipeline/data/stage4_embedding_prep"
        self.progress_file = project_root / "backend/utils/batch_progress.pkl"
        
        self.pg_config = {
            'host': os.getenv('POSTGRES_HOST', 'postgres.podwise.svc.cluster.local'),
            'port': os.getenv('POSTGRES_PORT', '5432'),
            'database': os.getenv('POSTGRES_DB', 'podcast'),
            'user': os.getenv('POSTGRES_USER', 'bdse37'),
            'password': os.getenv('POSTGRES_PASSWORD', '111111')
        }
        
        self.embedding_generator = BGE_M3_Embedding()
        self.milvus_connector = MilvusConnector()
        
        self.total_files = 0
        self.processed_files = 0
        self.failed_files = []
        self.start_time = None
        self.processed_file_list = set()
        
        self.output_path.mkdir(exist_ok=True)
        self.load_progress()
    
    def load_progress(self):
        """è¼‰å…¥é€²åº¦"""
        try:
            if self.progress_file.exists():
                with open(self.progress_file, 'rb') as f:
                    progress_data = pickle.load(f)
                    self.processed_files = progress_data.get('processed_files', 0)
                    self.processed_file_list = set(progress_data.get('processed_file_list', []))
                    logger.info(f"ğŸ“‹ è¼‰å…¥é€²åº¦ï¼šå·²è™•ç† {self.processed_files} å€‹æª”æ¡ˆ")
        except Exception as e:
            logger.warning(f"è¼‰å…¥é€²åº¦å¤±æ•—: {str(e)}")
    
    def save_progress(self):
        """å„²å­˜é€²åº¦"""
        try:
            progress_data = {
                'processed_files': self.processed_files,
                'processed_file_list': list(self.processed_file_list),
                'timestamp': datetime.now().isoformat()
            }
            with open(self.progress_file, 'wb') as f:
                pickle.dump(progress_data, f)
        except Exception as e:
            logger.warning(f"å„²å­˜é€²åº¦å¤±æ•—: {str(e)}")
    
    def clear_output_directory(self):
        """æ¸…ç©ºè¼¸å‡ºç›®éŒ„"""
        try:
            import shutil
            if self.output_path.exists():
                shutil.rmtree(self.output_path)
            self.output_path.mkdir(exist_ok=True)
            logger.info("âœ… å·²æ¸…ç©º stage4_embedding_prep ç›®éŒ„")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç›®éŒ„å¤±æ•—: {str(e)}")
    
    def scan_all_files(self) -> List[Path]:
        """æƒææ‰€æœ‰éœ€è¦è™•ç†çš„æª”æ¡ˆ"""
        all_files = []
        rss_dirs = [d for d in self.stage3_path.iterdir() if d.is_dir() and d.name.startswith("RSS_")]
        
        for rss_dir in rss_dirs:
            json_files = list(rss_dir.glob("*.json"))
            all_files.extend(json_files)
        
        self.total_files = len(all_files)
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(rss_dirs)} å€‹ RSS ç›®éŒ„ï¼Œ{self.total_files} å€‹ JSON æª”æ¡ˆ")
        return all_files
    
    def get_postgresql_episodes(self) -> Dict[str, Dict]:
        """å¾ PostgreSQL å–å¾—å®Œæ•´çš„ episodes å’Œ podcasts è³‡æ–™"""
        try:
            conn = psycopg2.connect(**self.pg_config)
            cursor = conn.cursor()
            
            query = """
            SELECT 
                e.episode_id,
                e.episode_title,
                e.duration,
                e.published_date,
                e.apple_episodes_ranking,
                e.created_at,
                e.languages,
                p.podcast_id,
                p.podcast_name,
                p.author,
                p.category,
                p.apple_rating as apple_rating
            FROM episodes e
            JOIN podcasts p ON e.podcast_id = p.podcast_id
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            episodes = {}
            for row in results:
                episode_id, episode_title, duration, published_date, apple_episodes_ranking, created_at, \
                languages, podcast_id, podcast_name, author, category, apple_rating = row
                
                episodes[str(episode_id)] = {
                    'episode_id': episode_id,
                    'episode_title': episode_title,
                    'duration': duration,
                    'published_date': published_date,
                    'apple_episodes_ranking': apple_episodes_ranking,
                    'created_at': created_at,
                    'languages': languages,
                    'podcast_id': podcast_id,
                    'podcast_name': podcast_name,
                    'author': author,
                    'category': category,
                    'apple_rating': apple_rating
                }
            
            cursor.close()
            conn.close()
            
            logger.info(f"âœ… å¾ PostgreSQL å–å¾— {len(episodes)} ç­† episodes è³‡æ–™")
            return episodes
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è©¢ PostgreSQL å¤±æ•—: {str(e)}")
            return {}
    
    def get_podcast_info(self, podcast_id: int) -> Optional[Dict]:
        """å¾ PostgreSQL å–å¾— podcast çš„åŸºæœ¬è³‡è¨Š"""
        try:
            conn = psycopg2.connect(**self.pg_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT podcast_id, podcast_name, author, category, apple_rating
                FROM podcasts
                WHERE podcast_id = %s
            """, (podcast_id,))
            
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if result:
                podcast_id, podcast_name, author, category, apple_rating = result
                return {
                    'podcast_id': podcast_id,
                    'podcast_name': podcast_name,
                    'author': author,
                    'category': category,
                    'apple_rating': apple_rating
                }
            else:
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢ podcast è³‡è¨Šå¤±æ•—: {str(e)}")
            return None

    def get_podcast_stats(self, podcast_id: int) -> Optional[Dict]:
        """å–å¾— podcast çš„çµ±è¨ˆè³‡è¨Š"""
        try:
            conn = psycopg2.connect(**self.pg_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT 
                    COUNT(duration) as episodes_with_duration,
                    AVG(duration) as avg_duration,
                    MIN(published_date) as earliest_date,
                    MAX(published_date) as latest_date
                FROM episodes
                WHERE podcast_id = %s AND duration IS NOT NULL
            """, (podcast_id,))
            
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if result and result[0] > 0:
                episodes_with_duration, avg_duration, earliest_date, latest_date = result
                return {
                    'avg_duration': int(avg_duration) if avg_duration else None,
                    'earliest_date': earliest_date,
                    'latest_date': latest_date,
                    'episodes_with_duration': episodes_with_duration
                }
            else:
                return None
                
        except Exception as e:
            logger.error(f"å–å¾— podcast çµ±è¨ˆè³‡è¨Šå¤±æ•—: {str(e)}")
            return None

    def extract_info_from_filename(self, filename: str) -> Tuple[Optional[int], Optional[str], Optional[str], Optional[str]]:
        """å¾æª”æ¡ˆåç¨±ä¸­æå–è³‡è¨Š"""
        try:
            name = filename.replace('.json', '')
            parts = name.split('_')
            if len(parts) >= 3 and parts[0] == 'RSS':
                podcast_id = int(parts[1])
                episode_title_parts = parts[2:]
                episode_title = '_'.join(episode_title_parts)
                ep_match = None
                for part in episode_title_parts:
                    if part.startswith('EP'):
                        ep_match = part.split()[0]
                        break
                main_title = episode_title
                if episode_title.startswith('podcast_'):
                    main_title = episode_title.split('_', 2)[-1]
                if main_title.startswith('EP'):
                    main_title = main_title.split(' ', 1)[-1] if ' ' in main_title else main_title
                return podcast_id, episode_title, ep_match, main_title
            return None, None, None, None
        except Exception as e:
            logger.error(f"è§£ææª”æ¡ˆåç¨±å¤±æ•— {filename}: {str(e)}")
            return None, None, None, None

    def find_postgresql_episode(self, podcast_id: int, ep_match: Optional[str], main_title: Optional[str], pg_episodes: Dict) -> Optional[Dict]:
        """æ ¹æ“š podcast_idã€EP ç·¨è™Ÿæˆ–ä¸»æ¨™é¡Œæ‰¾åˆ°å°æ‡‰çš„ PostgreSQL episode"""
        try:
            matching_episodes = []
            
            for episode_id, episode_data in pg_episodes.items():
                if episode_data['podcast_id'] == podcast_id:
                    if ep_match and ep_match in episode_data['episode_title']:
                        matching_episodes.append(episode_data)
                    elif main_title and main_title in episode_data['episode_title']:
                        matching_episodes.append(episode_data)
            
            if matching_episodes:
                for episode in matching_episodes:
                    if episode['duration'] is not None and episode['published_date'] is not None:
                        return episode
                return matching_episodes[0]
            else:
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢ PostgreSQL episode å¤±æ•—: {str(e)}")
            return None
    
    def ensure_field_has_value(self, value: Any, field_name: str, default_value: Any, max_length: Optional[int] = None) -> Any:
        """ç¢ºä¿æ¬„ä½æœ‰å€¼"""
        if value is None or value == '' or value == 'null' or value == 'None':
            return default_value
        
        if hasattr(value, '__class__') and value.__class__.__name__ == 'Decimal':
            value = float(value)
        
        if field_name == 'published_date' and value is not None:
            if isinstance(value, str):
                return str(value)[:64]
            elif hasattr(value, 'isoformat'):
                return value.isoformat()[:64]
            else:
                return str(value)[:64]
        
        if isinstance(value, str) and max_length:
            return str(value)[:max_length]
        
        return value
    
    def process_single_file(self, file_path: Path, pg_episodes: Dict) -> Tuple[List[Dict], Dict]:
        """è™•ç†å–®ä¸€ JSON æª”æ¡ˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            podcast_id, episode_title, ep_match, main_title = self.extract_info_from_filename(file_path.name)
            
            if podcast_id is None:
                raise Exception("ç„¡æ³•è§£ææª”æ¡ˆåç¨±")
            
            fixed_category = "unknown"
            if "podcast_1321" in file_path.name:
                fixed_category = "å•†æ¥­"
            elif "podcast_1304" in file_path.name:
                fixed_category = "æ•™è‚²"
            
            pg_episode = self.find_postgresql_episode(podcast_id, ep_match, main_title, pg_episodes)
            podcast_info = self.get_podcast_info(podcast_id)
            podcast_stats = self.get_podcast_stats(podcast_id)
            
            chunks = data.get('chunks', [])
            processed_chunks = []
            
            for i, chunk in enumerate(chunks):
                original_chunk_text = chunk.get('chunk_text', '')
                original_chunk_index = chunk.get('chunk_index', i)
                
                if not original_chunk_text or original_chunk_text.strip() == '':
                    original_chunk_text = 'ç„¡å…§å®¹'
                
                embedding = self.embedding_generator.generate_embedding(original_chunk_text)
                
                enhanced_tags = chunk.get('enhanced_tags', [])
                if isinstance(enhanced_tags, list):
                    tags_str = ','.join(enhanced_tags)
                else:
                    tags_str = str(enhanced_tags) if enhanced_tags else 'ç„¡æ¨™ç±¤'
                
                if len(tags_str) > 1024:
                    tags_str = tags_str[:1021] + "..."
                
                if pg_episode:
                    processed_chunk = {
                        'chunk_id': self.ensure_field_has_value(chunk.get('chunk_id', f"{pg_episode['episode_id']}_{i}"), 'chunk_id', f"{pg_episode['episode_id']}_{i}", 64),
                        'chunk_index': self.ensure_field_has_value(original_chunk_index, 'chunk_index', i),
                        'episode_id': self.ensure_field_has_value(pg_episode['episode_id'], 'episode_id', 0),
                        'podcast_id': self.ensure_field_has_value(pg_episode['podcast_id'], 'podcast_id', 0),
                        'podcast_name': self.ensure_field_has_value(pg_episode['podcast_name'], 'podcast_name', 'æœªçŸ¥æ’­å®¢', 255),
                        'author': self.ensure_field_has_value(pg_episode['author'], 'author', 'æœªçŸ¥ä½œè€…', 255),
                        'category': self.ensure_field_has_value(fixed_category, 'category', 'æœªçŸ¥åˆ†é¡', 64),
                        'episode_title': self.ensure_field_has_value(pg_episode['episode_title'], 'episode_title', 'æœªçŸ¥æ¨™é¡Œ', 255),
                        'duration': self.ensure_field_has_value(pg_episode['duration'], 'duration', 'æœªçŸ¥æ™‚é•·', 255),
                        'published_date': self.ensure_field_has_value(pg_episode['published_date'], 'published_date', 'æœªçŸ¥æ—¥æœŸ', 64),
                        'apple_rating': self.ensure_field_has_value(pg_episode['apple_rating'], 'apple_rating', 0),
                        'chunk_text': self.ensure_field_has_value(original_chunk_text, 'chunk_text', 'ç„¡å…§å®¹', 1024),
                        'embedding': embedding,
                        'language': self.ensure_field_has_value(pg_episode['languages'], 'language', 'zh', 16),
                        'created_at': self.ensure_field_has_value(pg_episode['created_at'].isoformat() if pg_episode['created_at'] else datetime.now().isoformat(), 'created_at', datetime.now().isoformat(), 64),
                        'source_model': 'bge-m3',
                        'tags': self.ensure_field_has_value(tags_str, 'tags', 'ç„¡æ¨™ç±¤', 1024)
                    }
                else:
                    if podcast_info:
                        fallback_duration = podcast_stats['avg_duration'] if podcast_stats and podcast_stats['avg_duration'] else 'æœªçŸ¥æ™‚é•·'
                        fallback_date = str(podcast_stats['earliest_date']) if podcast_stats and podcast_stats['earliest_date'] else 'æœªçŸ¥æ—¥æœŸ'
                        
                        processed_chunk = {
                            'chunk_id': self.ensure_field_has_value(chunk.get('chunk_id', f"unknown_{i}"), 'chunk_id', f"unknown_{i}", 64),
                            'chunk_index': self.ensure_field_has_value(original_chunk_index, 'chunk_index', i),
                            'episode_id': 0,
                            'podcast_id': self.ensure_field_has_value(podcast_id, 'podcast_id', 0),
                            'podcast_name': self.ensure_field_has_value(podcast_info['podcast_name'], 'podcast_name', 'æœªçŸ¥æ’­å®¢', 255),
                            'author': self.ensure_field_has_value(podcast_info['author'], 'author', 'æœªçŸ¥ä½œè€…', 255),
                            'category': self.ensure_field_has_value(fixed_category, 'category', 'æœªçŸ¥åˆ†é¡', 64),
                            'episode_title': self.ensure_field_has_value(main_title, 'episode_title', 'æœªçŸ¥æ¨™é¡Œ', 255),
                            'duration': self.ensure_field_has_value(fallback_duration, 'duration', 'æœªçŸ¥æ™‚é•·', 255),
                            'published_date': self.ensure_field_has_value(fallback_date, 'published_date', 'æœªçŸ¥æ—¥æœŸ', 64),
                            'apple_rating': self.ensure_field_has_value(podcast_info['apple_rating'], 'apple_rating', 0),
                            'chunk_text': self.ensure_field_has_value(original_chunk_text, 'chunk_text', 'ç„¡å…§å®¹', 1024),
                            'embedding': embedding,
                            'language': 'zh',
                            'created_at': datetime.now().isoformat(),
                            'source_model': 'bge-m3',
                            'tags': self.ensure_field_has_value(tags_str, 'tags', 'ç„¡æ¨™ç±¤', 1024)
                        }
                    else:
                        processed_chunk = {
                            'chunk_id': self.ensure_field_has_value(chunk.get('chunk_id', f"unknown_{i}"), 'chunk_id', f"unknown_{i}", 64),
                            'chunk_index': self.ensure_field_has_value(original_chunk_index, 'chunk_index', i),
                            'episode_id': 0,
                            'podcast_id': self.ensure_field_has_value(podcast_id, 'podcast_id', 0),
                            'podcast_name': 'æœªçŸ¥æ’­å®¢',
                            'author': 'æœªçŸ¥ä½œè€…',
                            'category': self.ensure_field_has_value(fixed_category, 'category', 'æœªçŸ¥åˆ†é¡', 64),
                            'episode_title': self.ensure_field_has_value(main_title, 'episode_title', 'æœªçŸ¥æ¨™é¡Œ', 255),
                            'duration': 'æœªçŸ¥æ™‚é•·',
                            'published_date': 'æœªçŸ¥æ—¥æœŸ',
                            'apple_rating': 0,
                            'chunk_text': self.ensure_field_has_value(original_chunk_text, 'chunk_text', 'ç„¡å…§å®¹', 1024),
                            'embedding': embedding,
                            'language': 'zh',
                            'created_at': datetime.now().isoformat(),
                            'source_model': 'bge-m3',
                            'tags': self.ensure_field_has_value(tags_str, 'tags', 'ç„¡æ¨™ç±¤', 1024)
                        }
                
                processed_chunks.append(processed_chunk)
            
            fixed_data = {
                'filename': file_path.name,
                'episode_id': str(pg_episode['episode_id']) if pg_episode else 'unknown',
                'collection_name': f"RSS_{podcast_id}" if podcast_id else 'unknown',
                'total_chunks': len(processed_chunks),
                'chunks': processed_chunks,
                'fixed_at': datetime.now().isoformat(),
                'source_model': 'bge-m3'
            }
            
            return processed_chunks, fixed_data
            
        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆ {file_path} å¤±æ•—: {str(e)}")
            return [], {}
    
    def save_fixed_json(self, fixed_data: Dict, original_file_path: Path):
        """å„²å­˜ä¿®æ­£å¾Œçš„ JSON æª”æ¡ˆ"""
        try:
            output_dir = self.output_path / original_file_path.parent.name
            output_dir.mkdir(exist_ok=True)
            
            output_file = output_dir / original_file_path.name
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(fixed_data, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜ä¿®æ­£å¾Œçš„æª”æ¡ˆå¤±æ•—: {str(e)}")
            return False
    
    def estimate_remaining_time(self) -> str:
        """é ä¼°å‰©é¤˜æ™‚é–“"""
        if self.processed_files == 0 or not self.start_time:
            return "è¨ˆç®—ä¸­..."
        
        elapsed_time = time.time() - self.start_time
        avg_time_per_file = elapsed_time / self.processed_files
        remaining_files = self.total_files - self.processed_files
        remaining_time = avg_time_per_file * remaining_files
        
        return str(timedelta(seconds=int(remaining_time)))
    
    def process_all_files(self):
        """è™•ç†æ‰€æœ‰æª”æ¡ˆ"""
        print("\n" + "="*80)
        print("ğŸš€ é–‹å§‹ç©©å®šçš„æ‰¹æ¬¡è™•ç† stage3_tagging åˆ° stage4_embedding_prep")
        print("="*80)
        
        graceful_exit = GracefulExit()
        
        # 1. æ¸…ç©ºè¼¸å‡ºç›®éŒ„
        print("ğŸ“ æ¸…ç©º stage4_embedding_prep ç›®éŒ„...")
        self.clear_output_directory()
        
        # 2. æª¢æŸ¥ BGE-M3 æ¨¡å‹
        print("ğŸ¤– æª¢æŸ¥ BGE-M3 æ¨¡å‹...")
        if not self.embedding_generator.model:
            print("âŒ BGE-M3 æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
            return
        
        # 3. é€£ç·š Milvus
        print("ğŸ”— é€£ç·š Milvus...")
        if not self.milvus_connector.connect():
            print("âŒ Milvus é€£ç·šå¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
            return
        
        # 4. æƒææ‰€æœ‰æª”æ¡ˆ
        print("ğŸ“‹ æƒææ‰€æœ‰æª”æ¡ˆ...")
        all_files = self.scan_all_files()
        if not all_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°éœ€è¦è™•ç†çš„æª”æ¡ˆ")
            return
        
        # 5. å–å¾— PostgreSQL è³‡æ–™
        print("ğŸ—„ï¸ å–å¾— PostgreSQL è³‡æ–™...")
        pg_episodes = self.get_postgresql_episodes()
        if not pg_episodes:
            print("âŒ ç„¡æ³•å–å¾— PostgreSQL è³‡æ–™")
            return
        
        # 6. é–‹å§‹æ‰¹æ¬¡è™•ç†
        print(f"\nğŸ”„ é–‹å§‹è™•ç† {self.total_files} å€‹æª”æ¡ˆ...")
        self.start_time = time.time()
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
        with tqdm(total=self.total_files, desc="è™•ç†é€²åº¦", unit="æª”æ¡ˆ") as pbar:
            pbar.update(self.processed_files)  # æ›´æ–°å·²è™•ç†çš„é€²åº¦
            
            for file_path in all_files:
                # æª¢æŸ¥æ˜¯å¦å·²ç¶“è™•ç†é
                if str(file_path) in self.processed_file_list:
                    pbar.update(1)
                    continue
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
                if graceful_exit.kill_now:
                    logger.info("æ”¶åˆ°é€€å‡ºä¿¡è™Ÿï¼Œæ­£åœ¨ä¿å­˜é€²åº¦...")
                    self.save_progress()
                    break
                
                try:
                    # è™•ç†æª”æ¡ˆ
                    processed_chunks, fixed_data = self.process_single_file(file_path, pg_episodes)
                    
                    if processed_chunks and fixed_data:
                        # å„²å­˜ä¿®æ­£å¾Œçš„ JSON æª”æ¡ˆ
                        if self.save_fixed_json(fixed_data, file_path):
                            # æ’å…¥åˆ° Milvus
                            if self.milvus_connector.insert_data(processed_chunks):
                                self.processed_files += 1
                                self.processed_file_list.add(str(file_path))
                                self.save_progress()  # æ¯è™•ç†å®Œä¸€å€‹æª”æ¡ˆå°±ä¿å­˜é€²åº¦
                                
                                pbar.set_postfix({
                                    'æˆåŠŸ': self.processed_files,
                                    'å‰©é¤˜æ™‚é–“': self.estimate_remaining_time()
                                })
                            else:
                                self.failed_files.append({
                                    'file': str(file_path),
                                    'reason': 'Milvus æ’å…¥å¤±æ•—',
                                    'chunks_count': len(processed_chunks)
                                })
                        else:
                            self.failed_files.append({
                                'file': str(file_path),
                                'reason': 'JSON å„²å­˜å¤±æ•—',
                                'chunks_count': len(processed_chunks)
                            })
                    else:
                        self.failed_files.append({
                            'file': str(file_path),
                            'reason': 'æª”æ¡ˆè™•ç†å¤±æ•—',
                            'chunks_count': 0
                        })
                    
                except Exception as e:
                    error_msg = f"è™•ç†ç•°å¸¸: {str(e)}"
                    self.failed_files.append({
                        'file': str(file_path),
                        'reason': error_msg,
                        'chunks_count': 0
                    })
                    logger.error(f"è™•ç†æª”æ¡ˆ {file_path} ç•°å¸¸: {str(e)}")
                
                pbar.update(1)
        
        # 7. è¼¸å‡ºçµæœ
        total_time = time.time() - self.start_time if self.start_time else 0
        print("\n" + "="*80)
        print("ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {self.total_files}")
        print(f"   æˆåŠŸè™•ç†: {self.processed_files}")
        print(f"   å¤±æ•—æª”æ¡ˆ: {len(self.failed_files)}")
        print(f"   ç¸½è€—æ™‚: {str(timedelta(seconds=int(total_time)))}")
        if total_time > 0:
            print(f"   å¹³å‡é€Ÿåº¦: {self.total_files/total_time:.2f} æª”æ¡ˆ/ç§’")
        
        if self.failed_files:
            print(f"\nâŒ å¤±æ•—æª”æ¡ˆæ¸…å–®:")
            for failed in self.failed_files:
                print(f"   {failed['file']}: {failed['reason']}")
            
            fail_log_path = self.output_path / "fail_log.json"
            with open(fail_log_path, 'w', encoding='utf-8') as f:
                json.dump(self.failed_files, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ å¤±æ•—æ¸…å–®å·²å„²å­˜è‡³: {fail_log_path}")
        
        print("\nâœ… æ‰€æœ‰è™•ç†å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•¸"""
    processor = StableBatchProcessor()
    processor.process_all_files()

if __name__ == "__main__":
    main() 