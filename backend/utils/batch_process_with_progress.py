#!/usr/bin/env python3
"""
å®Œæ•´çš„æ‰¹æ¬¡è™•ç†è…³æœ¬
- é€²åº¦æ¢é¡¯ç¤ºèˆ‡é ä¼°å‰©é¤˜æ™‚é–“
- åš´æ ¼æª¢æŸ¥ BGE-M3 æ¨¡å‹è¼‰å…¥
- è‡ªå‹•é€£ç·š Milvus (192.168.32.86)
- ä¿ç•™æ‰€æœ‰æ¬„ä½ (chunk_text, chunk_index, published_dateç­‰)
- å¤±æ•—æª”æ¡ˆè¨˜éŒ„
"""

import os
import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple
import logging
import psycopg2
from datetime import datetime, timedelta
from dotenv import load_dotenv
from pymilvus import connections, Collection, utility
import numpy as np
from tqdm import tqdm
import traceback

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv('backend/.env')

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BGE_M3_Embedding:
    """BGE-M3 embedding ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.model = None
        self.device = 'cuda'
        self.load_model()
    
    def load_model(self):
        """è¼‰å…¥ BGE-M3 æ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            logger.info(f"å˜—è©¦è¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° {self.device}...")
            
            # å˜—è©¦è¼‰å…¥åˆ° GPU
            self.model = SentenceTransformer('BAAI/bge-m3', device=self.device)
            
            # æ¸¬è©¦æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
            test_text = "æ¸¬è©¦æ–‡æœ¬"
            test_embedding = self.model.encode(test_text, normalize_embeddings=True)
            
            if len(test_embedding) == 1024 and not np.allclose(test_embedding, 0):
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° {self.device}")
                return True
            else:
                raise Exception("æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼šembedding ç‚ºé›¶å‘é‡")
                
        except Exception as e:
            logger.warning(f"GPU è¼‰å…¥å¤±æ•—: {str(e)}")
            
            # å˜—è©¦è¼‰å…¥åˆ° CPU
            try:
                logger.info("å˜—è©¦è¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° CPU...")
                self.device = 'cpu'
                self.model = SentenceTransformer('BAAI/bge-m3', device=self.device)
                
                # æ¸¬è©¦æ¨¡å‹
                test_text = "æ¸¬è©¦æ–‡æœ¬"
                test_embedding = self.model.encode(test_text, normalize_embeddings=True)
                
                if len(test_embedding) == 1024 and not np.allclose(test_embedding, 0):
                    logger.info("âœ… æˆåŠŸè¼‰å…¥ BGE-M3 æ¨¡å‹åˆ° CPU")
                    return True
                else:
                    raise Exception("æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼šembedding ç‚ºé›¶å‘é‡")
                    
            except Exception as e2:
                logger.error(f"CPU è¼‰å…¥ä¹Ÿå¤±æ•—: {str(e2)}")
                logger.error("âŒ ç„¡æ³•è¼‰å…¥ BGE-M3 æ¨¡å‹ï¼Œè«‹æª¢æŸ¥å®‰è£")
                return False
    
    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆ BGE-M3 embedding"""
        if not self.model:
            logger.error("BGE-M3 æ¨¡å‹æœªè¼‰å…¥")
            return [0.0] * 1024
        
        try:
            # ç”Ÿæˆ embedding
            embedding = self.model.encode(text, normalize_embeddings=True)
            embedding_list = embedding.tolist()
            
            # æª¢æŸ¥ embedding æ˜¯å¦ç‚ºé›¶å‘é‡
            if np.allclose(embedding, 0):
                logger.warning(f"âš ï¸ è­¦å‘Šï¼šæ–‡æœ¬ '{text[:50]}...' ç”¢ç”Ÿé›¶å‘é‡ embedding")
            
            return embedding_list
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ embedding å¤±æ•—: {str(e)}")
            return [0.0] * 1024

class MilvusConnector:
    """Milvus é€£ç·šå™¨"""
    
    def __init__(self):
        self.host = "192.168.32.86"
        self.user = "bdse37"
        self.password = "111111"
        self.collection_name = "podcast_chunks"
        self.connected = False
        self.collection = None
    
    def _safe_cast_field(self, value: Any, field_name: str, field_type: str, 
                        default_value: Any, max_length: Optional[int] = None) -> Any:
        """
        å¼·åŒ–å‹æ…‹èˆ‡é•·åº¦é˜²å‘†ï¼Œå®Œå…¨ç¬¦åˆ Milvus schema
        """
        try:
            # None è™•ç†
            if value is None:
                return default_value
            # å­—ä¸²å‹æ…‹
            if field_type == 'str':
                if isinstance(value, list):
                    result = str(value[0]) if value else str(default_value)
                elif isinstance(value, dict):
                    import json
                    result = json.dumps(value, ensure_ascii=False)
                else:
                    result = str(value)
                result = result[:max_length] if max_length else result
                return result
            # INT å‹æ…‹
            elif field_type == 'int':
                try:
                    return int(value)
                except Exception:
                    return default_value
            # FLOAT å‹æ…‹
            elif field_type == 'float':
                try:
                    return float(value)
                except Exception:
                    return default_value
            # LIST å‹æ…‹ï¼ˆåƒ… embedding ç”¨ï¼‰
            elif field_type == 'list':
                if isinstance(value, list):
                    # embedding å¿…é ˆ 1024 ç¶­
                    if field_name == 'embedding':
                        if len(value) != 1024:
                            return [0.0]*1024
                        return [float(x) for x in value]
                    return value
                else:
                    return [0.0]*1024 if field_name == 'embedding' else default_value
            else:
                return default_value
        except Exception as e:
            logger.warning(f"æ¬„ä½ {field_name} è½‰æ›å¤±æ•—: {str(e)}ï¼Œä½¿ç”¨é è¨­å€¼")
            if field_type == 'list' and field_name == 'embedding':
                return [0.0]*1024
            return default_value

    def connect(self) -> bool:
        """é€£ç·šåˆ° Milvus"""
        try:
            # å˜—è©¦å¸¸è¦‹çš„ port
            ports = [19530, 9091, 9000]
            
            for port in ports:
                try:
                    logger.info(f"å˜—è©¦é€£ç·š Milvus: {self.host}:{port}")
                    connections.connect(
                        alias="default",
                        host=self.host,
                        port=port,
                        user=self.user,
                        password=self.password
                    )
                    
                    # æ¸¬è©¦é€£ç·š
                    utility.list_collections()
                    logger.info(f"âœ… æˆåŠŸé€£ç·š Milvus: {self.host}:{port}")
                    
                    # æª¢æŸ¥ collection æ˜¯å¦å­˜åœ¨
                    if utility.has_collection(self.collection_name):
                        self.collection = Collection(self.collection_name)
                        self.collection.load()
                        logger.info(f"âœ… æˆåŠŸè¼‰å…¥ collection: {self.collection_name}")
                        self.connected = True
                        return True
                    else:
                        logger.error(f"âŒ Collection '{self.collection_name}' ä¸å­˜åœ¨")
                        return False
                        
                except Exception as e:
                    logger.warning(f"Port {port} é€£ç·šå¤±æ•—: {str(e)}")
                    continue
            
            logger.error("âŒ æ‰€æœ‰ port éƒ½ç„¡æ³•é€£ç·š Milvus")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Milvus é€£ç·šå¤±æ•—: {str(e)}")
            return False
    
    def insert_data(self, processed_chunks: List[Dict]) -> bool:
        """æ’å…¥è³‡æ–™åˆ° Milvusï¼Œæ’å…¥å‰åš´æ ¼é©—è­‰æ‰€æœ‰æ¬„ä½"""
        if not self.connected or not self.collection:
            logger.error("âŒ Milvus æœªé€£ç·š")
            return False
        try:
            for chunk in processed_chunks:
                # åš´æ ¼ä¾ schema è™•ç†æ‰€æœ‰æ¬„ä½
                single_chunk_data = {
                    'chunk_id': self._safe_cast_field(chunk.get('chunk_id'), 'chunk_id', 'str', '', 64),
                    'chunk_index': self._safe_cast_field(chunk.get('chunk_index'), 'chunk_index', 'int', 0),
                    'episode_id': self._safe_cast_field(chunk.get('episode_id'), 'episode_id', 'int', 0),
                    'podcast_id': self._safe_cast_field(chunk.get('podcast_id'), 'podcast_id', 'int', 0),
                    'podcast_name': self._safe_cast_field(chunk.get('podcast_name'), 'podcast_name', 'str', 'æœªçŸ¥æ’­å®¢', 255),
                    'author': self._safe_cast_field(chunk.get('author'), 'author', 'str', 'æœªçŸ¥ä½œè€…', 255),
                    'category': self._safe_cast_field(chunk.get('category'), 'category', 'str', 'æœªçŸ¥åˆ†é¡', 64),
                    'episode_title': self._safe_cast_field(chunk.get('episode_title'), 'episode_title', 'str', 'æœªçŸ¥æ¨™é¡Œ', 255),
                    'duration': self._safe_cast_field(chunk.get('duration'), 'duration', 'str', 'æœªçŸ¥æ™‚é•·', 255),
                    'published_date': self._safe_cast_field(chunk.get('published_date'), 'published_date', 'str', 'æœªçŸ¥æ—¥æœŸ', 64),
                    'apple_rating': self._safe_cast_field(chunk.get('apple_rating'), 'apple_rating', 'int', 0),
                    'chunk_text': self._safe_cast_field(chunk.get('chunk_text'), 'chunk_text', 'str', 'ç„¡å…§å®¹', 1024),
                    'embedding': self._safe_cast_field(chunk.get('embedding'), 'embedding', 'list', [0.0]*1024),
                    'language': self._safe_cast_field(chunk.get('language'), 'language', 'str', 'zh', 16),
                    'created_at': self._safe_cast_field(chunk.get('created_at'), 'created_at', 'str', datetime.now().isoformat(), 64),
                    'source_model': self._safe_cast_field(chunk.get('source_model'), 'source_model', 'str', 'bge-m3', 64),
                    'tags': self._safe_cast_field(chunk.get('tags'), 'tags', 'str', 'ç„¡æ¨™ç±¤', 1024)
                }
                self.collection.insert([single_chunk_data])
            self.collection.flush()
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(processed_chunks)} å€‹ chunks åˆ° Milvus")
            return True
        except Exception as e:
            logger.error(f"âŒ æ’å…¥ Milvus å¤±æ•—: {str(e)}")
            return False

class BatchProcessor:
    """æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(self):
        # ä½¿ç”¨çµ•å°è·¯å¾‘ï¼Œç¢ºä¿å¾ä»»ä½•ç›®éŒ„åŸ·è¡Œéƒ½èƒ½æ­£ç¢ºæ‰¾åˆ°æª”æ¡ˆ
        current_dir = Path(__file__).parent
        project_root = current_dir.parent.parent
        self.stage3_path = project_root / "backend/vector_pipeline/data/stage3_tagging"
        self.output_path = project_root / "backend/vector_pipeline/data/stage4_embedding_prep"
        self.progress_file = current_dir / "processing_progress.json"
        
        # PostgreSQL é€£æ¥è¨­å®š
        self.pg_config = {
            'host': os.getenv('POSTGRES_HOST', '10.233.50.117'),
            'port': os.getenv('POSTGRES_PORT', '5432'),
            'database': os.getenv('POSTGRES_DB', 'podcast'),
            'user': os.getenv('POSTGRES_USER', 'bdse37'),
            'password': os.getenv('POSTGRES_PASSWORD', '111111')
        }
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.embedding_generator = BGE_M3_Embedding()
        self.milvus_connector = MilvusConnector()
        
        # çµ±è¨ˆè³‡è¨Š
        self.total_files = 0
        self.processed_files = 0
        self.failed_files = []
        self.start_time = None
        
        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        self.output_path.mkdir(exist_ok=True)
        
        # è¼‰å…¥å·²è™•ç†çš„æª”æ¡ˆæ¸…å–®
        self.processed_files_set = self.load_processed_files()
    
    def clear_output_directory(self):
        """æ¸…ç©ºè¼¸å‡ºç›®éŒ„"""
        try:
            import shutil
            if self.output_path.exists():
                shutil.rmtree(self.output_path)
            self.output_path.mkdir(exist_ok=True)
            logger.info("âœ… å·²æ¸…ç©º stage4_embedding_prep ç›®éŒ„")
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç›®éŒ„å¤±æ•—: {str(e)}")
    
    def load_processed_files(self) -> Set[str]:
        """è¼‰å…¥å·²è™•ç†çš„æª”æ¡ˆæ¸…å–®"""
        processed_files = set()
        try:
            if self.progress_file.exists():
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    processed_files = set(progress_data.get('processed_files', []))
                logger.info(f"ğŸ“‹ è¼‰å…¥å·²è™•ç†æª”æ¡ˆæ¸…å–®: {len(processed_files)} å€‹æª”æ¡ˆ")
            else:
                logger.info("ğŸ“‹ æ²’æœ‰æ‰¾åˆ°é€²åº¦æª”æ¡ˆï¼Œå°‡å¾é ­é–‹å§‹è™•ç†")
        except Exception as e:
            logger.warning(f"âš ï¸ è¼‰å…¥é€²åº¦æª”æ¡ˆå¤±æ•—: {str(e)}ï¼Œå°‡å¾é ­é–‹å§‹è™•ç†")
        return processed_files
    
    def save_progress(self):
        """ä¿å­˜è™•ç†é€²åº¦"""
        try:
            # æƒæå·²è™•ç†çš„æª”æ¡ˆ
            processed_files = []
            if self.output_path.exists():
                for json_file in self.output_path.rglob("*.json"):
                    processed_files.append(str(json_file))
            
            progress_info = {
                "timestamp": datetime.now().isoformat(),
                "processed_files_count": len(processed_files),
                "processed_files": processed_files,
                "output_path": str(self.output_path),
                "note": "æ‰¹æ¬¡è™•ç†é€²åº¦ä¿å­˜"
            }
            
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ é€²åº¦å·²ä¿å­˜: {len(processed_files)} å€‹æª”æ¡ˆ")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é€²åº¦å¤±æ•—: {str(e)}")
    
    def scan_all_files(self) -> List[Path]:
        """æƒææ‰€æœ‰éœ€è¦è™•ç†çš„æª”æ¡ˆï¼Œè·³éå·²è™•ç†çš„æª”æ¡ˆ"""
        all_files = []
        rss_dirs = [d for d in self.stage3_path.iterdir() if d.is_dir() and d.name.startswith("RSS_")]
        
        for rss_dir in rss_dirs:
            json_files = list(rss_dir.glob("*.json"))
            all_files.extend(json_files)
        
        # éæ¿¾æ‰å·²è™•ç†çš„æª”æ¡ˆ
        unprocessed_files = []
        for file_path in all_files:
            # æª¢æŸ¥å°æ‡‰çš„è¼¸å‡ºæª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
            relative_path = file_path.relative_to(self.stage3_path)
            output_file = self.output_path / relative_path
            if str(output_file) not in self.processed_files_set and not output_file.exists():
                unprocessed_files.append(file_path)
        
        self.total_files = len(unprocessed_files)
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(rss_dirs)} å€‹ RSS ç›®éŒ„ï¼Œ{len(all_files)} å€‹ JSON æª”æ¡ˆ")
        logger.info(f"â­ï¸ è·³éå·²è™•ç†æª”æ¡ˆ: {len(all_files) - len(unprocessed_files)} å€‹")
        logger.info(f"ğŸ”„ å¾…è™•ç†æª”æ¡ˆ: {self.total_files} å€‹")
        return unprocessed_files
    
    def get_postgresql_episodes(self) -> Dict[str, Dict]:
        """å¾ PostgreSQL å–å¾—å®Œæ•´çš„ episodes å’Œ podcasts è³‡æ–™"""
        try:
            conn = psycopg2.connect(**self.pg_config)
            cursor = conn.cursor()
            
            query = """
            SELECT 
                e.episode_id,
                e.episode_title,
                e.duration,
                e.published_date,
                e.apple_episodes_ranking,
                e.created_at,
                e.languages,
                p.podcast_id,
                p.podcast_name,
                p.author,
                p.category,
                p.apple_rating as apple_rating
            FROM episodes e
            JOIN podcasts p ON e.podcast_id = p.podcast_id
            """
            
            cursor.execute(query)
            results = cursor.fetchall()
            
            episodes = {}
            for row in results:
                episode_id, episode_title, duration, published_date, apple_episodes_ranking, created_at, \
                languages, podcast_id, podcast_name, author, category, apple_rating = row
                
                episodes[str(episode_id)] = {
                    'episode_id': episode_id,
                    'episode_title': episode_title,
                    'duration': duration,
                    'published_date': published_date,
                    'apple_episodes_ranking': apple_episodes_ranking,
                    'created_at': created_at,
                    'languages': languages,
                    'podcast_id': podcast_id,
                    'podcast_name': podcast_name,
                    'author': author,
                    'category': category,
                    'apple_rating': apple_rating
                }
            
            cursor.close()
            conn.close()
            
            logger.info(f"âœ… å¾ PostgreSQL å–å¾— {len(episodes)} ç­† episodes è³‡æ–™")
            return episodes
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è©¢ PostgreSQL å¤±æ•—: {str(e)}")
            return {}
    
    def get_podcast_info(self, podcast_id: int) -> Optional[Dict]:
        """å¾ PostgreSQL å–å¾— podcast çš„åŸºæœ¬è³‡è¨Š"""
        try:
            conn = psycopg2.connect(**self.pg_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT podcast_id, podcast_name, author, category, apple_rating
                FROM podcasts
                WHERE podcast_id = %s
            """, (podcast_id,))
            
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if result:
                podcast_id, podcast_name, author, category, apple_rating = result
                return {
                    'podcast_id': podcast_id,
                    'podcast_name': podcast_name,
                    'author': author,
                    'category': category,
                    'apple_rating': apple_rating
                }
            else:
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢ podcast è³‡è¨Šå¤±æ•—: {str(e)}")
            return None

    def get_podcast_stats(self, podcast_id: int) -> Optional[Dict]:
        """å–å¾— podcast çš„çµ±è¨ˆè³‡è¨Šï¼ˆå¹³å‡ duration ç­‰ï¼‰"""
        try:
            conn = psycopg2.connect(**self.pg_config)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT 
                    COUNT(duration) as episodes_with_duration,
                    AVG(duration) as avg_duration,
                    MIN(published_date) as earliest_date,
                    MAX(published_date) as latest_date
                FROM episodes
                WHERE podcast_id = %s AND duration IS NOT NULL
            """, (podcast_id,))
            
            result = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if result and result[0] > 0:
                episodes_with_duration, avg_duration, earliest_date, latest_date = result
                return {
                    'avg_duration': int(avg_duration) if avg_duration else None,
                    'earliest_date': earliest_date,
                    'latest_date': latest_date,
                    'episodes_with_duration': episodes_with_duration
                }
            else:
                return None
                
        except Exception as e:
            logger.error(f"å–å¾— podcast çµ±è¨ˆè³‡è¨Šå¤±æ•—: {str(e)}")
            return None

    def extract_info_from_filename(self, filename: str) -> Tuple[Optional[int], Optional[str], Optional[str], Optional[str]]:
        """å¾æª”æ¡ˆåç¨±ä¸­æå– podcast_idã€episode_titleã€ep_matchã€ä¸»æ¨™é¡Œ"""
        try:
            name = filename.replace('.json', '')
            parts = name.split('_')
            if len(parts) >= 3 and parts[0] == 'RSS':
                podcast_id = int(parts[1])
                episode_title_parts = parts[2:]
                episode_title = '_'.join(episode_title_parts)
                ep_match = None
                for part in episode_title_parts:
                    if part.startswith('EP'):
                        ep_match = part.split()[0]
                        break
                main_title = episode_title
                if episode_title.startswith('podcast_'):
                    main_title = episode_title.split('_', 2)[-1]
                if main_title.startswith('EP'):
                    main_title = main_title.split(' ', 1)[-1] if ' ' in main_title else main_title
                return podcast_id, episode_title, ep_match, main_title
            return None, None, None, None
        except Exception as e:
            logger.error(f"è§£ææª”æ¡ˆåç¨±å¤±æ•— {filename}: {str(e)}")
            return None, None, None, None

    def normalize_title(self, title: str) -> str:
        """æ­£è¦åŒ–æ¨™é¡Œï¼ˆå»é™¤ç©ºç™½ã€è½‰å°å¯«ï¼‰"""
        if not title:
            return ""
        return title.replace(' ', '').replace('ã€€', '').lower()

    def find_postgresql_episode(self, podcast_id: int, ep_match: Optional[str], main_title: Optional[str], pg_episodes: Dict) -> Optional[Dict]:
        """æ ¹æ“š podcast_idã€EP ç·¨è™Ÿæˆ–ä¸»æ¨™é¡Œæ‰¾åˆ°å°æ‡‰çš„ PostgreSQL episodeï¼ˆæ”¯æ´æ¨¡ç³Šæ¯”å°ï¼‰"""
        try:
            # å¾å·²å–å¾—çš„ episodes ä¸­æœå°‹
            matching_episodes = []
            
            for episode_id, episode_data in pg_episodes.items():
                if episode_data['podcast_id'] == podcast_id:
                    episode_title = episode_data['episode_title']
                    
                    # 1. å®Œå…¨æ¯”å°
                    if main_title and main_title == episode_title:
                        matching_episodes.append((episode_data, 100))  # å®Œå…¨åŒ¹é…
                    
                    # 2. EP ç·¨è™Ÿæ¯”å°
                    elif ep_match and ep_match in episode_title:
                        matching_episodes.append((episode_data, 90))  # EP åŒ¹é…
                    
                    # 3. å¿½ç•¥ç©ºç™½çš„æ¨¡ç³Šæ¯”å°
                    elif main_title:
                        norm_main = self.normalize_title(main_title)
                        norm_episode = self.normalize_title(episode_title)
                        
                        if norm_main in norm_episode or norm_episode in norm_main:
                            # è¨ˆç®—ç›¸ä¼¼åº¦
                            similarity = len(set(norm_main) & set(norm_episode)) / len(set(norm_main) | set(norm_episode))
                            if similarity > 0.3:  # ç›¸ä¼¼åº¦é–¾å€¼
                                matching_episodes.append((episode_data, int(similarity * 100)))
            
            if matching_episodes:
                # æŒ‰åŒ¹é…åˆ†æ•¸æ’åºï¼Œé¸æ“‡æœ€ä½³åŒ¹é…
                matching_episodes.sort(key=lambda x: x[1], reverse=True)
                best_match = matching_episodes[0]
                
                logger.info(f"æ‰¾åˆ°åŒ¹é… episode: {best_match[0]['episode_title']} (åŒ¹é…åˆ†æ•¸: {best_match[1]})")
                return best_match[0]
            else:
                logger.warning(f"æŸ¥ç„¡ PostgreSQL è³‡æ–™: podcast_id={podcast_id}, main_title={main_title}")
                return None
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢ PostgreSQL episode å¤±æ•—: {str(e)}")
            return None
    
    def ensure_field_has_value(self, value: Any, field_name: str, default_value: Any, max_length: Optional[int] = None) -> Any:
        """ç¢ºä¿æ¬„ä½æœ‰å€¼ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é è¨­å€¼"""
        if value is None or value == '' or value == 'null' or value == 'None':
            return default_value
        
        # è™•ç† Decimal é¡å‹
        if hasattr(value, '__class__') and value.__class__.__name__ == 'Decimal':
            value = float(value)
        
        # ç‰¹æ®Šè™•ç† published_date
        if field_name == 'published_date' and value is not None:
            if isinstance(value, str):
                return str(value)[:64]
            elif hasattr(value, 'isoformat'):
                return value.isoformat()[:64]
            else:
                return str(value)[:64]
        
        # å¦‚æœæ˜¯å­—ä¸²ä¸”è¶…éæœ€å¤§é•·åº¦ï¼Œæˆªæ–·
        if isinstance(value, str) and max_length:
            return str(value)[:max_length]
        
        return value
    
    def process_single_file(self, file_path: Path, pg_episodes: Dict) -> Tuple[List[Dict], Dict]:
        """è™•ç†å–®ä¸€ JSON æª”æ¡ˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è§£ææª”å
            podcast_id, episode_title, ep_match, main_title = self.extract_info_from_filename(file_path.name)
            
            if podcast_id is None:
                raise Exception("ç„¡æ³•è§£ææª”æ¡ˆåç¨±")
            
            # æ ¹æ“šæª”æ¡ˆåç¨±è¨­å®šå›ºå®š category
            fixed_category = "unknown"
            if "podcast_1321" in file_path.name:
                fixed_category = "å•†æ¥­"
            elif "podcast_1304" in file_path.name:
                fixed_category = "æ•™è‚²"
            
            # å°‹æ‰¾å°æ‡‰çš„ episode
            pg_episode = self.find_postgresql_episode(podcast_id, ep_match, main_title, pg_episodes)
            
            # å–å¾— podcast åŸºæœ¬è³‡è¨Š
            podcast_info = self.get_podcast_info(podcast_id)
            podcast_stats = self.get_podcast_stats(podcast_id)
            
            # è™•ç† chunks
            chunks = data.get('chunks', [])
            processed_chunks = []
            
            for i, chunk in enumerate(chunks):
                # ä¿ç•™åŸå§‹çš„ chunk_text å’Œ chunk_index
                original_chunk_text = chunk.get('chunk_text', '')
                original_chunk_index = chunk.get('chunk_index', i)
                
                if not original_chunk_text or original_chunk_text.strip() == '':
                    original_chunk_text = 'ç„¡å…§å®¹'
                
                # è·³é embedding ç”Ÿæˆï¼ˆåƒ…è£œè³‡æ–™æ¨¡å¼ï¼‰
                embedding = [0.0] * 1024  # é è¨­ embedding å‘é‡
                
                # è™•ç† tags
                enhanced_tags = chunk.get('enhanced_tags', [])
                if isinstance(enhanced_tags, list):
                    tags_str = ','.join(enhanced_tags)
                else:
                    tags_str = str(enhanced_tags) if enhanced_tags else 'ç„¡æ¨™ç±¤'
                
                # ç¢ºä¿ tags ä¸è¶…é 1024 å­—å…ƒ
                if len(tags_str) > 1024:
                    tags_str = tags_str[:1021] + "..."
                
                if pg_episode:
                    # æœ‰æ‰¾åˆ° episodeï¼Œä½¿ç”¨ episode è³‡æ–™ï¼Œå¼·åˆ¶å‹æ…‹è½‰æ›
                    processed_chunk = {
                        'chunk_id': str(self.ensure_field_has_value(chunk.get('chunk_id', f"{pg_episode['episode_id']}_{i}"), 'chunk_id', f"{pg_episode['episode_id']}_{i}", 64)),
                        'chunk_index': int(self.ensure_field_has_value(original_chunk_index, 'chunk_index', i)),
                        'episode_id': int(self.ensure_field_has_value(pg_episode['episode_id'], 'episode_id', 0)),
                        'podcast_id': int(self.ensure_field_has_value(pg_episode['podcast_id'], 'podcast_id', 0)),
                        'podcast_name': str(self.ensure_field_has_value(pg_episode['podcast_name'], 'podcast_name', 'æœªçŸ¥æ’­å®¢', 255)),
                        'author': str(self.ensure_field_has_value(pg_episode['author'], 'author', 'æœªçŸ¥ä½œè€…', 255)),
                        'category': str(self.ensure_field_has_value(fixed_category, 'category', 'æœªçŸ¥åˆ†é¡', 64)),
                        'episode_title': str(self.ensure_field_has_value(pg_episode['episode_title'], 'episode_title', 'æœªçŸ¥æ¨™é¡Œ', 255)),
                        'duration': str(self.ensure_field_has_value(pg_episode['duration'], 'duration', 'æœªçŸ¥æ™‚é•·', 255)),
                        'published_date': str(self.ensure_field_has_value(pg_episode['published_date'], 'published_date', 'æœªçŸ¥æ—¥æœŸ', 64)),
                        'apple_rating': int(self.ensure_field_has_value(pg_episode['apple_rating'], 'apple_rating', 0)),
                        'chunk_text': str(self.ensure_field_has_value(original_chunk_text, 'chunk_text', 'ç„¡å…§å®¹', 1024)),
                        'embedding': embedding,
                        'language': str(self.ensure_field_has_value(pg_episode['languages'], 'language', 'zh', 16)),
                        'created_at': str(self.ensure_field_has_value(pg_episode['created_at'].isoformat() if pg_episode['created_at'] else datetime.now().isoformat(), 'created_at', datetime.now().isoformat(), 64)),
                        'source_model': 'bge-m3',
                        'tags': str(self.ensure_field_has_value(tags_str, 'tags', 'ç„¡æ¨™ç±¤', 1024))
                    }
                else:
                    # æ²’æ‰¾åˆ° episodeï¼Œä½¿ç”¨åŸºæœ¬è³‡è¨Šï¼Œå¼·åˆ¶å‹æ…‹è½‰æ›
                    if podcast_info:
                        fallback_duration = podcast_stats['avg_duration'] if podcast_stats and podcast_stats['avg_duration'] else 'æœªçŸ¥æ™‚é•·'
                        fallback_date = str(podcast_stats['earliest_date']) if podcast_stats and podcast_stats['earliest_date'] else 'æœªçŸ¥æ—¥æœŸ'
                        
                        processed_chunk = {
                            'chunk_id': str(self.ensure_field_has_value(chunk.get('chunk_id', f"unknown_{i}"), 'chunk_id', f"unknown_{i}", 64)),
                            'chunk_index': int(self.ensure_field_has_value(original_chunk_index, 'chunk_index', i)),
                            'episode_id': 0,
                            'podcast_id': int(self.ensure_field_has_value(podcast_id, 'podcast_id', 0)),
                            'podcast_name': str(self.ensure_field_has_value(podcast_info['podcast_name'], 'podcast_name', 'æœªçŸ¥æ’­å®¢', 255)),
                            'author': str(self.ensure_field_has_value(podcast_info['author'], 'author', 'æœªçŸ¥ä½œè€…', 255)),
                            'category': str(self.ensure_field_has_value(fixed_category, 'category', 'æœªçŸ¥åˆ†é¡', 64)),
                            'episode_title': str(self.ensure_field_has_value(main_title, 'episode_title', 'æœªçŸ¥æ¨™é¡Œ', 255)),
                            'duration': str(self.ensure_field_has_value(fallback_duration, 'duration', 'æœªçŸ¥æ™‚é•·', 255)),
                            'published_date': str(self.ensure_field_has_value(fallback_date, 'published_date', 'æœªçŸ¥æ—¥æœŸ', 64)),
                            'apple_rating': int(self.ensure_field_has_value(podcast_info['apple_rating'], 'apple_rating', 0)),
                            'chunk_text': str(self.ensure_field_has_value(original_chunk_text, 'chunk_text', 'ç„¡å…§å®¹', 1024)),
                            'embedding': embedding,
                            'language': 'zh',
                            'created_at': datetime.now().isoformat(),
                            'source_model': 'bge-m3',
                            'tags': str(self.ensure_field_has_value(tags_str, 'tags', 'ç„¡æ¨™ç±¤', 1024))
                        }
                    else:
                        processed_chunk = {
                            'chunk_id': str(self.ensure_field_has_value(chunk.get('chunk_id', f"unknown_{i}"), 'chunk_id', f"unknown_{i}", 64)),
                            'chunk_index': int(self.ensure_field_has_value(original_chunk_index, 'chunk_index', i)),
                            'episode_id': 0,
                            'podcast_id': int(self.ensure_field_has_value(podcast_id, 'podcast_id', 0)),
                            'podcast_name': 'æœªçŸ¥æ’­å®¢',
                            'author': 'æœªçŸ¥ä½œè€…',
                            'category': str(self.ensure_field_has_value(fixed_category, 'category', 'æœªçŸ¥åˆ†é¡', 64)),
                            'episode_title': str(self.ensure_field_has_value(main_title, 'episode_title', 'æœªçŸ¥æ¨™é¡Œ', 255)),
                            'duration': 'æœªçŸ¥æ™‚é•·',
                            'published_date': 'æœªçŸ¥æ—¥æœŸ',
                            'apple_rating': 0,
                            'chunk_text': str(self.ensure_field_has_value(original_chunk_text, 'chunk_text', 'ç„¡å…§å®¹', 1024)),
                            'embedding': embedding,
                            'language': 'zh',
                            'created_at': datetime.now().isoformat(),
                            'source_model': 'bge-m3',
                            'tags': str(self.ensure_field_has_value(tags_str, 'tags', 'ç„¡æ¨™ç±¤', 1024))
                        }
                
                processed_chunks.append(processed_chunk)
            
            # å»ºç«‹ä¿®æ­£å¾Œçš„ JSON çµæ§‹
            fixed_data = {
                'filename': file_path.name,
                'episode_id': str(pg_episode['episode_id']) if pg_episode else 'unknown',
                'collection_name': f"RSS_{podcast_id}" if podcast_id else 'unknown',
                'total_chunks': len(processed_chunks),
                'chunks': processed_chunks,
                'fixed_at': datetime.now().isoformat(),
                'source_model': 'bge-m3'
            }
            
            return processed_chunks, fixed_data
            
        except Exception as e:
            logger.error(f"è™•ç†æª”æ¡ˆ {file_path} å¤±æ•—: {str(e)}")
            return [], {}
    
    def save_fixed_json(self, fixed_data: Dict, original_file_path: Path):
        """å„²å­˜ä¿®æ­£å¾Œçš„ JSON æª”æ¡ˆ"""
        try:
            # å»ºç«‹å°æ‡‰çš„è¼¸å‡ºç›®éŒ„
            output_dir = self.output_path / original_file_path.parent.name
            output_dir.mkdir(exist_ok=True)
            
            # å„²å­˜ä¿®æ­£å¾Œçš„æª”æ¡ˆ
            output_file = output_dir / original_file_path.name
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(fixed_data, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜ä¿®æ­£å¾Œçš„æª”æ¡ˆå¤±æ•—: {str(e)}")
            return False
    
    def estimate_remaining_time(self) -> str:
        """é ä¼°å‰©é¤˜æ™‚é–“"""
        if self.processed_files == 0:
            return "è¨ˆç®—ä¸­..."
        
        elapsed_time = time.time() - self.start_time
        avg_time_per_file = elapsed_time / self.processed_files
        remaining_files = self.total_files - self.processed_files
        remaining_time = avg_time_per_file * remaining_files
        
        return str(timedelta(seconds=int(remaining_time)))
    
    def process_all_files(self):
        """è™•ç†æ‰€æœ‰æª”æ¡ˆ"""
        print("\n" + "="*80)
        print("ğŸš€ é–‹å§‹æ‰¹æ¬¡è™•ç† stage3_tagging åˆ° stage4_embedding_prep")
        print("="*80)
        
        # 1. æª¢æŸ¥è¼¸å‡ºç›®éŒ„ï¼ˆä¸æ¸…ç©ºï¼Œä¿ç•™å·²è™•ç†çš„æª”æ¡ˆï¼‰
        print("ğŸ“ æª¢æŸ¥ stage4_embedding_prep ç›®éŒ„...")
        self.output_path.mkdir(exist_ok=True)
        
        # 2. è·³é BGE-M3 æ¨¡å‹æª¢æŸ¥ï¼ˆåƒ…è£œè³‡æ–™æ¨¡å¼ï¼‰
        print("ğŸ¤– è·³é BGE-M3 æ¨¡å‹æª¢æŸ¥ï¼ˆåƒ…è£œè³‡æ–™æ¨¡å¼ï¼‰...")
        
        # 3. è·³é Milvus é€£ç·šï¼ˆåƒ…è£œè³‡æ–™æ¨¡å¼ï¼‰
        print("ğŸ”— è·³é Milvus é€£ç·šï¼ˆåƒ…è£œè³‡æ–™æ¨¡å¼ï¼‰...")
        
        # 4. æƒææ‰€æœ‰æª”æ¡ˆ
        print("ğŸ“‹ æƒææ‰€æœ‰æª”æ¡ˆ...")
        all_files = self.scan_all_files()
        if not all_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°éœ€è¦è™•ç†çš„æª”æ¡ˆ")
            return
        
        # 5. å–å¾— PostgreSQL è³‡æ–™ï¼ˆå¯é¸ï¼‰
        print("ğŸ—„ï¸ å˜—è©¦å–å¾— PostgreSQL è³‡æ–™...")
        pg_episodes = self.get_postgresql_episodes()
        
        # 6. è™•ç†å‰å…ˆæ¸…ç©ºå·²è™•ç†æª”æ¡ˆè¨˜éŒ„ï¼ˆé‡æ–°é–‹å§‹ï¼‰
        print("ğŸ”„ æ¸…ç©ºå·²è™•ç†æª”æ¡ˆè¨˜éŒ„ï¼Œé‡æ–°é–‹å§‹...")
        self.processed_files = set()
        self.save_progress()
        if not pg_episodes:
            print("âš ï¸ ç„¡æ³•å–å¾— PostgreSQL è³‡æ–™ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ç¹¼çºŒè™•ç†")
            pg_episodes = {}  # ä½¿ç”¨ç©ºå­—å…¸ï¼Œè™•ç†æ™‚æœƒä½¿ç”¨é è¨­å€¼
        
        # 6. é–‹å§‹æ‰¹æ¬¡è™•ç†
        print(f"\nğŸ”„ é–‹å§‹è™•ç† {self.total_files} å€‹æª”æ¡ˆ...")
        self.start_time = time.time()
        
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
        with tqdm(total=self.total_files, desc="è™•ç†é€²åº¦", unit="æª”æ¡ˆ") as pbar:
            for i, file_path in enumerate(all_files):
                try:
                    # è™•ç†æª”æ¡ˆ
                    processed_chunks, fixed_data = self.process_single_file(file_path, pg_episodes)
                    
                    if processed_chunks and fixed_data:
                        # å„²å­˜ä¿®æ­£å¾Œçš„ JSON æª”æ¡ˆï¼ˆåƒ…è£œè³‡æ–™æ¨¡å¼ï¼‰
                        if self.save_fixed_json(fixed_data, file_path):
                                self.processed_files += 1
                                pbar.set_postfix({
                                    'æˆåŠŸ': self.processed_files,
                                    'å‰©é¤˜æ™‚é–“': self.estimate_remaining_time()
                                })
                        else:
                            # å„²å­˜å¤±æ•—
                            self.failed_files.append({
                                'file': str(file_path),
                                'reason': 'JSON å„²å­˜å¤±æ•—',
                                'chunks_count': len(processed_chunks)
                            })
                    else:
                        # è™•ç†å¤±æ•—
                        self.failed_files.append({
                            'file': str(file_path),
                            'reason': 'æª”æ¡ˆè™•ç†å¤±æ•—',
                            'chunks_count': 0
                        })
                    
                except Exception as e:
                    # ç•°å¸¸è™•ç†
                    error_msg = f"è™•ç†ç•°å¸¸: {str(e)}"
                    self.failed_files.append({
                        'file': str(file_path),
                        'reason': error_msg,
                        'chunks_count': 0
                    })
                    logger.error(f"è™•ç†æª”æ¡ˆ {file_path} ç•°å¸¸: {str(e)}")
                
                pbar.update(1)
                
                # æ¯è™•ç† 50 å€‹æª”æ¡ˆä¿å­˜ä¸€æ¬¡é€²åº¦
                if (i + 1) % 50 == 0:
                    self.save_progress()
                    logger.info(f"ğŸ’¾ å·²è™•ç† {i + 1} å€‹æª”æ¡ˆï¼Œé€²åº¦å·²ä¿å­˜")
        
        # 7. è¼¸å‡ºçµæœ
        total_time = time.time() - self.start_time
        print("\n" + "="*80)
        print("ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {self.total_files}")
        print(f"   æˆåŠŸè™•ç†: {self.processed_files}")
        print(f"   å¤±æ•—æª”æ¡ˆ: {len(self.failed_files)}")
        print(f"   ç¸½è€—æ™‚: {str(timedelta(seconds=int(total_time)))}")
        print(f"   å¹³å‡é€Ÿåº¦: {self.total_files/total_time:.2f} æª”æ¡ˆ/ç§’")
        
        if self.failed_files:
            print(f"\nâŒ å¤±æ•—æª”æ¡ˆæ¸…å–®:")
            for failed in self.failed_files:
                print(f"   {failed['file']}: {failed['reason']}")
            
            # å„²å­˜å¤±æ•—æ¸…å–®
            fail_log_path = self.output_path / "fail_log.json"
            with open(fail_log_path, 'w', encoding='utf-8') as f:
                json.dump(self.failed_files, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ å¤±æ•—æ¸…å–®å·²å„²å­˜è‡³: {fail_log_path}")
        
        print("\nâœ… æ‰€æœ‰è™•ç†å®Œæˆï¼")
        
        # è™•ç†å®Œæˆå¾Œè‡ªå‹•é©—è­‰
        print(f"\nğŸ” é–‹å§‹é©—è­‰è™•ç†çµæœ...")
        try:
            import subprocess
            result = subprocess.run(["python", "backend/utils/validate_embedding_data.py"], 
                                  capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode == 0:
                print("âœ… è³‡æ–™é©—è­‰é€šéï¼æ‰€æœ‰æ¬„ä½æ ¼å¼æ­£ç¢ºï¼Œå¯ä»¥é€²è¡Œ embedding å¯«å…¥ Milvusã€‚")
                print("\nğŸ“‹ é©—è­‰å ±å‘Š:")
                print(result.stdout)
            else:
                print("âŒ è³‡æ–™é©—è­‰å¤±æ•—ï¼è«‹æª¢æŸ¥ä»¥ä¸‹å•é¡Œï¼š")
                print(result.stdout)
                print(result.stderr)
                print("\nâš ï¸ è«‹ä¿®æ­£è³‡æ–™æ ¼å¼å•é¡Œå¾Œå†é€²è¡Œ embedding å¯«å…¥ã€‚")
                
        except Exception as e:
            print(f"âš ï¸ é©—è­‰è…³æœ¬åŸ·è¡Œå¤±æ•—: {e}")
            print("è«‹æ‰‹å‹•åŸ·è¡Œ: python backend/utils/validate_embedding_data.py")

def main():
    """ä¸»å‡½æ•¸"""
    processor = BatchProcessor()
    processor.process_all_files()

if __name__ == "__main__":
    main() 