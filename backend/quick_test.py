"""
å¿«é€Ÿæ¸¬è©¦è…³æœ¬
æ¸¬è©¦åˆ‡æ–·ã€è²¼æ¨™ã€è½‰å‘é‡çš„åŸºæœ¬åŠŸèƒ½
"""

import logging
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

from vector_pipeline.utils.text_cleaner import TextCleaner
from vector_pipeline.core import TextChunker, VectorProcessor

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def quick_test():
    """å¿«é€Ÿæ¸¬è©¦åŸºæœ¬åŠŸèƒ½"""
    logger.info("=== å¿«é€Ÿæ¸¬è©¦é–‹å§‹ ===")
    
    # 1. æ¸¬è©¦æ–‡æœ¬æ¸…ç†
    logger.info("1. æ¸¬è©¦æ–‡æœ¬æ¸…ç†")
    cleaner = TextCleaner()
    
    test_text = "ğŸ§ AIç§‘æŠ€æ–°çŸ¥ï¼šäººå·¥æ™ºæ…§åœ¨å•†æ¥­ä¸­çš„æ‡‰ç”¨ ğŸ”¥"
    cleaned_text = cleaner.clean_text(test_text)
    
    logger.info(f"åŸå§‹æ–‡æœ¬: {test_text}")
    logger.info(f"æ¸…ç†å¾Œæ–‡æœ¬: {cleaned_text}")
    
    # 2. æ¸¬è©¦æ–‡æœ¬åˆ‡åˆ†
    logger.info("\n2. æ¸¬è©¦æ–‡æœ¬åˆ‡åˆ†")
    chunker = TextChunker(max_chunk_size=200, overlap_size=50)
    
    long_text = """
    äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ˜¯ç•¶ä»Šç§‘æŠ€ç™¼å±•çš„é‡è¦è¶¨å‹¢ã€‚åœ¨å•†æ¥­æ‡‰ç”¨ä¸­ï¼ŒAIæŠ€è¡“æ­£åœ¨æ”¹è®Šä¼æ¥­çš„é‹ç‡Ÿæ–¹å¼ï¼Œ
    å¾å®¢æˆ¶æœå‹™åˆ°ç”¢å“é–‹ç™¼ï¼Œå¾å¸‚å ´åˆ†æåˆ°æ±ºç­–åˆ¶å®šï¼ŒAIéƒ½ç™¼æ®è‘—è¶Šä¾†è¶Šé‡è¦çš„ä½œç”¨ã€‚
    
    åœ¨æ•™è‚²é ˜åŸŸï¼ŒAIæŠ€è¡“ç‚ºå­¸ç¿’è€…æä¾›äº†å€‹æ€§åŒ–çš„å­¸ç¿’é«”é©—ï¼Œæ™ºèƒ½å°å¸«ç³»çµ±å¯ä»¥æ ¹æ“šå­¸ç”Ÿçš„å­¸ç¿’é€²åº¦
    å’Œèˆˆè¶£èª¿æ•´æ•™å­¸å…§å®¹ï¼Œæé«˜å­¸ç¿’æ•ˆç‡ã€‚åŒæ™‚ï¼ŒAIä¹Ÿåœ¨å¹«åŠ©æ•™å¸«æ›´å¥½åœ°ç®¡ç†èª²å ‚å’Œè©•ä¼°å­¸ç”Ÿè¡¨ç¾ã€‚
    """
    
    chunks = chunker.chunk_text(long_text)
    logger.info(f"åˆ‡åˆ†å‡º {len(chunks)} å€‹æ–‡æœ¬å¡Š")
    
    for i, chunk in enumerate(chunks):
        logger.info(f"æ–‡æœ¬å¡Š {i+1}: {chunk.text[:100]}...")
    
    # 3. æ¸¬è©¦å‘é‡åŒ–
    logger.info("\n3. æ¸¬è©¦å‘é‡åŒ–")
    try:
        vector_processor = VectorProcessor(model_name="BAAI/bge-m3")
        
        # æ¸¬è©¦æ–‡æœ¬å‘é‡
        text_embedding = vector_processor.generate_embedding("AIæŠ€è¡“ç™¼å±•")
        logger.info(f"æ–‡æœ¬å‘é‡ç¶­åº¦: {len(text_embedding)}")
        
        # æ¸¬è©¦æ¨™ç±¤å‘é‡
        tag_embedding = vector_processor.generate_embedding("ç§‘æŠ€æŠ€è¡“")
        logger.info(f"æ¨™ç±¤å‘é‡ç¶­åº¦: {len(tag_embedding)}")
        
    except Exception as e:
        logger.error(f"å‘é‡åŒ–æ¸¬è©¦å¤±æ•—: {e}")
    
    # 4. æ¸¬è©¦æ¨™ç±¤æå–
    logger.info("\n4. æ¸¬è©¦æ¨™ç±¤æå–")
    
    def extract_simple_tags(text):
        """ç°¡å–®æ¨™ç±¤æå–"""
        tags = []
        text_lower = text.lower()
        
        keyword_mapping = {
            'ai': 'AIäººå·¥æ™ºæ…§',
            'äººå·¥æ™ºæ…§': 'AIäººå·¥æ™ºæ…§',
            'ç§‘æŠ€': 'ç§‘æŠ€æŠ€è¡“',
            'æŠ€è¡“': 'ç§‘æŠ€æŠ€è¡“',
            'å•†æ¥­': 'å•†æ¥­ç®¡ç†',
            'ä¼æ¥­': 'å•†æ¥­ç®¡ç†',
            'æ•™è‚²': 'æ•™è‚²å­¸ç¿’',
            'å­¸ç¿’': 'æ•™è‚²å­¸ç¿’'
        }
        
        for keyword, tag in keyword_mapping.items():
            if keyword in text_lower and tag not in tags:
                tags.append(tag)
                if len(tags) >= 3:
                    break
        
        return tags if tags else ['ä¸€èˆ¬å…§å®¹']
    
    # æ¸¬è©¦æ¨™ç±¤æå–
    for i, chunk in enumerate(chunks[:2]):  # åªæ¸¬è©¦å‰2å€‹å¡Š
        tags = extract_simple_tags(chunk.text)
        logger.info(f"æ–‡æœ¬å¡Š {i+1} æ¨™ç±¤: {tags}")
    
    logger.info("\n=== å¿«é€Ÿæ¸¬è©¦å®Œæˆ ===")


if __name__ == "__main__":
    quick_test() 