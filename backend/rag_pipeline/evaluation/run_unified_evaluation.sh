#!/bin/bash

# çµ±ä¸€ RAG è©•ä¼°åŸ·è¡Œè…³æœ¬

echo "=== çµ±ä¸€ RAG è©•ä¼°ç³»çµ± ==="
echo ""

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
if [ -z "$OPENAI_API_KEY" ]; then
    echo "âŒ éŒ¯èª¤: æœªè¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸"
    echo "è«‹åŸ·è¡Œ: export OPENAI_API_KEY='your_api_key'"
    exit 1
fi

echo "âœ… OpenAI API Key å·²è¨­å®š"
echo ""

# æª¢æŸ¥ Python ç’°å¢ƒ
if ! command -v python3 &> /dev/null; then
    echo "âŒ éŒ¯èª¤: æœªæ‰¾åˆ° python3"
    exit 1
fi

echo "âœ… Python3 å·²å®‰è£"
echo ""

# æª¢æŸ¥å¿…è¦æ–‡ä»¶
required_files=(
    "unified_rag_evaluator.py"
    "enhanced_rag_evaluator.py"
    "requirements.txt"
)

for file in "${required_files[@]}"; do
    if [ ! -f "$file" ]; then
        echo "âŒ éŒ¯èª¤: ç¼ºå°‘å¿…è¦æ–‡ä»¶ $file"
        exit 1
    fi
done

echo "âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶éƒ½å­˜åœ¨"
echo ""

# å‰µå»ºè¼¸å‡ºç›®éŒ„
mkdir -p unified_results
echo "âœ… è¼¸å‡ºç›®éŒ„å·²å‰µå»º"
echo ""

# é¸æ“‡åŸ·è¡Œæ¨¡å¼
echo "è«‹é¸æ“‡åŸ·è¡Œæ¨¡å¼:"
echo "1. å¿«é€Ÿæ¸¬è©¦ (æ¨è–¦)"
echo "2. å®Œæ•´è©•ä¼°"
echo "3. Podwise å°ˆç”¨è©•ä¼°"
echo "4. è‡ªå®šç¾©è©•ä¼°"
echo ""

read -p "è«‹è¼¸å…¥é¸é … (1-4): " choice

case $choice in
    1)
        echo ""
        echo "ğŸš€ åŸ·è¡Œå¿«é€Ÿæ¸¬è©¦..."
        python3 -c "
import asyncio
from unified_rag_evaluator import test_unified_evaluator
asyncio.run(test_unified_evaluator())
"
        ;;
    2)
        echo ""
        echo "ğŸš€ åŸ·è¡Œå®Œæ•´è©•ä¼°..."
        echo "æ³¨æ„: é€™å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“å’Œè¼ƒå¤š API èª¿ç”¨"
        read -p "ç¢ºèªç¹¼çºŒ? (y/N): " confirm
        if [[ $confirm == [yY] ]]; then
            python3 -c "
import asyncio
from unified_rag_evaluator import UnifiedRAGEvaluator

async def full_evaluation():
    evaluator = UnifiedRAGEvaluator(use_mock_services=True)
    
    # å‰µå»º Podwise åˆæˆè³‡æ–™é›†
    qa_pairs = evaluator.create_podwise_synthetic_dataset(num_questions=10)
    
    if qa_pairs:
        # è½‰æ›ç‚ºè©•ä¼°è³‡æ–™é›†æ ¼å¼
        eval_dataset = []
        for qa in qa_pairs:
            eval_dataset.append({
                'input': qa['question'],
                'expected': qa['answer'],
                'metadata': {'reference': qa['reference']}
            })
        
        # åŸ·è¡Œå„ç¨®è©•ä¼°
        print('ğŸ” åŸ·è¡Œ Baseline è©•ä¼°...')
        baseline_results = evaluator.baseline_evaluation(eval_dataset)
        
        print('ğŸ” åŸ·è¡Œ Naive RAG è©•ä¼°...')
        naive_results = evaluator.naive_rag_evaluation(eval_dataset)
        
        print('ğŸ” åŸ·è¡Œå¢å¼·ç‰ˆè©•ä¼°...')
        enhanced_results = await evaluator.evaluate_batch([qa['question'] for qa in qa_pairs])
        
        # ç”Ÿæˆå ±å‘Š
        evaluator.generate_comparison_report(enhanced_results, 'unified_results/full_evaluation_report.txt')
        
        print('âœ… å®Œæ•´è©•ä¼°å®Œæˆ')
    else:
        print('âŒ ç„¡æ³•å‰µå»ºåˆæˆè³‡æ–™é›†')

asyncio.run(full_evaluation())
"
        else
            echo "å·²å–æ¶ˆ"
            exit 0
        fi
        ;;
    3)
        echo ""
        echo "ğŸš€ Podwise å°ˆç”¨è©•ä¼°æ¨¡å¼"
        python3 -c "
import asyncio
from unified_rag_evaluator import UnifiedRAGEvaluator

async def podwise_evaluation():
    evaluator = UnifiedRAGEvaluator(use_mock_services=True)
    
    # å‰µå»º Podwise åˆæˆè³‡æ–™é›†
    qa_pairs = evaluator.create_podwise_synthetic_dataset(num_questions=5)
    
    if qa_pairs:
        print(f'âœ… æˆåŠŸå‰µå»º {len(qa_pairs)} å€‹å•ç­”å°')
        
        # åŸ·è¡Œè©•ä¼°
        questions = [qa['question'] for qa in qa_pairs]
        results = await evaluator.evaluate_batch(questions)
        
        # ç”Ÿæˆå ±å‘Š
        evaluator.generate_comparison_report(results, 'unified_results/podwise_evaluation_report.txt')
        
        print('âœ… Podwise è©•ä¼°å®Œæˆ')
    else:
        print('âŒ ç„¡æ³•å‰µå»º Podwise åˆæˆè³‡æ–™é›†')

asyncio.run(podwise_evaluation())
"
        ;;
    4)
        echo ""
        echo "ğŸš€ è‡ªå®šç¾©è©•ä¼°æ¨¡å¼"
        echo "è«‹æ‰‹å‹•åŸ·è¡Œä»¥ä¸‹å‘½ä»¤:"
        echo ""
        echo "python3 -c \""
        echo "import asyncio"
        echo "from unified_rag_evaluator import UnifiedRAGEvaluator"
        echo ""
        echo "async def custom_evaluation():"
        echo "    evaluator = UnifiedRAGEvaluator(use_mock_services=True)"
        echo "    # åœ¨é€™è£¡æ·»åŠ æ‚¨çš„è‡ªå®šç¾©è©•ä¼°é‚è¼¯"
        echo "    pass"
        echo ""
        echo "asyncio.run(custom_evaluation())"
        echo "\""
        ;;
    *)
        echo "âŒ ç„¡æ•ˆé¸é …"
        exit 1
        ;;
esac

echo ""
echo "=== è©•ä¼°å®Œæˆ ==="
echo "çµæœä¿å­˜åœ¨ unified_results/ ç›®éŒ„ä¸­"
echo ""

# æª¢æŸ¥æ˜¯å¦æœ‰çµæœæ–‡ä»¶
if [ -f "unified_results/unified_rag_evaluation_report.txt" ]; then
    echo "ğŸ“Š è©•ä¼°çµæœæ‘˜è¦:"
    python3 -c "
import json
try:
    with open('unified_results/unified_rag_evaluation_report.txt', 'r', encoding='utf-8') as f:
        content = f.read()
    print(content)
except Exception as e:
    print(f'ç„¡æ³•è®€å–çµæœæ–‡ä»¶: {e}')
"
fi

echo ""
echo "ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­°:"
echo "1. æŸ¥çœ‹è©³ç´°çµæœæ–‡ä»¶"
echo "2. èª¿æ•´è©•ä¼°åƒæ•¸"
echo "3. åˆ†æè©•ä¼°æŒ‡æ¨™"
echo "4. å„ªåŒ– RAG ç³»çµ±" 