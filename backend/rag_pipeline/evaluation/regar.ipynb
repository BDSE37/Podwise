# RAG ç³»çµ±å®Œæ•´æ¶æ§‹
# ä½¿ç”¨ Milvus å‘é‡è³‡æ–™åº« + bge-m3 + Qwen2.5-Taiwan-7B-Instruct

## 1. ç’°å¢ƒè¨­ç½®å’Œå¥—ä»¶å®‰è£

```bash
# åŸ·è¡Œæ­¤å€å¡Šå‰è«‹å…ˆå®‰è£å¿…è¦å¥—ä»¶
!pip install pymilvus transformers torch sentence-transformers pandas numpy openai huggingface_hub accelerate bitsandbytes
```

## 2. å°å…¥å¿…è¦å¥—ä»¶

```python
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
import os
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Milvus ç›¸é—œ
from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType

# æ¨¡å‹ç›¸é—œ
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# OpenAI (å¦‚æœéœ€è¦å‚™ç”¨)
import openai
```

## 3. é…ç½®åƒæ•¸è¨­å®š

```python
# é…ç½®åƒæ•¸
class Config:
    # Milvus é…ç½®
    MILVUS_HOST = "localhost"  # è«‹æ ¹æ“šæ‚¨çš„è¨­ç½®ä¿®æ”¹
    MILVUS_PORT = "19530"     # è«‹æ ¹æ“šæ‚¨çš„è¨­ç½®ä¿®æ”¹
    COLLECTION_NAME = "podwise_chunks"
    
    # å‘é‡ç¶­åº¦
    VECTOR_DIM = 1024
    
    # æ¨¡å‹é…ç½®
    EMBEDDING_MODEL = "BAAI/bge-m3"
    LLM_MODEL = "benchang1110/Qwen2.5-Taiwan-7B-Instruct"
    
    # OpenAI é…ç½® (å‚™ç”¨)
    OPENAI_API_KEY = "your-openai-api-key"  # è«‹å¡«å…¥æ‚¨çš„ OpenAI API Key
    
    # æª¢ç´¢åƒæ•¸
    TOP_K = 5
    SIMILARITY_THRESHOLD = 0.7
    
    # ç”Ÿæˆåƒæ•¸
    MAX_LENGTH = 2048
    TEMPERATURE = 0.7

config = Config()
```

## 4. åˆå§‹åŒ–æ¨¡å‹å’Œé€£æ¥

```python
class RAGSystem:
    def __init__(self):
        self.config = config
        self.embedding_model = None
        self.llm_model = None
        self.tokenizer = None
        self.collection = None
        
    def initialize_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ bge-m3"""
        print("æ­£åœ¨è¼‰å…¥åµŒå…¥æ¨¡å‹...")
        self.embedding_model = SentenceTransformer(self.config.EMBEDDING_MODEL)
        print("åµŒå…¥æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def initialize_llm_model(self):
        """åˆå§‹åŒ– LLM æ¨¡å‹"""
        print("æ­£åœ¨è¼‰å…¥ LLM æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.config.LLM_MODEL)
        
        # ä½¿ç”¨ GPU å¦‚æœå¯ç”¨
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.llm_model = AutoModelForCausalLM.from_pretrained(
            self.config.LLM_MODEL,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True
        )
        print(f"LLM æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨è¨­å‚™: {device}")
        
    def connect_milvus(self):
        """é€£æ¥ Milvus è³‡æ–™åº«"""
        try:
            connections.connect("default", host=self.config.MILVUS_HOST, port=self.config.MILVUS_PORT)
            print("æˆåŠŸé€£æ¥åˆ° Milvus")
            
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if utility.has_collection(self.config.COLLECTION_NAME):
                self.collection = Collection(self.config.COLLECTION_NAME)
                self.collection.load()
                print(f"æˆåŠŸè¼‰å…¥é›†åˆ: {self.config.COLLECTION_NAME}")
            else:
                print(f"é›†åˆ {self.config.COLLECTION_NAME} ä¸å­˜åœ¨")
                
        except Exception as e:
            print(f"é€£æ¥ Milvus å¤±æ•—: {e}")
            
    def initialize_all(self):
        """åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶"""
        self.initialize_embedding_model()
        self.initialize_llm_model()
        self.connect_milvus()

# å‰µå»º RAG ç³»çµ±å¯¦ä¾‹
rag_system = RAGSystem()
```

## 5. è¼‰å…¥æ¸¬è©¦è³‡æ–™

```python
def load_test_data(csv_path: str) -> pd.DataFrame:
    """è¼‰å…¥æ¸¬è©¦ CSV è³‡æ–™"""
    try:
        df = pd.read_csv(csv_path)
        print(f"æˆåŠŸè¼‰å…¥æ¸¬è©¦è³‡æ–™ï¼Œå…± {len(df)} ç­†")
        print("è³‡æ–™æ¬„ä½:", df.columns.tolist())
        return df
    except Exception as e:
        print(f"è¼‰å…¥æ¸¬è©¦è³‡æ–™å¤±æ•—: {e}")
        return None

# è«‹æŒ‡å®šæ‚¨çš„ CSV æª”æ¡ˆè·¯å¾‘
CSV_PATH = "your_test_data.csv"  # è«‹ä¿®æ”¹ç‚ºæ‚¨çš„æª”æ¡ˆè·¯å¾‘
test_data = load_test_data(CSV_PATH)

# é¡¯ç¤ºè³‡æ–™æ¦‚è¦½
if test_data is not None:
    print("\nè³‡æ–™æ¦‚è¦½:")
    print(test_data.head())
```

## 6. æŸ¥è©¢å‘é‡åŒ–

```python
def vectorize_query(query: str) -> np.ndarray:
    """å°‡æŸ¥è©¢æ–‡æœ¬è½‰æ›ç‚ºå‘é‡"""
    if rag_system.embedding_model is None:
        raise ValueError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")
        
    embedding = rag_system.embedding_model.encode([query])
    return embedding[0].astype(np.float32)

# æ¸¬è©¦å‘é‡åŒ–åŠŸèƒ½
test_query = "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"
test_vector = vectorize_query(test_query)
print(f"æ¸¬è©¦æŸ¥è©¢: {test_query}")
print(f"å‘é‡ç¶­åº¦: {test_vector.shape}")
print(f"å‘é‡å‰5å€‹å€¼: {test_vector[:5]}")
```

## 7. å‘é‡æª¢ç´¢åŠŸèƒ½

```python
def search_similar_chunks(query_vector: np.ndarray, top_k: int = None) -> List[Dict]:
    """åœ¨ Milvus ä¸­æœå°‹ç›¸ä¼¼çš„æ–‡æœ¬å¡Š"""
    if top_k is None:
        top_k = config.TOP_K
        
    if rag_system.collection is None:
        raise ValueError("Milvus é›†åˆæœªé€£æ¥")
        
    search_params = {
        "metric_type": "COSINE",
        "params": {"nprobe": 16}
    }
    
    # åŸ·è¡Œå‘é‡æœå°‹
    results = rag_system.collection.search(
        data=[query_vector],
        anns_field="embedding",
        param=search_params,
        limit=top_k,
        output_fields=[
            "chunk_id", "chunk_text", "podcast_name", "author", 
            "episode_title", "category", "published_date"
        ]
    )
    
    retrieved_chunks = []
    for hits in results:
        for hit in hits:
            chunk_data = {
                "chunk_id": hit.entity.get("chunk_id"),
                "chunk_text": hit.entity.get("chunk_text"),
                "podcast_name": hit.entity.get("podcast_name"),
                "author": hit.entity.get("author"),
                "episode_title": hit.entity.get("episode_title"),
                "category": hit.entity.get("category"),
                "published_date": hit.entity.get("published_date"),
                "similarity_score": hit.score
            }
            retrieved_chunks.append(chunk_data)
    
    return retrieved_chunks
```

## 8. LLM æ–‡æœ¬ç”Ÿæˆ

```python
def generate_response(query: str, context_chunks: List[Dict]) -> str:
    """ä½¿ç”¨æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”"""
    
    # æ§‹å»ºä¸Šä¸‹æ–‡
    context_text = ""
    for i, chunk in enumerate(context_chunks, 1):
        context_text += f"\nåƒè€ƒè³‡æ–™ {i}:\n"
        context_text += f"ä¾†æº: {chunk['podcast_name']} - {chunk['episode_title']}\n"
        context_text += f"å…§å®¹: {chunk['chunk_text']}\n"
        context_text += f"ç›¸ä¼¼åº¦: {chunk['similarity_score']:.4f}\n"
    
    # æ§‹å»ºæç¤ºè©
    prompt = f"""åŸºæ–¼ä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œã€‚è«‹æä¾›æº–ç¢ºã€æœ‰ç”¨çš„å›ç­”ï¼Œä¸¦åœ¨é©ç•¶æ™‚å¼•ç”¨è³‡æ–™ä¾†æºã€‚

åƒè€ƒè³‡æ–™:
{context_text}

å•é¡Œ: {query}

å›ç­”:"""
    
    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”
    inputs = rag_system.tokenizer.encode(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = rag_system.llm_model.generate(
            inputs,
            max_length=len(inputs[0]) + config.MAX_LENGTH,
            temperature=config.TEMPERATURE,
            do_sample=True,
            pad_token_id=rag_system.tokenizer.eos_token_id
        )
    
    # è§£ç¢¼å›ç­”
    full_response = rag_system.tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = full_response[len(prompt):].strip()
    
    return response
```

## 9. å®Œæ•´çš„ RAG æŸ¥è©¢æµç¨‹

```python
def rag_query(query: str) -> Dict[str, Any]:
    """å®Œæ•´çš„ RAG æŸ¥è©¢æµç¨‹"""
    
    print(f"\nğŸ” æŸ¥è©¢: {query}")
    print("=" * 80)
    
    # æ­¥é©Ÿ 1: æŸ¥è©¢å‘é‡åŒ–
    print("1. æ­£åœ¨é€²è¡ŒæŸ¥è©¢å‘é‡åŒ–...")
    query_vector = vectorize_query(query)
    
    # æ­¥é©Ÿ 2: å‘é‡æª¢ç´¢
    print("2. æ­£åœ¨æª¢ç´¢ç›¸é—œæ–‡æª”...")
    retrieved_chunks = search_similar_chunks(query_vector)
    
    if not retrieved_chunks:
        return {
            "query": query,
            "answer": "æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„è³‡æ–™ã€‚",
            "retrieved_chunks": [],
            "timestamp": datetime.now().isoformat()
        }
    
    # æ­¥é©Ÿ 3: ç”Ÿæˆå›ç­”
    print("3. æ­£åœ¨ç”Ÿæˆå›ç­”...")
    answer = generate_response(query, retrieved_chunks)
    
    # æ­¥é©Ÿ 4: æ•´ç†çµæœ
    result = {
        "query": query,
        "answer": answer,
        "retrieved_chunks": retrieved_chunks,
        "timestamp": datetime.now().isoformat()
    }
    
    print("4. æŸ¥è©¢å®Œæˆ!")
    return result
```

## 10. æ¸¬è©¦é›†è©•ä¼°åŠŸèƒ½

```python
def evaluate_test_set(test_df: pd.DataFrame, query_column: str, expected_answer_column: str = None):
    """è©•ä¼°æ¸¬è©¦é›†"""
    
    results = []
    
    print(f"é–‹å§‹è©•ä¼°æ¸¬è©¦é›†ï¼Œå…± {len(test_df)} å€‹æŸ¥è©¢...")
    
    for index, row in test_df.iterrows():
        query = row[query_column]
        expected_answer = row[expected_answer_column] if expected_answer_column else None
        
        print(f"\nè™•ç†ç¬¬ {index + 1}/{len(test_df)} å€‹æŸ¥è©¢...")
        
        try:
            result = rag_query(query)
            result['test_index'] = index
            result['expected_answer'] = expected_answer
            results.append(result)
            
        except Exception as e:
            print(f"æŸ¥è©¢å¤±æ•—: {e}")
            results.append({
                "test_index": index,
                "query": query,
                "answer": f"éŒ¯èª¤: {e}",
                "expected_answer": expected_answer,
                "retrieved_chunks": [],
                "timestamp": datetime.now().isoformat()
            })
    
    return results

def save_evaluation_results(results: List[Dict], output_path: str):
    """å„²å­˜è©•ä¼°çµæœ"""
    results_df = pd.DataFrame(results)
    results_df.to_csv(output_path, index=False, encoding='utf-8')
    print(f"è©•ä¼°çµæœå·²å„²å­˜è‡³: {output_path}")
```

## 11. ä¸»è¦åŸ·è¡Œå€å¡Š

```python
# åˆå§‹åŒ–ç³»çµ±
print("æ­£åœ¨åˆå§‹åŒ– RAG ç³»çµ±...")
rag_system.initialize_all()

# å¦‚æœæœ‰æ¸¬è©¦è³‡æ–™ï¼Œé€²è¡Œè©•ä¼°
if test_data is not None and len(test_data) > 0:
    # å‡è¨­æ¸¬è©¦è³‡æ–™æœ‰ 'query' æ¬„ä½ï¼Œå¦‚æœæœ‰æœŸæœ›ç­”æ¡ˆæ¬„ä½è«‹æŒ‡å®š
    QUERY_COLUMN = "query"  # è«‹æ ¹æ“šæ‚¨çš„è³‡æ–™ä¿®æ”¹
    EXPECTED_ANSWER_COLUMN = None  # å¦‚æœæœ‰æœŸæœ›ç­”æ¡ˆæ¬„ä½ï¼Œè«‹æŒ‡å®šæ¬„ä½åç¨±
    
    # åŸ·è¡Œæ¸¬è©¦é›†è©•ä¼°
    evaluation_results = evaluate_test_set(test_data, QUERY_COLUMN, EXPECTED_ANSWER_COLUMN)
    
    # å„²å­˜çµæœ
    output_path = f"rag_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    save_evaluation_results(evaluation_results, output_path)
    
else:
    print("æœªæ‰¾åˆ°æ¸¬è©¦è³‡æ–™ï¼Œå¯ä»¥æ‰‹å‹•æ¸¬è©¦æŸ¥è©¢:")
    
    # æ‰‹å‹•æ¸¬è©¦ç¯„ä¾‹
    test_queries = [
        "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",
        "å¦‚ä½•å­¸ç¿’ç¨‹å¼è¨­è¨ˆï¼Ÿ",
        "å¥åº·é£²é£Ÿçš„å»ºè­°æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    for query in test_queries:
        result = rag_query(query)
        print(f"\nğŸ“‹ æŸ¥è©¢çµæœ:")
        print(f"å•é¡Œ: {result['query']}")
        print(f"å›ç­”: {result['answer']}")
        print(f"æª¢ç´¢åˆ° {len(result['retrieved_chunks'])} å€‹ç›¸é—œæ–‡æª”")
```

## 12. çµæœåˆ†æå’Œè¦–è¦ºåŒ–

```python
def analyze_results(results: List[Dict]):
    """åˆ†æè©•ä¼°çµæœ"""
    
    print("\nğŸ“Š è©•ä¼°çµæœåˆ†æ:")
    print("=" * 50)
    
    total_queries = len(results)
    successful_queries = sum(1 for r in results if not r['answer'].startswith('éŒ¯èª¤:'))
    
    print(f"ç¸½æŸ¥è©¢æ•¸: {total_queries}")
    print(f"æˆåŠŸæŸ¥è©¢æ•¸: {successful_queries}")
    print(f"æˆåŠŸç‡: {successful_queries/total_queries*100:.2f}%")
    
    # æª¢ç´¢æ–‡æª”æ•¸é‡çµ±è¨ˆ
    doc_counts = [len(r['retrieved_chunks']) for r in results if r['retrieved_chunks']]
    if doc_counts:
        print(f"å¹³å‡æª¢ç´¢æ–‡æª”æ•¸: {np.mean(doc_counts):.2f}")
        print(f"æª¢ç´¢æ–‡æª”æ•¸ç¯„åœ: {min(doc_counts)} - {max(doc_counts)}")
    
    # ç›¸ä¼¼åº¦åˆ†æ•¸çµ±è¨ˆ
    similarity_scores = []
    for result in results:
        for chunk in result.get('retrieved_chunks', []):
            similarity_scores.append(chunk['similarity_score'])
    
    if similarity_scores:
        print(f"å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•¸: {np.mean(similarity_scores):.4f}")
        print(f"ç›¸ä¼¼åº¦åˆ†æ•¸ç¯„åœ: {min(similarity_scores):.4f} - {max(similarity_scores):.4f}")

# å¦‚æœæœ‰è©•ä¼°çµæœï¼Œé€²è¡Œåˆ†æ
if 'evaluation_results' in locals():
    analyze_results(evaluation_results)
```

## 13. å·¥å…·å‡½æ•¸

```python
def display_query_details(result: Dict):
    """é¡¯ç¤ºæŸ¥è©¢çš„è©³ç´°çµæœ"""
    print(f"\nğŸ” æŸ¥è©¢: {result['query']}")
    print(f"â° æ™‚é–“: {result['timestamp']}")
    print(f"\nğŸ’¡ å›ç­”:\n{result['answer']}")
    
    print(f"\nğŸ“š æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª” ({len(result['retrieved_chunks'])} å€‹):")
    for i, chunk in enumerate(result['retrieved_chunks'], 1):
        print(f"\næ–‡æª” {i}:")
        print(f"  ä¾†æº: {chunk['podcast_name']} - {chunk['episode_title']}")
        print(f"  ä½œè€…: {chunk['author']}")
        print(f"  åˆ†é¡: {chunk['category']}")
        print(f"  ç›¸ä¼¼åº¦: {chunk['similarity_score']:.4f}")
        print(f"  å…§å®¹é è¦½: {chunk['chunk_text'][:200]}...")

def interactive_query():
    """äº’å‹•å¼æŸ¥è©¢ä»‹é¢"""
    while True:
        query = input("\nè«‹è¼¸å…¥æŸ¥è©¢ (è¼¸å…¥ 'quit' çµæŸ): ").strip()
        
        if query.lower() == 'quit':
            break
            
        if not query:
            continue
            
        try:
            result = rag_query(query)
            display_query_details(result)
        except Exception as e:
            print(f"æŸ¥è©¢éŒ¯èª¤: {e}")

# å•Ÿå‹•äº’å‹•å¼æŸ¥è©¢ (å¯é¸)
# interactive_query()
```

print("âœ… RAG ç³»çµ±æ¶æ§‹å®Œæˆï¼")
print("è«‹ç¢ºä¿:")
print("1. ä¿®æ”¹ Config é¡åˆ¥ä¸­çš„åƒæ•¸ä»¥ç¬¦åˆæ‚¨çš„ç’°å¢ƒ")
print("2. ç¢ºä¿ Milvus æœå‹™æ­£åœ¨é‹è¡Œ")
print("3. æŒ‡å®šæ­£ç¢ºçš„æ¸¬è©¦ CSV æª”æ¡ˆè·¯å¾‘")
print("4. æ ¹æ“šæ‚¨çš„ CSV æ¬„ä½åç¨±ä¿®æ”¹ QUERY_COLUMN è®Šæ•¸")