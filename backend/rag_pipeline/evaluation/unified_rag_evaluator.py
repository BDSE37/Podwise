#!/usr/bin/env python3
"""
çµ±ä¸€ RAG è©•ä¼°å™¨ - Podwise å°ˆæ¡ˆ

æ•´åˆåŠŸèƒ½ï¼š
1. åŸºç¤ RAG è©•ä¼°ï¼ˆBaselineã€Naive RAGã€Ragasï¼‰
2. å¢å¼·ç‰ˆè©•ä¼°ï¼ˆMilvus æª¢ç´¢ã€æœ¬åœ°/OpenAI LLM å°æ¯”ï¼‰
3. Podwise å°ˆç”¨åŠŸèƒ½ï¼ˆå‘é‡è³‡æ–™åº«æ•´åˆï¼‰
4. ihower æ¡†æ¶æ•´åˆ
5. æç¤ºè©æ¨¡æ¿æ•´åˆ

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 3.0.0
"""

import os
import sys
import json
import time
import logging
import asyncio
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from datetime import datetime
import hashlib
from pathlib import Path

# æ·»åŠ è·¯å¾‘ä»¥ä¾¿å°å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å˜—è©¦å°å…¥ Podwise å°ˆæ¡ˆç›¸é—œæ¨¡çµ„
try:
    from config.integrated_config import get_config
    PODWISE_CONFIG_AVAILABLE = True
except ImportError:
    PODWISE_CONFIG_AVAILABLE = False
    logger.warning("Podwise é…ç½®æ¨¡çµ„æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é è¨­é…ç½®")

try:
    from config.prompt_templates import get_prompt_template, format_prompt
    PROMPT_TEMPLATES_AVAILABLE = True
except ImportError:
    PROMPT_TEMPLATES_AVAILABLE = False
    logger.warning("æç¤ºè©æ¨¡æ¿æ¨¡çµ„æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é è¨­æ¨¡æ¿")

# é è¨­é…ç½®
def get_default_config():
    class Config:
        class Database:
            milvus_host = os.getenv("MILVUS_HOST", "worker3")
            milvus_port = int(os.getenv("MILVUS_PORT", "19530"))
            milvus_collection = os.getenv("MILVUS_COLLECTION", "podcast_chunks")
        class API:
            openai_api_key = os.getenv("OPENAI_API_KEY", "")
        database = Database()
        api = API()
    return Config()

@dataclass
class EvaluationResult:
    """è©•ä¼°çµæœæ•¸æ“šé¡åˆ¥"""
    question: str
    answer: str
    confidence: float
    factuality: float
    relevance: float
    coherence: float
    inference_time: float
    token_count: int
    model_name: str
    retrieval_method: str
    sources: List[Dict[str, Any]]

@dataclass
class ComparisonResult:
    """å°æ¯”çµæœæ•¸æ“šé¡åˆ¥"""
    question: str
    local_llm_result: EvaluationResult
    openai_result: EvaluationResult
    comparison_metrics: Dict[str, float]

class MilvusOptimizedSearch:
    """å„ªåŒ–çš„ Milvus æœå°‹æœå‹™"""
    
    def __init__(self):
        """åˆå§‹åŒ–å„ªåŒ–çš„ Milvus æœå°‹"""
        self.config = get_config() if PODWISE_CONFIG_AVAILABLE else get_default_config()
        self.collection = None
        self.is_connected = False
        self._connect()
        
    def _connect(self):
        """é€£æ¥åˆ° Milvus"""
        try:
            from pymilvus import connections, Collection, utility
            
            # é€£æ¥åˆ° Milvus
            connections.connect(
                alias="default",
                host=self.config.database.milvus_host,
                port=self.config.database.milvus_port
            )
            
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collection_name = self.config.database.milvus_collection
            if utility.has_collection(collection_name):
                self.collection = Collection(collection_name)
                self.is_connected = True
                
                # æª¢æŸ¥é›†åˆçµ±è¨ˆè³‡è¨Š
                stats = self.collection.num_entities
                logger.info(f"âœ… Milvus é›†åˆ '{collection_name}' é€£æ¥æˆåŠŸï¼Œå‘é‡æ•¸é‡: {stats}")
                
            else:
                logger.warning(f"âš ï¸ Milvus é›†åˆ '{collection_name}' ä¸å­˜åœ¨")
                
        except ImportError:
            logger.warning("âš ï¸ pymilvus æœªå®‰è£")
        except Exception as e:
            logger.error(f"âŒ Milvus é€£æ¥å¤±æ•—: {e}")
    
    async def search(self, query: str, top_k: int = 10, nprobe: int = 16) -> List[Dict[str, Any]]:
        """åŸ·è¡Œå„ªåŒ–çš„å‘é‡æœå°‹"""
        try:
            if not self.is_connected or self.collection is None:
                logger.warning("Milvus æœªé€£æ¥ï¼Œè¿”å›ç©ºçµæœ")
                return []
            
            # æ–‡æœ¬å‘é‡åŒ–ï¼ˆä½¿ç”¨ç°¡å–®çš„å“ˆå¸Œæ–¹æ³•ä½œç‚ºç¤ºä¾‹ï¼‰
            embedding = self._text_to_vector(query)
            if not embedding:
                return []
            
            # è¼‰å…¥é›†åˆ
            self.collection.load()
            
            # å„ªåŒ–çš„æœå°‹åƒæ•¸
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": nprobe}
            }
            
            # åŸ·è¡Œæœå°‹
            results = self.collection.search(
                data=[embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["chunk_id", "chunk_text", "tags", "podcast_name", "episode_title", "category", "language"]
            )
            
            # æ ¼å¼åŒ–çµæœ
            formatted_results = []
            for hits in results:
                for hit in hits:
                    # è§£æ tags æ¬„ä½
                    tags = []
                    try:
                        if hasattr(hit, 'entity') and hit.entity:
                            tags_data = hit.entity.get("tags")
                            if tags_data:
                                if isinstance(tags_data, str):
                                    tags = json.loads(tags_data)
                                else:
                                    tags = tags_data
                    except:
                        tags = []
                    
                    # å®‰å…¨ç²å–å¯¦é«”æ•¸æ“š
                    entity_data = {}
                    chunk_text = ""
                    if hasattr(hit, 'entity') and hit.entity:
                        try:
                            entity_data = {
                                "podcast_name": hit.entity.get("podcast_name") or "",
                                "episode_title": hit.entity.get("episode_title") or "",
                                "category": hit.entity.get("category") or "",
                                "language": hit.entity.get("language") or "",
                                "chunk_id": hit.entity.get("chunk_id") or ""
                            }
                            chunk_text = hit.entity.get("chunk_text") or ""
                        except Exception as e:
                            logger.warning(f"ç²å–å¯¦é«”æ•¸æ“šå¤±æ•—: {e}")
                            # ä½¿ç”¨å±¬æ€§è¨ªå•
                            try:
                                entity_data = {
                                    "podcast_name": getattr(hit.entity, "podcast_name", ""),
                                    "episode_title": getattr(hit.entity, "episode_title", ""),
                                    "category": getattr(hit.entity, "category", ""),
                                    "language": getattr(hit.entity, "language", ""),
                                    "chunk_id": getattr(hit.entity, "chunk_id", "")
                                }
                                chunk_text = getattr(hit.entity, "chunk_text", "")
                            except:
                                pass
                    
                    formatted_results.append({
                        "chunk_text": chunk_text,
                        "similarity": float(hit.score),
                        "metadata": entity_data,
                        "tags": tags
                    })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Milvus æœå°‹å¤±æ•—: {e}")
            return []
    
    def _text_to_vector(self, text: str) -> Optional[List[float]]:
        """å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ä½¿ç”¨ç°¡å–®çš„å“ˆå¸Œæ–¹æ³•ä½œç‚ºç¤ºä¾‹
            # å¯¦éš›ä½¿ç”¨æ™‚æ‡‰è©²ä½¿ç”¨çœŸæ­£çš„åµŒå…¥æ¨¡å‹
            hash_obj = hashlib.md5(text.encode('utf-8'))
            hash_hex = hash_obj.hexdigest()
            
            # å°‡å“ˆå¸Œè½‰æ›ç‚º 1536 ç¶­å‘é‡ï¼ˆæ¨¡æ“¬ OpenAI åµŒå…¥ç¶­åº¦ï¼‰
            vector = []
            for i in range(0, len(hash_hex), 2):
                if len(vector) >= 1536:
                    break
                hex_pair = hash_hex[i:i+2]
                vector.append(int(hex_pair, 16) / 255.0)
            
            # å¡«å……åˆ° 1536 ç¶­
            while len(vector) < 1536:
                vector.append(0.0)
            
            return vector[:1536]
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å‘é‡åŒ–å¤±æ•—: {e}")
            return None

class PodwiseVectorSearch:
    """Podwise å‘é‡è³‡æ–™åº«æœå°‹æ¨¡çµ„"""
    
    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–å‘é‡æœå°‹å¼•æ“
        
        Args:
            data_dir: å‘é‡è³‡æ–™ç›®éŒ„è·¯å¾‘ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•æª¢æ¸¬
        """
        if data_dir is None:
            # è‡ªå‹•æª¢æ¸¬è·¯å¾‘
            current_dir = Path(__file__).parent
            project_root = current_dir.parent.parent.parent
            data_dir = project_root / "backend" / "vector_pipeline" / "data" / "stage4_embedding_prep"
        
        self.data_dir = Path(data_dir)
        self.chunks_data = []
        self.embeddings = []
        self.chunk_texts = []
        self.metadata = []
        
        # è¼‰å…¥æ‰€æœ‰å‘é‡è³‡æ–™
        self._load_vector_data()
        
    def _load_vector_data(self):
        """è¼‰å…¥æ‰€æœ‰ RSS ç›®éŒ„ä¸­çš„å‘é‡è³‡æ–™"""
        logger.info(f"é–‹å§‹è¼‰å…¥å‘é‡è³‡æ–™å¾: {self.data_dir}")
        
        total_chunks = 0
        
        # éæ­·æ‰€æœ‰ RSS ç›®éŒ„
        for rss_dir in self.data_dir.iterdir():
            if rss_dir.is_dir() and rss_dir.name.startswith("RSS_"):
                logger.info(f"è¼‰å…¥ç›®éŒ„: {rss_dir.name}")
                
                # è¼‰å…¥è©²ç›®éŒ„ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶
                for json_file in rss_dir.glob("*.json"):
                    if not json_file.name.endswith(".backup"):
                        try:
                            with open(json_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                
                            # è™•ç†æ¯å€‹ chunk
                            for chunk in data:
                                if isinstance(chunk, dict) and 'chunk_text' in chunk and 'embedding' in chunk:
                                    self.chunks_data.append(chunk)
                                    self.chunk_texts.append(chunk['chunk_text'])
                                    self.embeddings.append(chunk['embedding'])
                                    
                                    # æå–å…ƒè³‡æ–™
                                    metadata = {
                                        'chunk_id': chunk.get('chunk_id'),
                                        'episode_id': chunk.get('episode_id'),
                                        'podcast_id': chunk.get('podcast_id'),
                                        'podcast_name': chunk.get('podcast_name'),
                                        'episode_title': chunk.get('episode_title'),
                                        'author': chunk.get('author'),
                                        'category': chunk.get('category'),
                                        'published_date': chunk.get('published_date'),
                                        'rss_dir': rss_dir.name
                                    }
                                    self.metadata.append(metadata)
                                    
                                    total_chunks += 1
                                    
                        except json.JSONDecodeError as e:
                            logger.warning(f"JSON è§£æéŒ¯èª¤ï¼Œè·³éæ–‡ä»¶ {json_file}: {str(e)}")
                            continue
                        except Exception as e:
                            logger.warning(f"è¼‰å…¥æ–‡ä»¶å¤±æ•— {json_file}: {str(e)}")
                            continue
        
        # è½‰æ›ç‚º numpy é™£åˆ—ä»¥æé«˜æ•ˆèƒ½
        if self.embeddings:
            self.embeddings = np.array(self.embeddings)
            logger.info(f"æˆåŠŸè¼‰å…¥ {total_chunks} å€‹ chunksï¼Œå‘é‡ç¶­åº¦: {self.embeddings.shape}")
        else:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•å‘é‡è³‡æ–™")
    
    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:
        """åŸ·è¡Œå‘é‡ç›¸ä¼¼æ€§æœå°‹"""
        if not self.embeddings.size:
            logger.warning("å‘é‡è³‡æ–™åº«ç‚ºç©º")
            return []
        
        try:
            # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
            query_vector = np.array(query_embedding).reshape(1, -1)
            similarities = self._cosine_similarity(query_vector, self.embeddings)
            
            # ç²å– top-k çµæœ
            top_indices = np.argsort(similarities[0])[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                similarity = similarities[0][idx]
                result = {
                    'chunk_text': self.chunk_texts[idx],
                    'similarity': float(similarity),
                    'metadata': self.metadata[idx]
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"å‘é‡æœå°‹å¤±æ•—: {str(e)}")
            return []
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        # æ­£è¦åŒ–å‘é‡
        a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)
        b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        return np.dot(a_norm, b_norm.T)
    
    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–å‘é‡è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š"""
        if not self.embeddings.size:
            return {"total_chunks": 0, "vector_dimension": 0}
        
        # çµ±è¨ˆæ’­å®¢æ•¸é‡
        podcast_ids = set()
        episode_ids = set()
        categories = set()
        
        for metadata in self.metadata:
            if metadata.get('podcast_id'):
                podcast_ids.add(metadata['podcast_id'])
            if metadata.get('episode_id'):
                episode_ids.add(metadata['episode_id'])
            if metadata.get('category'):
                categories.add(metadata['category'])
        
        return {
            "total_chunks": len(self.chunks_data),
            "vector_dimension": self.embeddings.shape[1] if self.embeddings.size else 0,
            "total_podcasts": len(podcast_ids),
            "total_episodes": len(episode_ids),
            "categories": list(categories),
            "data_loaded": True
        }

class LocalLLMService:
    """æœ¬åœ° LLM æœå‹™"""
    
    def __init__(self, model_path: str = None):
        """åˆå§‹åŒ–æœ¬åœ° LLM æœå‹™"""
        if model_path is None:
            # è‡ªå‹•æª¢æ¸¬è·¯å¾‘
            current_dir = Path(__file__).parent
            project_root = current_dir.parent.parent.parent
            model_path = project_root / "Qwen2.5-Taiwan-7B-Instruct"
        
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.prompt_templates = {}
        self._load_prompt_templates()
        self._load_model()
    
    def _load_prompt_templates(self):
        """è¼‰å…¥æç¤ºè©æ¨¡æ¿"""
        if PROMPT_TEMPLATES_AVAILABLE:
            try:
                # è¼‰å…¥ Podwise æç¤ºè©æ¨¡æ¿
                self.prompt_templates = {
                    "answer_generation": get_prompt_template("answer_generation"),
                    "faq_fallback": get_prompt_template("faq_fallback"),
                    "default_fallback": get_prompt_template("default_fallback")
                }
                logger.info("âœ… æˆåŠŸè¼‰å…¥ Podwise æç¤ºè©æ¨¡æ¿")
            except Exception as e:
                logger.warning(f"è¼‰å…¥ Podwise æç¤ºè©æ¨¡æ¿å¤±æ•—: {e}")
                self._load_default_templates()
        else:
            self._load_default_templates()
    
    def _load_default_templates(self):
        """è¼‰å…¥é è¨­æç¤ºè©æ¨¡æ¿"""
        self.prompt_templates = {
            "answer_generation": """å—¨å—¨ğŸ‘‹ æƒ³äº†è§£ã€Œ{question}ã€å—ï¼Ÿä»¥ä¸‹æ˜¯ç›¸é—œçš„ç²¾å½©ç¯€ç›®ï¼š

ğŸ§ã€Šæ¨¡æ“¬æœ¬åœ° LLM æ¨è–¦ç¯€ç›®ã€‹ç¬¬ 2 é›†ï¼šã€ˆå•†æ¥­åˆ†æã€‰
ğŸ‘‰ é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„æœ¬åœ° LLM å›æ‡‰ï¼ŒåŸºæ–¼ prompt_templates.py æ ¼å¼

ğŸ’¡ å»ºè­°ï¼š
- é€™æ˜¯ä¸€å€‹æ¨¡æ“¬å›æ‡‰
- å¯¦éš›ä½¿ç”¨æ™‚æœƒè¼‰å…¥çœŸå¯¦çš„ Qwen2.5-Taiwan-7B-Instruct æ¨¡å‹
- æœƒæ ¹æ“šæ‚¨çš„å•é¡Œæä¾›ç›¸é—œçš„ Podcast æ¨è–¦

å¸Œæœ›é€™å€‹å›æ‡‰å°æ‚¨æœ‰å¹«åŠ©ï¼ğŸ˜Š""",
            
            "faq_fallback": """å—¨å—¨ğŸ‘‹ é—œæ–¼ã€Œ{question}ã€ï¼Œé€™æ˜¯ä¸€å€‹å¸¸è¦‹å•é¡Œï¼

ğŸ“š å¸¸è¦‹è§£ç­”ï¼š
- é€™æ˜¯ä¸€å€‹ FAQ å‚™æ´å›æ‡‰
- åŸºæ–¼ Podwise çš„å¸¸è¦‹å•é¡Œè³‡æ–™åº«
- æä¾›æ¨™æº–åŒ–çš„è§£ç­”æ ¼å¼

ğŸ’¡ å°æé†’ï¼šå¦‚æœéœ€è¦æ›´è©³ç´°çš„è³‡è¨Šï¼Œå¯ä»¥å˜—è©¦é‡æ–°æè¿°æ‚¨çš„å•é¡Œå–”ï¼""",
            
            "default_fallback": """å—¨å—¨ğŸ‘‹ æŠ±æ­‰ï¼Œæˆ‘ç›®å‰ç„¡æ³•å®Œå…¨ç†è§£æ‚¨çš„å•é¡Œã€Œ{question}ã€

ğŸ¤” å»ºè­°æ‚¨å¯ä»¥ï¼š
- é‡æ–°æè¿°æ‚¨çš„å•é¡Œ
- æä¾›æ›´å¤šä¸Šä¸‹æ–‡è³‡è¨Š
- å˜—è©¦ä¸åŒçš„è¡¨é”æ–¹å¼

æˆ‘æœƒåŠªåŠ›ç‚ºæ‚¨æä¾›æ›´å¥½çš„æœå‹™ï¼ğŸ˜Š"""
        }
        logger.info("âœ… è¼‰å…¥é è¨­æç¤ºè©æ¨¡æ¿")
    
    def _load_model(self):
        """è¼‰å…¥æœ¬åœ°æ¨¡å‹ï¼ˆæ¨¡æ“¬ç‰ˆæœ¬ï¼‰"""
        try:
            # é€™è£¡æ‡‰è©²æ˜¯å¯¦éš›çš„æ¨¡å‹è¼‰å…¥é‚è¼¯
            # ç›®å‰ä½¿ç”¨æ¨¡æ“¬ç‰ˆæœ¬
            logger.info(f"âœ… æ¨¡æ“¬è¼‰å…¥æœ¬åœ°æ¨¡å‹: {self.model_path}")
            self.model = "mock_model"
            self.tokenizer = "mock_tokenizer"
        except Exception as e:
            logger.warning(f"æœ¬åœ°æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self.model = None
            self.tokenizer = None
    
    async def generate_answer(self, prompt: str, context: str = "") -> Tuple[str, float, int]:
        """ç”Ÿæˆå›ç­”"""
        try:
            start_time = time.time()
            
            # é¸æ“‡åˆé©çš„æ¨¡æ¿
            if "æ¨è–¦" in prompt or "podcast" in prompt.lower():
                template = self.prompt_templates.get("answer_generation", self.prompt_templates["default_fallback"])
            elif any(keyword in prompt for keyword in ["ä»€éº¼", "å¦‚ä½•", "ç‚ºä»€éº¼"]):
                template = self.prompt_templates.get("faq_fallback", self.prompt_templates["default_fallback"])
            else:
                template = self.prompt_templates.get("default_fallback", self.prompt_templates["default_fallback"])
            
            # æ ¼å¼åŒ–æç¤ºè©
            if PROMPT_TEMPLATES_AVAILABLE:
                try:
                    formatted_prompt = format_prompt(template, question=prompt, context=context)
                except:
                    formatted_prompt = template.format(question=prompt, context=context)
            else:
                formatted_prompt = template.format(question=prompt, context=context)
            
            # æ¨¡æ“¬ç”Ÿæˆå›ç­”
            answer = formatted_prompt
            
            # è¨ˆç®—æ™‚é–“å’Œ token æ•¸
            inference_time = time.time() - start_time
            token_count = len(answer.split())
            
            return answer, inference_time, token_count
            
        except Exception as e:
            logger.error(f"æœ¬åœ° LLM ç”Ÿæˆå¤±æ•—: {e}")
            return "æœ¬åœ° LLM æœªè¼‰å…¥", 0.0, 0

class OpenAIService:
    """OpenAI æœå‹™"""
    
    def __init__(self):
        """åˆå§‹åŒ– OpenAI æœå‹™"""
        self.config = get_config() if PODWISE_CONFIG_AVAILABLE else get_default_config()
        self.client = None
        self.prompt_templates = {}
        self._setup_client()
        self._load_prompt_templates()
    
    def _setup_client(self):
        """è¨­å®š OpenAI å®¢æˆ¶ç«¯"""
        try:
            from openai import OpenAI
            
            api_key = self.config.api.openai_api_key
            if api_key and api_key != "your_openai_api_key_here":
                self.client = OpenAI(api_key=api_key)
                logger.info("âœ… OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("âš ï¸ OpenAI API Key æœªè¨­å®šæˆ–ç„¡æ•ˆ")
                self.client = None
                
        except ImportError:
            logger.warning("âš ï¸ openai å¥—ä»¶æœªå®‰è£")
            self.client = None
        except Exception as e:
            logger.error(f"âŒ OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            self.client = None
    
    def _load_prompt_templates(self):
        """è¼‰å…¥æç¤ºè©æ¨¡æ¿"""
        if PROMPT_TEMPLATES_AVAILABLE:
            try:
                # è¼‰å…¥ Podwise æç¤ºè©æ¨¡æ¿
                self.prompt_templates = {
                    "answer_generation": get_prompt_template("answer_generation"),
                    "faq_fallback": get_prompt_template("faq_fallback"),
                    "default_fallback": get_prompt_template("default_fallback")
                }
                logger.info("âœ… æˆåŠŸè¼‰å…¥ Podwise æç¤ºè©æ¨¡æ¿")
            except Exception as e:
                logger.warning(f"è¼‰å…¥ Podwise æç¤ºè©æ¨¡æ¿å¤±æ•—: {e}")
                self._load_default_templates()
        else:
            self._load_default_templates()
    
    def _load_default_templates(self):
        """è¼‰å…¥é è¨­æç¤ºè©æ¨¡æ¿"""
        self.prompt_templates = {
            "answer_generation": """å—¨å—¨ğŸ‘‹ æƒ³äº†è§£ã€Œ{question}ã€å—ï¼Ÿä»¥ä¸‹æ˜¯ç›¸é—œçš„ç²¾å½©ç¯€ç›®ï¼š

ğŸ§ã€Šæ¨¡æ“¬ OpenAI æ¨è–¦ç¯€ç›®ã€‹ç¬¬ 5 é›†ï¼šã€ˆæ·±åº¦å­¸ç¿’ã€‰
ğŸ‘‰ é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„ OpenAI å›æ‡‰ï¼ŒåŸºæ–¼ prompt_templates.py æ ¼å¼

ğŸ’¡ å»ºè­°ï¼š
- é€™æ˜¯ä¸€å€‹æ¨¡æ“¬å›æ‡‰
- å¯¦éš›ä½¿ç”¨æ™‚æœƒèª¿ç”¨ OpenAI API
- æœƒæ ¹æ“šæ‚¨çš„å•é¡Œæä¾›ç›¸é—œçš„ Podcast æ¨è–¦

å¸Œæœ›é€™å€‹å›æ‡‰å°æ‚¨æœ‰å¹«åŠ©ï¼ğŸ˜Š""",
            
            "faq_fallback": """å—¨å—¨ğŸ‘‹ é—œæ–¼ã€Œ{question}ã€ï¼Œé€™æ˜¯ä¸€å€‹å¸¸è¦‹å•é¡Œï¼

ğŸ“š å¸¸è¦‹è§£ç­”ï¼š
- é€™æ˜¯ä¸€å€‹ FAQ å‚™æ´å›æ‡‰
- åŸºæ–¼ Podwise çš„å¸¸è¦‹å•é¡Œè³‡æ–™åº«
- æä¾›æ¨™æº–åŒ–çš„è§£ç­”æ ¼å¼

ğŸ’¡ å°æé†’ï¼šå¦‚æœéœ€è¦æ›´è©³ç´°çš„è³‡è¨Šï¼Œå¯ä»¥å˜—è©¦é‡æ–°æè¿°æ‚¨çš„å•é¡Œå–”ï¼""",
            
            "default_fallback": """å—¨å—¨ğŸ‘‹ æŠ±æ­‰ï¼Œæˆ‘ç›®å‰ç„¡æ³•å®Œå…¨ç†è§£æ‚¨çš„å•é¡Œã€Œ{question}ã€

ğŸ¤” å»ºè­°æ‚¨å¯ä»¥ï¼š
- é‡æ–°æè¿°æ‚¨çš„å•é¡Œ
- æä¾›æ›´å¤šä¸Šä¸‹æ–‡è³‡è¨Š
- å˜—è©¦ä¸åŒçš„è¡¨é”æ–¹å¼

æˆ‘æœƒåŠªåŠ›ç‚ºæ‚¨æä¾›æ›´å¥½çš„æœå‹™ï¼ğŸ˜Š"""
        }
        logger.info("âœ… è¼‰å…¥é è¨­æç¤ºè©æ¨¡æ¿")
    
    async def generate_answer(self, prompt: str, context: str = "") -> Tuple[str, float, int]:
        """ç”Ÿæˆå›ç­”"""
        try:
            if not self.client:
                return "ç”Ÿæˆå¤±æ•—: OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–", 0.0, 0
            
            start_time = time.time()
            
            # é¸æ“‡åˆé©çš„æ¨¡æ¿
            if "æ¨è–¦" in prompt or "podcast" in prompt.lower():
                template = self.prompt_templates.get("answer_generation", self.prompt_templates["default_fallback"])
            elif any(keyword in prompt for keyword in ["ä»€éº¼", "å¦‚ä½•", "ç‚ºä»€éº¼"]):
                template = self.prompt_templates.get("faq_fallback", self.prompt_templates["default_fallback"])
            else:
                template = self.prompt_templates.get("default_fallback", self.prompt_templates["default_fallback"])
            
            # æ ¼å¼åŒ–æç¤ºè©
            if PROMPT_TEMPLATES_AVAILABLE:
                try:
                    formatted_prompt = format_prompt(template, question=prompt, context=context)
                except:
                    formatted_prompt = template.format(question=prompt, context=context)
            else:
                formatted_prompt = template.format(question=prompt, context=context)
            
            # èª¿ç”¨ OpenAI API
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Podcast æ¨è–¦åŠ©æ‰‹ï¼Œå°ˆé–€ç‚ºç”¨æˆ¶æ¨è–¦ç›¸é—œçš„ Podcast ç¯€ç›®ã€‚"},
                    {"role": "user", "content": formatted_prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content
            
            # è¨ˆç®—æ™‚é–“å’Œ token æ•¸
            inference_time = time.time() - start_time
            token_count = response.usage.total_tokens if hasattr(response, 'usage') else len(answer.split())
            
            return answer, inference_time, token_count
            
        except Exception as e:
            logger.error(f"OpenAI ç”Ÿæˆå¤±æ•—: {e}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}", 0.0, 0

class UnifiedRAGEvaluator:
    """çµ±ä¸€ RAG è©•ä¼°å™¨"""
    
    def __init__(self, use_mock_services: bool = True):
        """åˆå§‹åŒ–çµ±ä¸€è©•ä¼°å™¨"""
        self.use_mock_services = use_mock_services
        
        # åˆå§‹åŒ–æœå°‹æœå‹™
        self.milvus_search = MilvusOptimizedSearch()
        self.podwise_search = PodwiseVectorSearch()
        
        # åˆå§‹åŒ– LLM æœå‹™
        if use_mock_services:
            self.local_llm_service = LocalLLMService()
            self.openai_service = OpenAIService()
        else:
            # å¯¦éš›æœå‹™åˆå§‹åŒ–
            self.local_llm_service = LocalLLMService()
            self.openai_service = OpenAIService()
        
        logger.info("âœ… çµ±ä¸€ RAG è©•ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def evaluate_single_question(self, question: str) -> ComparisonResult:
        """è©•ä¼°å–®å€‹å•é¡Œ"""
        try:
            # åŸ·è¡Œå‘é‡æœå°‹
            search_results = await self.milvus_search.search(question, top_k=5)
            
            # æ§‹å»ºä¸Šä¸‹æ–‡
            context = self._build_context(search_results)
            
            # ç”Ÿæˆå›ç­”
            local_answer, local_time, local_tokens = await self.local_llm_service.generate_answer(question, context)
            openai_answer, openai_time, openai_tokens = await self.openai_service.generate_answer(question, context)
            
            # è¨ˆç®—æŒ‡æ¨™
            local_confidence = self._calculate_confidence(local_answer, search_results)
            openai_confidence = self._calculate_confidence(openai_answer, search_results)
            
            local_factuality = self._calculate_factuality(local_answer, search_results)
            openai_factuality = self._calculate_factuality(openai_answer, search_results)
            
            local_relevance = self._calculate_relevance(local_answer, question)
            openai_relevance = self._calculate_relevance(openai_answer, question)
            
            local_coherence = self._calculate_coherence(local_answer)
            openai_coherence = self._calculate_coherence(openai_answer)
            
            # å‰µå»ºçµæœå°è±¡
            local_result = EvaluationResult(
                question=question,
                answer=local_answer,
                confidence=local_confidence,
                factuality=local_factuality,
                relevance=local_relevance,
                coherence=local_coherence,
                inference_time=local_time,
                token_count=local_tokens,
                model_name="Qwen2.5-Taiwan-7B-Instruct",
                retrieval_method="Milvus",
                sources=search_results
            )
            
            openai_result = EvaluationResult(
                question=question,
                answer=openai_answer,
                confidence=openai_confidence,
                factuality=openai_factuality,
                relevance=openai_relevance,
                coherence=openai_coherence,
                inference_time=openai_time,
                token_count=openai_tokens,
                model_name="GPT-3.5-Turbo",
                retrieval_method="Milvus",
                sources=search_results
            )
            
            # è¨ˆç®—å°æ¯”æŒ‡æ¨™
            comparison_metrics = {
                "confidence_diff": local_confidence - openai_confidence,
                "factuality_diff": local_factuality - openai_factuality,
                "relevance_diff": local_relevance - openai_relevance,
                "coherence_diff": local_coherence - openai_coherence,
                "speed_ratio": local_time / openai_time if openai_time > 0 else 0,
                "token_ratio": local_tokens / openai_tokens if openai_tokens > 0 else 0
            }
            
            return ComparisonResult(
                question=question,
                local_llm_result=local_result,
                openai_result=openai_result,
                comparison_metrics=comparison_metrics
            )
            
        except Exception as e:
            logger.error(f"è©•ä¼°å•é¡Œå¤±æ•—: {e}")
            # è¿”å›éŒ¯èª¤çµæœ
            error_result = EvaluationResult(
                question=question,
                answer=f"è©•ä¼°å¤±æ•—: {str(e)}",
                confidence=0.0,
                factuality=0.0,
                relevance=0.0,
                coherence=0.0,
                inference_time=0.0,
                token_count=0,
                model_name="Error",
                retrieval_method="Error",
                sources=[]
            )
            
            return ComparisonResult(
                question=question,
                local_llm_result=error_result,
                openai_result=error_result,
                comparison_metrics={}
            )
    
    def _build_context(self, search_results: List[Dict[str, Any]]) -> str:
        """æ§‹å»ºä¸Šä¸‹æ–‡"""
        if not search_results:
            return ""
        
        context_parts = []
        for i, result in enumerate(search_results[:3]):  # åªä½¿ç”¨å‰3å€‹çµæœ
            chunk_text = result.get("chunk_text", "")
            metadata = result.get("metadata", {})
            podcast_name = metadata.get("podcast_name", "æœªçŸ¥æ’­å®¢")
            episode_title = metadata.get("episode_title", "æœªçŸ¥ç¯€ç›®")
            
            context_parts.append(f"[{i+1}] {podcast_name} - {episode_title}: {chunk_text[:200]}...")
        
        return "\n\n".join(context_parts)
    
    def _calculate_confidence(self, answer: str, sources: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—ä¿¡å¿ƒåº¦"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—") or answer.startswith("è©•ä¼°å¤±æ•—"):
            return 0.0
        
        # åŸºæ–¼å›ç­”é•·åº¦å’Œä¾†æºæ•¸é‡çš„ç°¡å–®ä¿¡å¿ƒåº¦è¨ˆç®—
        base_confidence = min(1.0, len(answer) / 100.0)
        source_confidence = min(1.0, len(sources) / 5.0)
        
        return (base_confidence + source_confidence) / 2.0
    
    def _calculate_factuality(self, answer: str, sources: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—äº‹å¯¦æ€§"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—") or answer.startswith("è©•ä¼°å¤±æ•—"):
            return 0.0
        
        # åŸºæ–¼ä¾†æºæ•¸é‡çš„ç°¡å–®äº‹å¯¦æ€§è¨ˆç®—
        return min(1.0, len(sources) / 3.0)
    
    def _calculate_relevance(self, answer: str, question: str) -> float:
        """è¨ˆç®—ç›¸é—œæ€§"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—") or answer.startswith("è©•ä¼°å¤±æ•—"):
            return 0.0
        
        # ç°¡å–®çš„é—œéµè©åŒ¹é…
        question_words = set(question.lower().split())
        answer_words = set(answer.lower().split())
        
        if not question_words:
            return 0.0
        
        overlap = len(question_words.intersection(answer_words))
        return min(1.0, overlap / len(question_words))
    
    def _calculate_coherence(self, answer: str) -> float:
        """è¨ˆç®—é€£è²«æ€§"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—") or answer.startswith("è©•ä¼°å¤±æ•—"):
            return 0.0
        
        # åŸºæ–¼å¥å­æ•¸é‡çš„ç°¡å–®é€£è²«æ€§è¨ˆç®—
        sentences = answer.split('ã€‚')
        if len(sentences) <= 1:
            return 0.5
        
        return min(1.0, len(sentences) / 5.0)
    
    async def evaluate_batch(self, questions: List[str]) -> List[ComparisonResult]:
        """æ‰¹é‡è©•ä¼°å•é¡Œ"""
        results = []
        for question in questions:
            result = await self.evaluate_single_question(question)
            results.append(result)
            logger.info(f"âœ… å®Œæˆè©•ä¼°: {question[:30]}...")
        return results
    
    def generate_comparison_report(self, results: List[ComparisonResult], output_path: str = None) -> str:
        """ç”Ÿæˆå°æ¯”å ±å‘Š"""
        try:
            # æº–å‚™æ•¸æ“š
            data = []
            for result in results:
                row = {
                    "å•é¡Œ": result.question,
                    "æœ¬åœ°LLMç­”æ¡ˆ": result.local_llm_result.answer,
                    "OpenAIç­”æ¡ˆ": result.openai_result.answer,
                    "æœ¬åœ°ä¿¡å¿ƒåº¦": result.local_llm_result.confidence,
                    "OpenAIä¿¡å¿ƒåº¦": result.openai_result.confidence,
                    "æœ¬åœ°äº‹å¯¦æ€§": result.local_llm_result.factuality,
                    "OpenAIäº‹å¯¦æ€§": result.openai_result.factuality,
                    "æœ¬åœ°ç›¸é—œæ€§": result.local_llm_result.relevance,
                    "OpenAIç›¸é—œæ€§": result.openai_result.relevance,
                    "æœ¬åœ°é€£è²«æ€§": result.local_llm_result.coherence,
                    "OpenAIé€£è²«æ€§": result.openai_result.coherence,
                    "æœ¬åœ°æ¨ç†æ™‚é–“(ç§’)": result.local_llm_result.inference_time,
                    "OpenAIæ¨ç†æ™‚é–“(ç§’)": result.openai_result.inference_time,
                    "æœ¬åœ°Tokenæ•¸": result.local_llm_result.token_count,
                    "OpenAI Tokenæ•¸": result.openai_result.token_count,
                    "ä¿¡å¿ƒåº¦å·®ç•°": result.comparison_metrics.get("confidence_diff", 0),
                    "äº‹å¯¦æ€§å·®ç•°": result.comparison_metrics.get("factuality_diff", 0),
                    "é€Ÿåº¦æ¯”": result.comparison_metrics.get("speed_ratio", 0),
                    "Tokenæ¯”": result.comparison_metrics.get("token_ratio", 0)
                }
                data.append(row)
            
            # å‰µå»º DataFrame
            df = pd.DataFrame(data)
            
            # ç”Ÿæˆå ±å‘Š
            report = f"""# RAG è©•ä¼°å°æ¯”å ±å‘Š

## è©•ä¼°æ¦‚è¦½
- è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å•é¡Œæ•¸é‡: {len(results)}
- æª¢ç´¢æ–¹æ³•: Milvus (IVF_FLAT ç´¢å¼•)
- æœ¬åœ° LLM: Qwen2.5-Taiwan-7B-Instruct
- OpenAI LLM: GPT-3.5-Turbo

## è©³ç´°å°æ¯”çµæœ

{df.to_string(index=False)}

## çµ±è¨ˆæ‘˜è¦

### å¹³å‡æŒ‡æ¨™
- æœ¬åœ° LLM å¹³å‡ä¿¡å¿ƒåº¦: {df['æœ¬åœ°ä¿¡å¿ƒåº¦'].mean():.3f}
- OpenAI å¹³å‡ä¿¡å¿ƒåº¦: {df['OpenAIä¿¡å¿ƒåº¦'].mean():.3f}
- æœ¬åœ° LLM å¹³å‡äº‹å¯¦æ€§: {df['æœ¬åœ°äº‹å¯¦æ€§'].mean():.3f}
- OpenAI å¹³å‡äº‹å¯¦æ€§: {df['OpenAIäº‹å¯¦æ€§'].mean():.3f}
- æœ¬åœ° LLM å¹³å‡æ¨ç†æ™‚é–“: {df['æœ¬åœ°æ¨ç†æ™‚é–“(ç§’)'].mean():.3f} ç§’
- OpenAI å¹³å‡æ¨ç†æ™‚é–“: {df['OpenAIæ¨ç†æ™‚é–“(ç§’)'].mean():.3f} ç§’

### æ€§èƒ½å°æ¯”
- å¹³å‡é€Ÿåº¦æ¯” (æœ¬åœ°/OpenAI): {df['é€Ÿåº¦æ¯”'].mean():.3f}
- å¹³å‡ Token æ¯” (æœ¬åœ°/OpenAI): {df['Tokenæ¯”'].mean():.3f}
- å¹³å‡ä¿¡å¿ƒåº¦å·®ç•° (æœ¬åœ°-OpenAI): {df['ä¿¡å¿ƒåº¦å·®ç•°'].mean():.3f}
- å¹³å‡äº‹å¯¦æ€§å·®ç•° (æœ¬åœ°-OpenAI): {df['äº‹å¯¦æ€§å·®ç•°'].mean():.3f}

## çµè«–
1. æœ¬åœ° LLM åœ¨æ¨ç†æ™‚é–“ä¸Š {'å„ªæ–¼' if df['é€Ÿåº¦æ¯”'].mean() < 1 else 'åŠ£æ–¼'} OpenAI
2. æœ¬åœ° LLM åœ¨ä¿¡å¿ƒåº¦ä¸Š {'å„ªæ–¼' if df['ä¿¡å¿ƒåº¦å·®ç•°'].mean() > 0 else 'åŠ£æ–¼'} OpenAI
3. æœ¬åœ° LLM åœ¨äº‹å¯¦æ€§ä¸Š {'å„ªæ–¼' if df['äº‹å¯¦æ€§å·®ç•°'].mean() > 0 else 'åŠ£æ–¼'} OpenAI
"""
            
            # ä¿å­˜å ±å‘Š
            if output_path:
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                
                # ä¿å­˜ CSV
                csv_path = output_path.replace('.txt', '.csv')
                df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                
                logger.info(f"âœ… å ±å‘Šå·²ä¿å­˜: {output_path}")
                logger.info(f"âœ… CSV å·²ä¿å­˜: {csv_path}")
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå ±å‘Šå¤±æ•—: {e}")
            return f"å ±å‘Šç”Ÿæˆå¤±æ•—: {str(e)}"
    
    # åŸºç¤ RAG è©•ä¼°æ–¹æ³•ï¼ˆæ•´åˆè‡ª rag_evaluator.pyï¼‰
    def baseline_evaluation(self, eval_dataset: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Baseline è©•ä¼°ï¼ˆç„¡ RAGï¼‰"""
        results = []
        
        for item in eval_dataset:
            question = item["input"]
            expected = item["expected"]
            
            # ä½¿ç”¨ OpenAI æœå‹™ç”Ÿæˆå›ç­”
            answer, inference_time, token_count = asyncio.run(
                self.openai_service.generate_answer(question, "")
            )
            
            # è¨ˆç®—ç°¡å–®çš„ç›¸ä¼¼åº¦æŒ‡æ¨™
            similarity = self._calculate_simple_similarity(answer, expected)
            
            result = {
                "method": "Baseline",
                "question": question,
                "expected": expected,
                "answer": answer,
                "similarity_score": similarity,
                "inference_time": inference_time,
                "token_count": token_count
            }
            results.append(result)
        
        return results
    
    def naive_rag_evaluation(self, eval_dataset: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Naive RAG è©•ä¼°"""
        results = []
        
        for item in eval_dataset:
            question = item["input"]
            expected = item["expected"]
            context = item["metadata"].get("reference", "")
            
            # ä½¿ç”¨ OpenAI æœå‹™ç”Ÿæˆå›ç­”
            answer, inference_time, token_count = asyncio.run(
                self.openai_service.generate_answer(question, context)
            )
            
            # è¨ˆç®—æŒ‡æ¨™
            similarity = self._calculate_simple_similarity(answer, expected)
            relevance = self._calculate_relevance(answer, question)
            
            result = {
                "method": "Naive RAG",
                "question": question,
                "expected": expected,
                "answer": answer,
                "context": context,
                "similarity_score": similarity,
                "relevance_score": relevance,
                "inference_time": inference_time,
                "token_count": token_count
            }
            results.append(result)
        
        return results
    
    def _calculate_simple_similarity(self, answer: str, expected: str) -> float:
        """è¨ˆç®—ç°¡å–®ç›¸ä¼¼åº¦"""
        if not answer or not expected:
            return 0.0
        
        # ç°¡å–®çš„è©å½™é‡ç–Šè¨ˆç®—
        answer_words = set(answer.lower().split())
        expected_words = set(expected.lower().split())
        
        if not expected_words:
            return 0.0
        
        overlap = len(answer_words.intersection(expected_words))
        return overlap / len(expected_words)
    
    # Podwise å°ˆç”¨åŠŸèƒ½ï¼ˆæ•´åˆè‡ª podwise_rag_evaluation.pyï¼‰
    def create_podwise_synthetic_dataset(self, num_questions: int = 5) -> list:
        """ç‚º Podwise è³‡æ–™å‰µå»ºåˆæˆå•ç­”å°"""
        try:
            # ç²å–çµ±è¨ˆè³‡è¨Š
            stats = self.podwise_search.get_statistics()
            logger.info(f"ğŸ“Š å‘é‡è³‡æ–™åº«çµ±è¨ˆ:")
            logger.info(f"   - ç¸½ chunks: {stats['total_chunks']}")
            logger.info(f"   - æ’­å®¢æ•¸é‡: {stats['total_podcasts']}")
            logger.info(f"   - ç¯€ç›®æ•¸é‡: {stats['total_episodes']}")
            logger.info(f"   - åˆ†é¡: {stats['categories']}")
            
            # å¾å‘é‡è³‡æ–™åº«ä¸­éš¨æ©Ÿé¸æ“‡ä¸€äº› chunks ä½œç‚ºå…§å®¹
            if stats['total_chunks'] > 0:
                # éš¨æ©Ÿé¸æ“‡ä¸€äº› chunks
                import random
                sample_size = min(10, stats['total_chunks'])
                sample_indices = random.sample(range(stats['total_chunks']), sample_size)
                
                # çµ„åˆå…§å®¹
                combined_content = ""
                for idx in sample_indices:
                    chunk_text = self.podwise_search.chunk_texts[idx]
                    metadata = self.podwise_search.metadata[idx]
                    combined_content += f"\n\næ’­å®¢: {metadata['podcast_name']}\n"
                    combined_content += f"ç¯€ç›®: {metadata['episode_title']}\n"
                    combined_content += f"å…§å®¹: {chunk_text}\n"
                
                # ç”Ÿæˆå•ç­”å°
                qa_pairs = self._produce_questions(combined_content, num_questions)
                
                # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
                result = []
                for pair in qa_pairs:
                    if hasattr(pair, 'model_dump'):
                        qa_data = pair.model_dump()
                    else:
                        qa_data = {
                            'reference': pair.reference,
                            'question': pair.question,
                            'answer': pair.answer
                        }
                    result.append(qa_data)
                
                return result
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ å‰µå»º Podwise åˆæˆè³‡æ–™é›†å¤±æ•—: {str(e)}")
            return []
    
    def _produce_questions(self, content: str, num_questions: int = 2) -> List[Any]:
        """ç”¢ç”Ÿå•é¡Œï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            from pydantic import Field, BaseModel
            from typing import List
            
            class QAPair(BaseModel):
                reference: str = Field(..., description="åŸå§‹æ–‡æœ¬æ®µè½")
                question: str = Field(..., description="å•é¡Œ")
                answer: str = Field(..., description="ç­”æ¡ˆ")
            
            # ç°¡åŒ–çš„å•é¡Œç”Ÿæˆé‚è¼¯
            questions = [
                "æˆ‘æƒ³å­¸ç¿’æŠ•è³‡ç†è²¡ï¼Œæœ‰ä»€éº¼æ¨è–¦çš„ Podcast å—ï¼Ÿ",
                "é€šå‹¤æ™‚é–“æƒ³è½ä¸€äº›è¼•é¬†çš„å…§å®¹",
                "æœ‰ä»€éº¼é—œæ–¼å‰µæ¥­çš„ç¯€ç›®æ¨è–¦å—ï¼Ÿ",
                "å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ",
                "æ¨è–¦ä¸€äº›å•†æ¥­ç›¸é—œçš„ Podcast"
            ]
            
            # é¸æ“‡æŒ‡å®šæ•¸é‡çš„å•é¡Œ
            selected_questions = questions[:num_questions]
            
            # ç”Ÿæˆå•ç­”å°
            pairs = []
            for question in selected_questions:
                # ç°¡å–®çš„å›ç­”ç”Ÿæˆ
                answer = f"åŸºæ–¼æä¾›çš„å…§å®¹ï¼Œæˆ‘ç‚ºæ‚¨æ¨è–¦ç›¸é—œçš„ Podcast ç¯€ç›®ã€‚{content[:100]}..."
                
                pair = QAPair(
                    reference=content[:200] + "...",
                    question=question,
                    answer=answer
                )
                pairs.append(pair)
            
            return pairs
            
        except Exception as e:
            logger.error(f"ç”¢ç”Ÿå•é¡Œå¤±æ•—: {e}")
            return []

def get_unified_evaluator() -> UnifiedRAGEvaluator:
    """ç²å–çµ±ä¸€è©•ä¼°å™¨å¯¦ä¾‹"""
    return UnifiedRAGEvaluator(use_mock_services=True)

async def test_unified_evaluator():
    """æ¸¬è©¦çµ±ä¸€è©•ä¼°å™¨"""
    print("=== æ¸¬è©¦çµ±ä¸€ RAG è©•ä¼°å™¨ ===")
    
    # åˆå§‹åŒ–è©•ä¼°å™¨
    evaluator = get_unified_evaluator()
    
    # æ¸¬è©¦å•é¡Œ
    test_questions = [
        "æˆ‘æƒ³å­¸ç¿’æŠ•è³‡ç†è²¡ï¼Œæœ‰ä»€éº¼æ¨è–¦çš„ Podcast å—ï¼Ÿ",
        "é€šå‹¤æ™‚é–“æƒ³è½ä¸€äº›è¼•é¬†çš„å…§å®¹",
        "æœ‰ä»€éº¼é—œæ–¼å‰µæ¥­çš„ç¯€ç›®æ¨è–¦å—ï¼Ÿ"
    ]
    
    # åŸ·è¡Œè©•ä¼°
    results = await evaluator.evaluate_batch(test_questions)
    
    # ç”Ÿæˆå ±å‘Š
    report = evaluator.generate_comparison_report(
        results, 
        "unified_rag_evaluation_report.txt"
    )
    
    print("âœ… è©•ä¼°å®Œæˆ")
    print(report)
    
    return results

if __name__ == "__main__":
    asyncio.run(test_unified_evaluator()) 