#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆ RAG è©•ä¼°å™¨

åŠŸèƒ½ç‰¹è‰²ï¼š
1. å°ˆç”¨ Milvus æª¢ç´¢ï¼ˆæ”¯æ´ IVF_FLAT ç´¢å¼•å„ªåŒ–ï¼‰
2. æœ¬åœ° Qwen2.5-Taiwan-7B-Instruct èˆ‡ OpenAI å°æ¯”
3. å®Œæ•´è©•ä¼°æŒ‡æ¨™ï¼ˆä¿¡å¿ƒåº¦ã€äº‹å¯¦æ€§ã€æ¨ç†æ™‚é–“ã€token æ•¸ï¼‰
4. è¡¨æ ¼æ ¼å¼å°æ¯”å ±å‘Š

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 2.0.0
"""

import os
import sys
import json
import time
import logging
import asyncio
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from datetime import datetime
import hashlib

# æ·»åŠ è·¯å¾‘ä»¥ä¾¿å°å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

try:
    from config.integrated_config import get_config
except ImportError:
    # ç°¡åŒ–é…ç½®
    def get_config():
        class Config:
            class Database:
                milvus_host = os.getenv("MILVUS_HOST", "worker3")
                milvus_port = int(os.getenv("MILVUS_PORT", "19530"))
                milvus_collection = os.getenv("MILVUS_COLLECTION", "podcast_chunks")
            class API:
                openai_api_key = os.getenv("OPENAI_API_KEY", "")
            database = Database()
            api = API()
        return Config()

logger = logging.getLogger(__name__)


@dataclass
class EvaluationResult:
    """è©•ä¼°çµæœæ•¸æ“šé¡åˆ¥"""
    question: str
    answer: str
    confidence: float
    factuality: float
    relevance: float
    coherence: float
    inference_time: float
    token_count: int
    model_name: str
    retrieval_method: str
    sources: List[Dict[str, Any]]


@dataclass
class ComparisonResult:
    """å°æ¯”çµæœæ•¸æ“šé¡åˆ¥"""
    question: str
    local_llm_result: EvaluationResult
    openai_result: EvaluationResult
    comparison_metrics: Dict[str, float]


class MilvusOptimizedSearch:
    """å„ªåŒ–çš„ Milvus æœå°‹æœå‹™"""
    
    def __init__(self):
        """åˆå§‹åŒ–å„ªåŒ–çš„ Milvus æœå°‹"""
        self.config = get_config()
        self.collection = None
        self.is_connected = False
        self._connect()
        
    def _connect(self):
        """é€£æ¥åˆ° Milvus"""
        try:
            from pymilvus import connections, Collection, utility
            
            # é€£æ¥åˆ° Milvus
            connections.connect(
                alias="default",
                host=self.config.database.milvus_host,
                port=self.config.database.milvus_port
            )
            
            # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collection_name = self.config.database.milvus_collection
            if utility.has_collection(collection_name):
                self.collection = Collection(collection_name)
                self.is_connected = True
                
                # æª¢æŸ¥é›†åˆçµ±è¨ˆè³‡è¨Š
                stats = self.collection.num_entities
                logger.info(f"âœ… Milvus é›†åˆ '{collection_name}' é€£æ¥æˆåŠŸï¼Œå‘é‡æ•¸é‡: {stats}")
                
                # æ ¹æ“šå‘é‡æ•¸é‡å»ºè­°ç´¢å¼•é¡å‹
                if stats < 1000000:
                    logger.info("ğŸ“Š å»ºè­°ä½¿ç”¨ IVF_FLAT ç´¢å¼•ï¼ˆå‘é‡æ•¸ < 1Mï¼‰")
                else:
                    logger.info("ğŸ“Š å»ºè­°ä½¿ç”¨ HNSW æˆ– DiskANN ç´¢å¼•ï¼ˆå‘é‡æ•¸ > 1Mï¼‰")
                    
            else:
                logger.warning(f"âš ï¸ Milvus é›†åˆ '{collection_name}' ä¸å­˜åœ¨")
                
        except ImportError:
            logger.warning("âš ï¸ pymilvus æœªå®‰è£")
        except Exception as e:
            logger.error(f"âŒ Milvus é€£æ¥å¤±æ•—: {e}")
    
    async def search(self, query: str, top_k: int = 10, nprobe: int = 16) -> List[Dict[str, Any]]:
        """
        åŸ·è¡Œå„ªåŒ–çš„å‘é‡æœå°‹
        
        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›çµæœæ•¸é‡
            nprobe: æœå°‹æ¢é‡æ•¸é‡
            
        Returns:
            List[Dict[str, Any]]: æœå°‹çµæœ
        """
        try:
            if not self.is_connected or self.collection is None:
                logger.warning("Milvus æœªé€£æ¥ï¼Œè¿”å›ç©ºçµæœ")
                return []
            
            # æ–‡æœ¬å‘é‡åŒ–ï¼ˆä½¿ç”¨ç°¡å–®çš„å“ˆå¸Œæ–¹æ³•ä½œç‚ºç¤ºä¾‹ï¼‰
            embedding = self._text_to_vector(query)
            if not embedding:
                return []
            
            # è¼‰å…¥é›†åˆ
            self.collection.load()
            
            # å„ªåŒ–çš„æœå°‹åƒæ•¸
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": nprobe}
            }
            
            # åŸ·è¡Œæœå°‹
            results = self.collection.search(
                data=[embedding],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["chunk_id", "chunk_text", "tags", "podcast_name", "episode_title", "category", "language"]
            )
            
            # æ ¼å¼åŒ–çµæœ
            formatted_results = []
            for hits in results:
                for hit in hits:
                    # è§£æ tags æ¬„ä½
                    tags = []
                    try:
                        if hasattr(hit, 'entity') and hit.entity:
                            tags_data = hit.entity.get("tags")
                            if tags_data:
                                if isinstance(tags_data, str):
                                    tags = json.loads(tags_data)
                                else:
                                    tags = tags_data
                    except:
                        tags = []
                    
                    # å®‰å…¨ç²å–å¯¦é«”æ•¸æ“š
                    entity_data = {}
                    chunk_text = ""
                    if hasattr(hit, 'entity') and hit.entity:
                        try:
                            entity_data = {
                                "podcast_name": hit.entity.get("podcast_name") or "",
                                "episode_title": hit.entity.get("episode_title") or "",
                                "category": hit.entity.get("category") or "",
                                "language": hit.entity.get("language") or "",
                                "chunk_id": hit.entity.get("chunk_id") or ""
                            }
                            chunk_text = hit.entity.get("chunk_text") or ""
                        except Exception as e:
                            logger.warning(f"ç²å–å¯¦é«”æ•¸æ“šå¤±æ•—: {e}")
                            # ä½¿ç”¨å±¬æ€§è¨ªå•
                            try:
                                entity_data = {
                                    "podcast_name": getattr(hit.entity, "podcast_name", ""),
                                    "episode_title": getattr(hit.entity, "episode_title", ""),
                                    "category": getattr(hit.entity, "category", ""),
                                    "language": getattr(hit.entity, "language", ""),
                                    "chunk_id": getattr(hit.entity, "chunk_id", "")
                                }
                                chunk_text = getattr(hit.entity, "chunk_text", "")
                            except Exception as e2:
                                logger.warning(f"å±¬æ€§è¨ªå•ä¹Ÿå¤±æ•—: {e2}")
                    
                    formatted_results.append({
                        "content": chunk_text,
                        "similarity_score": float(hit.score),
                        "metadata": entity_data,
                        "tags": tags,
                        "source": "milvus"
                    })
            
            logger.info(f"âœ… Milvus æœå°‹æˆåŠŸï¼Œè¿”å› {len(formatted_results)} å€‹çµæœ")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ Milvus æœå°‹å¤±æ•—: {e}")
            return []
    
    def _text_to_vector(self, text: str) -> Optional[List[float]]:
        """æ–‡æœ¬å‘é‡åŒ–ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            # ä½¿ç”¨ MD5 å“ˆå¸Œç”Ÿæˆ 1024 ç¶­å‘é‡
            hash_obj = hashlib.md5(text.encode())
            hash_hex = hash_obj.hexdigest()
            
            # æ“´å±•åˆ° 1024 ç¶­
            vector = []
            for i in range(1024):
                # å¾ªç’°ä½¿ç”¨å“ˆå¸Œå€¼
                idx = i % len(hash_hex)
                vector.append(float(int(hash_hex[idx:idx+2], 16)) / 255.0)
            
            return vector
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å‘é‡åŒ–å¤±æ•—: {e}")
            return None


class LocalLLMService:
    """æœ¬åœ° LLM æœå‹™ï¼ˆQwen2.5-Taiwan-7B-Instructï¼‰"""
    
    def __init__(self, model_path: str = "/home/bai/Desktop/Podwise/Qwen2.5-Taiwan-7B-Instruct"):
        """åˆå§‹åŒ–æœ¬åœ° LLM æœå‹™"""
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        self.prompt_templates_available = False
        self._load_model()
        self._load_prompt_templates()
    
    def _load_prompt_templates(self):
        """è¼‰å…¥æç¤ºè©æ¨¡æ¿"""
        try:
            # å˜—è©¦å¤šç¨®å°å…¥è·¯å¾‘
            try:
                from config.prompt_templates import get_prompt_template, format_prompt
            except ImportError:
                try:
                    from ..config.prompt_templates import get_prompt_template, format_prompt
                except ImportError:
                    # ä½¿ç”¨çµ•å°è·¯å¾‘
                    import sys
                    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
                    from config.prompt_templates import get_prompt_template, format_prompt
            
            self.get_prompt_template = get_prompt_template
            self.format_prompt = format_prompt
            self.prompt_templates_available = True
            logger.info("âœ… æœ¬åœ° LLM æç¤ºè©æ¨¡æ¿è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥æç¤ºè©æ¨¡æ¿: {e}")
            self.prompt_templates_available = False
    
    def _load_model(self):
        """è¼‰å…¥æœ¬åœ°æ¨¡å‹"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            if os.path.exists(self.model_path):
                # ä½¿ç”¨æ­£ç¢ºçš„ tokenizer é¡åˆ¥
                try:
                    # å˜—è©¦ä½¿ç”¨ AutoTokenizer
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        self.model_path,
                        trust_remote_code=True
                    )
                except Exception as e:
                    logger.warning(f"AutoTokenizer è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨é è¨­ tokenizer: {e}")
                    # å¦‚æœå¤±æ•—ï¼Œä½¿ç”¨é è¨­çš„ tokenizer
                    from transformers import PreTrainedTokenizer
                    self.tokenizer = PreTrainedTokenizer.from_pretrained(
                        self.model_path,
                        trust_remote_code=True
                    )
                
                # è¨­ç½® pad token
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # è¼‰å…¥æ¨¡å‹
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
                self.is_loaded = True
                logger.info(f"âœ… æœ¬åœ° LLM è¼‰å…¥æˆåŠŸ: {self.model_path}")
            else:
                logger.warning(f"âš ï¸ æœ¬åœ°æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {self.model_path}")
                
        except ImportError:
            logger.warning("âš ï¸ transformers æœªå®‰è£")
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ° LLM è¼‰å…¥å¤±æ•—: {e}")
            # æä¾›æ›´è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯
            import traceback
            logger.error(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
    
    async def generate_answer(self, prompt: str, context: str = "") -> Tuple[str, float, int]:
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ prompt_templates.pyï¼‰
        
        Args:
            prompt: å•é¡Œ
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            Tuple[str, float, int]: (ç­”æ¡ˆ, æ¨ç†æ™‚é–“, token æ•¸)
        """
        start_time = time.time()
        
        try:
            if not self.is_loaded or self.model is None or self.tokenizer is None:
                return "æœ¬åœ° LLM æœªè¼‰å…¥", 0.0, 0
            
            # æ§‹å»ºæç¤ºè©
            if self.prompt_templates_available:
                try:
                    # ä½¿ç”¨æ­£å¼çš„æç¤ºè©æ¨¡æ¿
                    answer_template = self.get_prompt_template("answer_generation")
                    
                    # æ§‹å»ºæ¨¡æ“¬çš„é ˜å°è€…æ±ºç­–çµæœ
                    mock_leader_decision = {
                        "top_recommendations": [
                            {
                                "title": "æœ¬åœ° LLM æ¨è–¦ç¯€ç›®",
                                "category": "å•†æ¥­",
                                "confidence": 0.88,
                                "source": "æœ¬åœ° LLM åˆ†æ",
                                "episode": "ç¬¬ 2 é›†",
                                "rss_id": "RSS004"
                            }
                        ],
                        "explanation": "åŸºæ–¼æ‚¨çš„å•é¡Œï¼Œæˆ‘å€‘æ‰¾åˆ°äº†ç›¸é—œçš„å•†æ¥­é¡ Podcast ç¯€ç›®",
                        "recommendation_count": 1,
                        "categories_included": ["å•†æ¥­"]
                    }
                    
                    # æ ¼å¼åŒ–æç¤ºè©
                    formatted_prompt = self.format_prompt(
                        answer_template,
                        leader_decision=json.dumps(mock_leader_decision, ensure_ascii=False),
                        user_question=prompt,
                        user_context={"context": context}
                    )
                    
                    # æ·»åŠ ç³»çµ±æç¤º
                    full_prompt = f"""ä½ æ˜¯ Podriï¼Œä¸€ä½å°ˆæ¥­çš„ Podcast æ¨è–¦åŠ©æ‰‹ã€‚

{formatted_prompt}

è«‹åŸºæ–¼ä»¥ä¸Šè³‡è¨Šç”Ÿæˆå›ç­”ï¼š"""
                    
                except Exception as e:
                    logger.warning(f"æç¤ºè©æ¨¡æ¿æ ¼å¼åŒ–å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æç¤º: {e}")
                    full_prompt = f"""
åŸºæ–¼ä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œï¼š

è³‡è¨Šï¼š
{context}

å•é¡Œï¼š{prompt}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
"""
            else:
                # å‚™ç”¨æç¤ºè©
                full_prompt = f"""
åŸºæ–¼ä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œï¼š

è³‡è¨Šï¼š
{context}

å•é¡Œï¼š{prompt}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
"""
            
            # ç·¨ç¢¼è¼¸å…¥
            inputs = self.tokenizer(full_prompt, return_tensors="pt")
            input_ids = inputs["input_ids"]
            
            # ç”Ÿæˆç­”æ¡ˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç¢¼ç­”æ¡ˆ
            answer = self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
            
            # è¨ˆç®— token æ•¸
            token_count = len(outputs[0])
            
            # è¨ˆç®—æ¨ç†æ™‚é–“
            inference_time = time.time() - start_time
            
            return answer.strip(), inference_time, token_count
            
        except Exception as e:
            logger.error(f"æœ¬åœ° LLM ç”Ÿæˆå¤±æ•—: {e}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}", time.time() - start_time, 0


class OpenAIService:
    """OpenAI æœå‹™ï¼ˆæ•´åˆ prompt_templates.pyï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ– OpenAI æœå‹™"""
        self.config = get_config()
        self.api_key = self.config.api.openai_api_key
        self.prompt_templates_available = False
        self._load_prompt_templates()
    
    def _load_prompt_templates(self):
        """è¼‰å…¥æç¤ºè©æ¨¡æ¿"""
        try:
            # å˜—è©¦å¤šç¨®å°å…¥è·¯å¾‘
            try:
                from config.prompt_templates import get_prompt_template, format_prompt
            except ImportError:
                try:
                    from ..config.prompt_templates import get_prompt_template, format_prompt
                except ImportError:
                    # ä½¿ç”¨çµ•å°è·¯å¾‘
                    import sys
                    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
                    from config.prompt_templates import get_prompt_template, format_prompt
            
            self.get_prompt_template = get_prompt_template
            self.format_prompt = format_prompt
            self.prompt_templates_available = True
            logger.info("âœ… OpenAI æç¤ºè©æ¨¡æ¿è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥æç¤ºè©æ¨¡æ¿: {e}")
            self.prompt_templates_available = False
        
    async def generate_answer(self, prompt: str, context: str = "") -> Tuple[str, float, int]:
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ prompt_templates.pyï¼‰
        
        Args:
            prompt: å•é¡Œ
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            Tuple[str, float, int]: (ç­”æ¡ˆ, æ¨ç†æ™‚é–“, token æ•¸)
        """
        start_time = time.time()
        
        try:
            if not self.api_key:
                return "OpenAI API Key æœªé…ç½®", 0.0, 0
            
            from openai import OpenAI
            
            # å‰µå»ºå®¢æˆ¶ç«¯
            client = OpenAI(api_key=self.api_key)
            
            # æ§‹å»ºæç¤ºè©
            if self.prompt_templates_available:
                try:
                    # ä½¿ç”¨æ­£å¼çš„æç¤ºè©æ¨¡æ¿
                    answer_template = self.get_prompt_template("answer_generation")
                    
                    # æ§‹å»ºæ¨¡æ“¬çš„é ˜å°è€…æ±ºç­–çµæœ
                    mock_leader_decision = {
                        "top_recommendations": [
                            {
                                "title": "OpenAI æ¨è–¦ç¯€ç›®",
                                "category": "æ•™è‚²",
                                "confidence": 0.92,
                                "source": "OpenAI åˆ†æ",
                                "episode": "ç¬¬ 5 é›†",
                                "rss_id": "RSS005"
                            },
                            {
                                "title": "é€²éšå­¸ç¿’ç¯€ç›®",
                                "category": "æ•™è‚²",
                                "confidence": 0.89,
                                "source": "OpenAI åˆ†æ",
                                "episode": "ç¬¬ 3 é›†",
                                "rss_id": "RSS006"
                            }
                        ],
                        "explanation": "åŸºæ–¼æ‚¨çš„å•é¡Œï¼Œæˆ‘å€‘æ‰¾åˆ°äº†å¤šå€‹ç›¸é—œçš„æ•™è‚²é¡ Podcast ç¯€ç›®",
                        "recommendation_count": 2,
                        "categories_included": ["æ•™è‚²"]
                    }
                    
                    # æ ¼å¼åŒ–æç¤ºè©
                    formatted_prompt = self.format_prompt(
                        answer_template,
                        leader_decision=json.dumps(mock_leader_decision, ensure_ascii=False),
                        user_question=prompt,
                        user_context={"context": context}
                    )
                    
                    # æ§‹å»ºç³»çµ±æç¤º
                    system_prompt = "ä½ æ˜¯ Podriï¼Œä¸€ä½å°ˆæ¥­çš„ Podcast æ¨è–¦åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„è³‡è¨Šç”Ÿæˆå‹å–„ã€å°ˆæ¥­çš„å›ç­”ã€‚"
                    user_prompt = formatted_prompt
                    
                except Exception as e:
                    logger.warning(f"æç¤ºè©æ¨¡æ¿æ ¼å¼åŒ–å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æç¤º: {e}")
                    system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
                    user_prompt = f"""
åŸºæ–¼ä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œï¼š

è³‡è¨Šï¼š
{context}

å•é¡Œï¼š{prompt}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
"""
            else:
                # å‚™ç”¨æç¤ºè©
                system_prompt = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚"
                user_prompt = f"""
åŸºæ–¼ä»¥ä¸‹è³‡è¨Šå›ç­”å•é¡Œï¼š

è³‡è¨Šï¼š
{context}

å•é¡Œï¼š{prompt}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
"""
            
            # ç™¼é€è«‹æ±‚
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=512,
                temperature=0.7
            )
            
            answer = response.choices[0].message.content
            token_count = response.usage.total_tokens
            inference_time = time.time() - start_time
            
            return answer, inference_time, token_count
            
        except Exception as e:
            logger.error(f"OpenAI ç”Ÿæˆå¤±æ•—: {e}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}", time.time() - start_time, 0


class MockLocalLLMService:
    """æ¨¡æ“¬æœ¬åœ° LLM æœå‹™ï¼ˆç”¨æ–¼æ¸¬è©¦ prompt_templates.py æ•´åˆï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡æ“¬æœ¬åœ° LLM æœå‹™"""
        self.prompt_templates_available = False
        self._load_prompt_templates()
    
    def _load_prompt_templates(self):
        """è¼‰å…¥æç¤ºè©æ¨¡æ¿"""
        try:
            # ä½¿ç”¨çµ•å°è·¯å¾‘
            import sys
            import os
            
            # ç²å–ç•¶å‰æ–‡ä»¶çš„è·¯å¾‘
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # å‘ä¸Šå…©ç´šåˆ° backend ç›®éŒ„
            backend_dir = os.path.dirname(os.path.dirname(current_dir))
            # æ·»åŠ åˆ° Python è·¯å¾‘
            sys.path.insert(0, backend_dir)
            
            from rag_pipeline.config.prompt_templates import get_prompt_template, format_prompt
            
            self.get_prompt_template = get_prompt_template
            self.format_prompt = format_prompt
            self.prompt_templates_available = True
            logger.info("âœ… æ¨¡æ“¬æœ¬åœ° LLM æç¤ºè©æ¨¡æ¿è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥æç¤ºè©æ¨¡æ¿: {e}")
            self.prompt_templates_available = False
    
    async def generate_answer(self, prompt: str, context: str = "") -> Tuple[str, float, int]:
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ prompt_templates.pyï¼‰
        
        Args:
            prompt: å•é¡Œ
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            Tuple[str, float, int]: (ç­”æ¡ˆ, æ¨ç†æ™‚é–“, token æ•¸)
        """
        start_time = time.time()
        
        try:
            # æ§‹å»ºæç¤ºè©
            if self.prompt_templates_available:
                try:
                    # ä½¿ç”¨æ­£å¼çš„æç¤ºè©æ¨¡æ¿
                    answer_template = self.get_prompt_template("answer_generation")
                    
                    # æ§‹å»ºæ¨¡æ“¬çš„é ˜å°è€…æ±ºç­–çµæœ
                    mock_leader_decision = {
                        "top_recommendations": [
                            {
                                "title": "æ¨¡æ“¬æœ¬åœ° LLM æ¨è–¦ç¯€ç›®",
                                "category": "å•†æ¥­",
                                "confidence": 0.88,
                                "source": "æ¨¡æ“¬æœ¬åœ° LLM åˆ†æ",
                                "episode": "ç¬¬ 2 é›†",
                                "rss_id": "RSS004"
                            }
                        ],
                        "explanation": "åŸºæ–¼æ‚¨çš„å•é¡Œï¼Œæˆ‘å€‘æ‰¾åˆ°äº†ç›¸é—œçš„å•†æ¥­é¡ Podcast ç¯€ç›®",
                        "recommendation_count": 1,
                        "categories_included": ["å•†æ¥­"]
                    }
                    
                    # æ ¼å¼åŒ–æç¤ºè©
                    formatted_prompt = self.format_prompt(
                        answer_template,
                        leader_decision=json.dumps(mock_leader_decision, ensure_ascii=False),
                        user_question=prompt,
                        user_context={"context": context}
                    )
                    
                    # æ¨¡æ“¬ LLM å›æ‡‰ï¼ˆåŸºæ–¼æ¨¡æ¿æ ¼å¼ï¼‰
                    answer = f"""å—¨å—¨ğŸ‘‹ æƒ³äº†è§£ã€Œ{prompt}ã€å—ï¼Ÿä»¥ä¸‹æ˜¯ç›¸é—œçš„ç²¾å½©ç¯€ç›®ï¼š

ğŸ§ã€Šæ¨¡æ“¬æœ¬åœ° LLM æ¨è–¦ç¯€ç›®ã€‹ç¬¬ 2 é›†ï¼šã€ˆå•†æ¥­åˆ†æã€‰
ğŸ‘‰ é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„æœ¬åœ° LLM å›æ‡‰ï¼ŒåŸºæ–¼ prompt_templates.py çš„æ ¼å¼ç”Ÿæˆ
ğŸ“… ç™¼å¸ƒæ™‚é–“ï¼š2024å¹´

ğŸ’¡ å°æé†’ï¼šå¦‚æœæ‚¨å°å…¶ä»–é¡åˆ¥ä¹Ÿæ„Ÿèˆˆè¶£ï¼Œæˆ‘ä¹Ÿå¯ä»¥æ¨è–¦ä¸€äº›ç›¸é—œçš„ç¯€ç›®å–”ï¼

æœ‰èˆˆè¶£çš„è©±å¯ä»¥é»ä¾†è½è½ï¼Œè®“è€³æœµå’Œè…¦è¢‹éƒ½å……å¯¦ä¸€ä¸‹ ğŸ˜Š"""
                    
                except Exception as e:
                    logger.warning(f"æç¤ºè©æ¨¡æ¿æ ¼å¼åŒ–å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰: {e}")
                    answer = f"é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„æœ¬åœ° LLM å›æ‡‰ï¼Œå•é¡Œï¼š{prompt}ã€‚ä¸Šä¸‹æ–‡ï¼š{context[:100]}..."
            else:
                # å‚™ç”¨å›æ‡‰
                answer = f"é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„æœ¬åœ° LLM å›æ‡‰ï¼Œå•é¡Œï¼š{prompt}ã€‚ä¸Šä¸‹æ–‡ï¼š{context[:100]}..."
            
            # æ¨¡æ“¬æ¨ç†æ™‚é–“
            await asyncio.sleep(2.0)
            
            # è¨ˆç®—æ™‚é–“å’Œ token æ•¸
            inference_time = time.time() - start_time
            token_count = len(answer.split()) * 1.5  # ç²—ç•¥ä¼°ç®—
            
            return answer, inference_time, int(token_count)
            
        except Exception as e:
            logger.error(f"æ¨¡æ“¬æœ¬åœ° LLM ç”Ÿæˆå¤±æ•—: {e}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}", time.time() - start_time, 0


class MockOpenAIService:
    """æ¨¡æ“¬ OpenAI æœå‹™ï¼ˆç”¨æ–¼æ¸¬è©¦ prompt_templates.py æ•´åˆï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡æ“¬ OpenAI æœå‹™"""
        self.prompt_templates_available = False
        self._load_prompt_templates()
    
    def _load_prompt_templates(self):
        """è¼‰å…¥æç¤ºè©æ¨¡æ¿"""
        try:
            # ä½¿ç”¨çµ•å°è·¯å¾‘
            import sys
            import os
            
            # ç²å–ç•¶å‰æ–‡ä»¶çš„è·¯å¾‘
            current_dir = os.path.dirname(os.path.abspath(__file__))
            # å‘ä¸Šå…©ç´šåˆ° backend ç›®éŒ„
            backend_dir = os.path.dirname(os.path.dirname(current_dir))
            # æ·»åŠ åˆ° Python è·¯å¾‘
            sys.path.insert(0, backend_dir)
            
            from rag_pipeline.config.prompt_templates import get_prompt_template, format_prompt
            
            self.get_prompt_template = get_prompt_template
            self.format_prompt = format_prompt
            self.prompt_templates_available = True
            logger.info("âœ… æ¨¡æ“¬ OpenAI æç¤ºè©æ¨¡æ¿è¼‰å…¥æˆåŠŸ")
        except ImportError as e:
            logger.warning(f"âš ï¸ ç„¡æ³•è¼‰å…¥æç¤ºè©æ¨¡æ¿: {e}")
            self.prompt_templates_available = False
        
    async def generate_answer(self, prompt: str, context: str = "") -> Tuple[str, float, int]:
        """
        ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ prompt_templates.pyï¼‰
        
        Args:
            prompt: å•é¡Œ
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            Tuple[str, float, int]: (ç­”æ¡ˆ, æ¨ç†æ™‚é–“, token æ•¸)
        """
        start_time = time.time()
        
        try:
            # æ§‹å»ºæç¤ºè©
            if self.prompt_templates_available:
                try:
                    # ä½¿ç”¨æ­£å¼çš„æç¤ºè©æ¨¡æ¿
                    answer_template = self.get_prompt_template("answer_generation")
                    
                    # æ§‹å»ºæ¨¡æ“¬çš„é ˜å°è€…æ±ºç­–çµæœ
                    mock_leader_decision = {
                        "top_recommendations": [
                            {
                                "title": "æ¨¡æ“¬ OpenAI æ¨è–¦ç¯€ç›®",
                                "category": "æ•™è‚²",
                                "confidence": 0.92,
                                "source": "æ¨¡æ“¬ OpenAI åˆ†æ",
                                "episode": "ç¬¬ 5 é›†",
                                "rss_id": "RSS005"
                            },
                            {
                                "title": "é€²éšå­¸ç¿’ç¯€ç›®",
                                "category": "æ•™è‚²",
                                "confidence": 0.89,
                                "source": "æ¨¡æ“¬ OpenAI åˆ†æ",
                                "episode": "ç¬¬ 3 é›†",
                                "rss_id": "RSS006"
                            }
                        ],
                        "explanation": "åŸºæ–¼æ‚¨çš„å•é¡Œï¼Œæˆ‘å€‘æ‰¾åˆ°äº†å¤šå€‹ç›¸é—œçš„æ•™è‚²é¡ Podcast ç¯€ç›®",
                        "recommendation_count": 2,
                        "categories_included": ["æ•™è‚²"]
                    }
                    
                    # æ ¼å¼åŒ–æç¤ºè©
                    formatted_prompt = self.format_prompt(
                        answer_template,
                        leader_decision=json.dumps(mock_leader_decision, ensure_ascii=False),
                        user_question=prompt,
                        user_context={"context": context}
                    )
                    
                    # æ¨¡æ“¬ OpenAI å›æ‡‰ï¼ˆåŸºæ–¼æ¨¡æ¿æ ¼å¼ï¼‰
                    answer = f"""å—¨å—¨ğŸ‘‹ æƒ³äº†è§£ã€Œ{prompt}ã€å—ï¼Ÿä»¥ä¸‹æ˜¯ç›¸é—œçš„ç²¾å½©ç¯€ç›®ï¼š

ğŸ§ã€Šæ¨¡æ“¬ OpenAI æ¨è–¦ç¯€ç›®ã€‹ç¬¬ 5 é›†ï¼šã€ˆæ·±åº¦å­¸ç¿’ã€‰
ğŸ‘‰ é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„ OpenAI å›æ‡‰ï¼ŒåŸºæ–¼ prompt_templates.py çš„æ ¼å¼ç”Ÿæˆ
ğŸ“… ç™¼å¸ƒæ™‚é–“ï¼š2024å¹´

ğŸ§ã€Šé€²éšå­¸ç¿’ç¯€ç›®ã€‹ç¬¬ 3 é›†ï¼šã€ˆæŠ€èƒ½æå‡ã€‰
ğŸ‘‰ æä¾›å¯¦ç”¨çš„å­¸ç¿’æŠ€å·§å’Œè·æ¶¯ç™¼å±•å»ºè­°
ğŸ“… ç™¼å¸ƒæ™‚é–“ï¼š2024å¹´

ğŸ’¡ å°æé†’ï¼šé€™äº›ç¯€ç›®éƒ½ç¶“éç²¾å¿ƒæŒ‘é¸ï¼Œå¸Œæœ›èƒ½å¹«åŠ©æ‚¨æ›´å¥½åœ°å­¸ç¿’å’Œæˆé•·ï¼

æœ‰èˆˆè¶£çš„è©±å¯ä»¥é»ä¾†è½è½ï¼Œè®“è€³æœµå’Œè…¦è¢‹éƒ½å……å¯¦ä¸€ä¸‹ ğŸ˜Š"""
                    
                except Exception as e:
                    logger.warning(f"æç¤ºè©æ¨¡æ¿æ ¼å¼åŒ–å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨å›æ‡‰: {e}")
                    answer = f"é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„ OpenAI å›æ‡‰ï¼Œå•é¡Œï¼š{prompt}ã€‚ä¸Šä¸‹æ–‡ï¼š{context[:100]}..."
            else:
                # å‚™ç”¨å›æ‡‰
                answer = f"é€™æ˜¯ä¸€å€‹æ¨¡æ“¬çš„ OpenAI å›æ‡‰ï¼Œå•é¡Œï¼š{prompt}ã€‚ä¸Šä¸‹æ–‡ï¼š{context[:100]}..."
            
            # æ¨¡æ“¬æ¨ç†æ™‚é–“
            await asyncio.sleep(0.5)
            
            # è¨ˆç®—æ™‚é–“å’Œ token æ•¸
            inference_time = time.time() - start_time
            token_count = len(answer.split()) * 1.5  # ç²—ç•¥ä¼°ç®—
            
            return answer, inference_time, int(token_count)
            
        except Exception as e:
            logger.error(f"æ¨¡æ“¬ OpenAI ç”Ÿæˆå¤±æ•—: {e}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}", time.time() - start_time, 0


class EnhancedRAGEvaluator:
    """å¢å¼·ç‰ˆ RAG è©•ä¼°å™¨ï¼ˆæ•´åˆ prompt_templates.pyï¼‰"""
    
    def __init__(self, use_mock_services: bool = True):
        """åˆå§‹åŒ–å¢å¼·ç‰ˆ RAG è©•ä¼°å™¨"""
        self.milvus_search = MilvusOptimizedSearch()
        
        if use_mock_services:
            # ä½¿ç”¨æ¨¡æ“¬æœå‹™é€²è¡Œæ¸¬è©¦
            self.local_llm = MockLocalLLMService()
            self.openai_service = MockOpenAIService()
            logger.info("âœ… ä½¿ç”¨æ¨¡æ“¬ LLM æœå‹™é€²è¡Œæ¸¬è©¦")
        else:
            # ä½¿ç”¨çœŸå¯¦æœå‹™
            self.local_llm = LocalLLMService()
            self.openai_service = OpenAIService()
            logger.info("âœ… ä½¿ç”¨çœŸå¯¦ LLM æœå‹™")
        
        logger.info("âœ… å¢å¼·ç‰ˆ RAG è©•ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def evaluate_single_question(self, question: str) -> ComparisonResult:
        """
        è©•ä¼°å–®å€‹å•é¡Œ
        
        Args:
            question: å•é¡Œ
            
        Returns:
            ComparisonResult: å°æ¯”çµæœ
        """
        # 1. Milvus æª¢ç´¢
        retrieval_start = time.time()
        search_results = await self.milvus_search.search(question, top_k=10, nprobe=16)
        retrieval_time = time.time() - retrieval_start
        
        # æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._build_context(search_results)
        
        # 2. æœ¬åœ° LLM ç”Ÿæˆ
        local_answer, local_time, local_tokens = await self.local_llm.generate_answer(question, context)
        
        # 3. OpenAI ç”Ÿæˆ
        openai_answer, openai_time, openai_tokens = await self.openai_service.generate_answer(question, context)
        
        # 4. è©•ä¼°æŒ‡æ¨™
        local_result = EvaluationResult(
            question=question,
            answer=local_answer,
            confidence=self._calculate_confidence(local_answer, search_results),
            factuality=self._calculate_factuality(local_answer, search_results),
            relevance=self._calculate_relevance(local_answer, question),
            coherence=self._calculate_coherence(local_answer),
            inference_time=local_time,
            token_count=local_tokens,
            model_name="Qwen2.5-Taiwan-7B-Instruct",
            retrieval_method="Milvus",
            sources=search_results
        )
        
        openai_result = EvaluationResult(
            question=question,
            answer=openai_answer,
            confidence=self._calculate_confidence(openai_answer, search_results),
            factuality=self._calculate_factuality(openai_answer, search_results),
            relevance=self._calculate_relevance(openai_answer, question),
            coherence=self._calculate_coherence(openai_answer),
            inference_time=openai_time,
            token_count=openai_tokens,
            model_name="GPT-3.5-Turbo",
            retrieval_method="Milvus",
            sources=search_results
        )
        
        # 5. å°æ¯”æŒ‡æ¨™
        comparison_metrics = {
            "confidence_diff": local_result.confidence - openai_result.confidence,
            "factuality_diff": local_result.factuality - openai_result.factuality,
            "relevance_diff": local_result.relevance - openai_result.relevance,
            "coherence_diff": local_result.coherence - openai_result.coherence,
            "speed_ratio": local_result.inference_time / openai_result.inference_time if openai_result.inference_time > 0 else 0,
            "token_ratio": local_result.token_count / openai_result.token_count if openai_result.token_count > 0 else 0
        }
        
        return ComparisonResult(
            question=question,
            local_llm_result=local_result,
            openai_result=openai_result,
            comparison_metrics=comparison_metrics
        )
    
    def _build_context(self, search_results: List[Dict[str, Any]]) -> str:
        """æ§‹å»ºä¸Šä¸‹æ–‡"""
        if not search_results:
            return ""
        
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):
            content = result.get("content", "")
            if content:
                context_parts.append(f"{i}. {content[:200]}...")
        
        return "\n\n".join(context_parts)
    
    def _calculate_confidence(self, answer: str, sources: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—ä¿¡å¿ƒåº¦"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—"):
            return 0.0
        
        # ç°¡å–®çš„åŸºæ–¼ç­”æ¡ˆé•·åº¦å’Œä¾†æºæ•¸é‡çš„ä¿¡å¿ƒåº¦è¨ˆç®—
        base_confidence = min(len(answer) / 100, 1.0)  # ç­”æ¡ˆé•·åº¦
        source_confidence = min(len(sources) / 5, 1.0)  # ä¾†æºæ•¸é‡
        
        return (base_confidence + source_confidence) / 2
    
    def _calculate_factuality(self, answer: str, sources: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—äº‹å¯¦æ€§"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—"):
            return 0.0
        
        # ç°¡å–®çš„äº‹å¯¦æ€§è©•ä¼°ï¼ˆåŸºæ–¼æ˜¯å¦åŒ…å«å…·é«”è³‡è¨Šï¼‰
        factual_indicators = ["æ ¹æ“š", "åŸºæ–¼", "è³‡æ–™é¡¯ç¤º", "ç ”ç©¶æŒ‡å‡º", "çµ±è¨ˆ", "æ•¸æ“š"]
        factual_count = sum(1 for indicator in factual_indicators if indicator in answer)
        
        return min(factual_count / 3, 1.0)
    
    def _calculate_relevance(self, answer: str, question: str) -> float:
        """è¨ˆç®—ç›¸é—œæ€§"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—"):
            return 0.0
        
        # ç°¡å–®çš„ç›¸é—œæ€§è©•ä¼°ï¼ˆåŸºæ–¼é—œéµè©é‡ç–Šï¼‰
        question_words = set(question.lower().split())
        answer_words = set(answer.lower().split())
        
        if not question_words:
            return 0.0
        
        overlap = len(question_words.intersection(answer_words))
        return min(overlap / len(question_words), 1.0)
    
    def _calculate_coherence(self, answer: str) -> float:
        """è¨ˆç®—é€£è²«æ€§"""
        if not answer or answer.startswith("ç”Ÿæˆå¤±æ•—"):
            return 0.0
        
        # ç°¡å–®çš„é€£è²«æ€§è©•ä¼°ï¼ˆåŸºæ–¼å¥å­çµæ§‹ï¼‰
        sentences = answer.split("ã€‚")
        if len(sentences) <= 1:
            return 0.5
        
        # æª¢æŸ¥å¥å­é•·åº¦çš„ä¸€è‡´æ€§
        lengths = [len(s) for s in sentences if s.strip()]
        if not lengths:
            return 0.5
        
        avg_length = sum(lengths) / len(lengths)
        variance = sum((l - avg_length) ** 2 for l in lengths) / len(lengths)
        
        # è®Šç•°ä¿‚æ•¸è¶Šå°ï¼Œé€£è²«æ€§è¶Šé«˜
        cv = (variance ** 0.5) / avg_length if avg_length > 0 else 1.0
        return max(0, 1 - cv)
    
    async def evaluate_batch(self, questions: List[str]) -> List[ComparisonResult]:
        """
        æ‰¹é‡è©•ä¼°å•é¡Œ
        
        Args:
            questions: å•é¡Œåˆ—è¡¨
            
        Returns:
            List[ComparisonResult]: å°æ¯”çµæœåˆ—è¡¨
        """
        results = []
        for i, question in enumerate(questions, 1):
            logger.info(f"è©•ä¼°å•é¡Œ {i}/{len(questions)}: {question[:50]}...")
            result = await self.evaluate_single_question(question)
            results.append(result)
        
        return results
    
    def generate_comparison_report(self, results: List[ComparisonResult], output_path: str = None) -> str:
        """
        ç”Ÿæˆå°æ¯”å ±å‘Š
        
        Args:
            results: å°æ¯”çµæœåˆ—è¡¨
            output_path: è¼¸å‡ºè·¯å¾‘
            
        Returns:
            str: å ±å‘Šå…§å®¹
        """
        # æº–å‚™æ•¸æ“š
        data = []
        for result in results:
            data.append({
                "å•é¡Œ": result.question,
                "æœ¬åœ°LLMç­”æ¡ˆ": result.local_llm_result.answer[:100] + "..." if len(result.local_llm_result.answer) > 100 else result.local_llm_result.answer,
                "OpenAIç­”æ¡ˆ": result.openai_result.answer[:100] + "..." if len(result.openai_result.answer) > 100 else result.openai_result.answer,
                "æœ¬åœ°ä¿¡å¿ƒåº¦": f"{result.local_llm_result.confidence:.3f}",
                "OpenAIä¿¡å¿ƒåº¦": f"{result.openai_result.confidence:.3f}",
                "æœ¬åœ°äº‹å¯¦æ€§": f"{result.local_llm_result.factuality:.3f}",
                "OpenAIäº‹å¯¦æ€§": f"{result.openai_result.factuality:.3f}",
                "æœ¬åœ°ç›¸é—œæ€§": f"{result.local_llm_result.relevance:.3f}",
                "OpenAIç›¸é—œæ€§": f"{result.openai_result.relevance:.3f}",
                "æœ¬åœ°é€£è²«æ€§": f"{result.local_llm_result.coherence:.3f}",
                "OpenAIé€£è²«æ€§": f"{result.openai_result.coherence:.3f}",
                "æœ¬åœ°æ¨ç†æ™‚é–“(ç§’)": f"{result.local_llm_result.inference_time:.3f}",
                "OpenAIæ¨ç†æ™‚é–“(ç§’)": f"{result.openai_result.inference_time:.3f}",
                "æœ¬åœ°Tokenæ•¸": result.local_llm_result.token_count,
                "OpenAI Tokenæ•¸": result.openai_result.token_count,
                "ä¿¡å¿ƒåº¦å·®ç•°": f"{result.comparison_metrics['confidence_diff']:.3f}",
                "äº‹å¯¦æ€§å·®ç•°": f"{result.comparison_metrics['factuality_diff']:.3f}",
                "é€Ÿåº¦æ¯”": f"{result.comparison_metrics['speed_ratio']:.3f}",
                "Tokenæ¯”": f"{result.comparison_metrics['token_ratio']:.3f}"
            })
        
        # å‰µå»º DataFrame
        df = pd.DataFrame(data)
        
        # ç”Ÿæˆå ±å‘Š
        report = f"""
# RAG è©•ä¼°å°æ¯”å ±å‘Š

## è©•ä¼°æ¦‚è¦½
- è©•ä¼°æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- å•é¡Œæ•¸é‡: {len(results)}
- æª¢ç´¢æ–¹æ³•: Milvus (IVF_FLAT ç´¢å¼•)
- æœ¬åœ° LLM: Qwen2.5-Taiwan-7B-Instruct
- OpenAI LLM: GPT-3.5-Turbo

## è©³ç´°å°æ¯”çµæœ

{df.to_string(index=False)}

## çµ±è¨ˆæ‘˜è¦

### å¹³å‡æŒ‡æ¨™
- æœ¬åœ° LLM å¹³å‡ä¿¡å¿ƒåº¦: {df['æœ¬åœ°ä¿¡å¿ƒåº¦'].astype(float).mean():.3f}
- OpenAI å¹³å‡ä¿¡å¿ƒåº¦: {df['OpenAIä¿¡å¿ƒåº¦'].astype(float).mean():.3f}
- æœ¬åœ° LLM å¹³å‡äº‹å¯¦æ€§: {df['æœ¬åœ°äº‹å¯¦æ€§'].astype(float).mean():.3f}
- OpenAI å¹³å‡äº‹å¯¦æ€§: {df['OpenAIäº‹å¯¦æ€§'].astype(float).mean():.3f}
- æœ¬åœ° LLM å¹³å‡æ¨ç†æ™‚é–“: {df['æœ¬åœ°æ¨ç†æ™‚é–“(ç§’)'].astype(float).mean():.3f} ç§’
- OpenAI å¹³å‡æ¨ç†æ™‚é–“: {df['OpenAIæ¨ç†æ™‚é–“(ç§’)'].astype(float).mean():.3f} ç§’

### æ€§èƒ½å°æ¯”
- å¹³å‡é€Ÿåº¦æ¯” (æœ¬åœ°/OpenAI): {df['é€Ÿåº¦æ¯”'].astype(float).mean():.3f}
- å¹³å‡ Token æ¯” (æœ¬åœ°/OpenAI): {df['Tokenæ¯”'].astype(float).mean():.3f}
- å¹³å‡ä¿¡å¿ƒåº¦å·®ç•° (æœ¬åœ°-OpenAI): {df['ä¿¡å¿ƒåº¦å·®ç•°'].astype(float).mean():.3f}
- å¹³å‡äº‹å¯¦æ€§å·®ç•° (æœ¬åœ°-OpenAI): {df['äº‹å¯¦æ€§å·®ç•°'].astype(float).mean():.3f}

## çµè«–
1. æœ¬åœ° LLM åœ¨æ¨ç†æ™‚é–“ä¸Š {'å„ªæ–¼' if df['é€Ÿåº¦æ¯”'].astype(float).mean() < 1 else 'åŠ£æ–¼'} OpenAI
2. æœ¬åœ° LLM åœ¨ä¿¡å¿ƒåº¦ä¸Š {'å„ªæ–¼' if df['ä¿¡å¿ƒåº¦å·®ç•°'].astype(float).mean() > 0 else 'åŠ£æ–¼'} OpenAI
3. æœ¬åœ° LLM åœ¨äº‹å¯¦æ€§ä¸Š {'å„ªæ–¼' if df['äº‹å¯¦æ€§å·®ç•°'].astype(float).mean() > 0 else 'åŠ£æ–¼'} OpenAI
"""
        
        # ä¿å­˜å ±å‘Š
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            # ä¿å­˜ CSV
            csv_path = output_path.replace('.txt', '.csv')
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… å ±å‘Šå·²ä¿å­˜: {output_path}")
            logger.info(f"âœ… CSV å·²ä¿å­˜: {csv_path}")
        
        return report


# å…¨åŸŸå¯¦ä¾‹
enhanced_evaluator = EnhancedRAGEvaluator()


def get_enhanced_evaluator() -> EnhancedRAGEvaluator:
    """ç²å–å¢å¼·ç‰ˆè©•ä¼°å™¨å¯¦ä¾‹"""
    return EnhancedRAGEvaluator(use_mock_services=True)


# æ¸¬è©¦å‡½æ•¸
async def test_enhanced_evaluator():
    """æ¸¬è©¦å¢å¼·ç‰ˆè©•ä¼°å™¨ï¼ˆæ•´åˆ prompt_templates.pyï¼‰"""
    print("é–‹å§‹å¢å¼·ç‰ˆ RAG è©•ä¼°ï¼ˆæ•´åˆ prompt_templates.pyï¼‰...")
    
    evaluator = get_enhanced_evaluator()
    
    # æ¸¬è©¦å•é¡Œ
    test_questions = [
        "æˆ‘æƒ³å­¸ç¿’æŠ•è³‡ç†è²¡ï¼Œæœ‰ä»€éº¼æ¨è–¦çš„ Podcast å—ï¼Ÿ",
        "é€šå‹¤æ™‚é–“æƒ³è½ä¸€äº›è¼•é¬†çš„å…§å®¹",
        "æœ‰ä»€éº¼é—œæ–¼å‰µæ¥­çš„ç¯€ç›®æ¨è–¦å—ï¼Ÿ"
    ]
    
    # åŸ·è¡Œè©•ä¼°
    results = await evaluator.evaluate_batch(test_questions)
    
    # ç”Ÿæˆå ±å‘Š
    report = evaluator.generate_comparison_report(
        results, 
        output_path="enhanced_comparison_report.txt"
    )
    
    print("âœ… å¢å¼·ç‰ˆå°æ¯”è©•ä¼°å®Œæˆï¼ˆæ•´åˆ prompt_templates.pyï¼‰")
    print(report)
    
    # é©—è­‰ prompt_templates.py æ•´åˆ
    print("\nğŸ” Prompt Templates æ•´åˆé©—è­‰:")
    print(f"- æœ¬åœ° LLM æç¤ºè©æ¨¡æ¿å¯ç”¨: {evaluator.local_llm.prompt_templates_available}")
    print(f"- OpenAI æç¤ºè©æ¨¡æ¿å¯ç”¨: {evaluator.openai_service.prompt_templates_available}")
    
    if evaluator.local_llm.prompt_templates_available and evaluator.openai_service.prompt_templates_available:
        print("âœ… æç¤ºè©æ¨¡æ¿æ•´åˆæˆåŠŸï¼")
    else:
        print("âš ï¸ æç¤ºè©æ¨¡æ¿æ•´åˆéœ€è¦æ”¹é€²")


if __name__ == "__main__":
    asyncio.run(test_enhanced_evaluator()) 