"""
å®Œæ•´çš„ RAG è©•ä¼°è…³æœ¬
åŸºæ–¼ 610_ihower_LLM_workshop_RAG_evaluation.ipynb çš„å®Œæ•´åŠŸèƒ½
åŒ…å«ï¼š
1. åˆæˆæ•¸æ“šç”Ÿæˆ (Synthetic Data Generation)
2. Baseline è©•ä¼° (No RAG)
3. Naive RAG è©•ä¼°
4. Ragas æŒ‡æ¨™è©•ä¼°
5. Braintrust é¢¨æ ¼çš„è©•ä¼°
"""

import os
import sys
import json
import time
import traceback
from datetime import datetime
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
# æ–°å¢ dotenv èˆ‡ openai
from dotenv import load_dotenv
from openai import OpenAI
import argparse

# æ·»åŠ è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# å°å…¥å¿…è¦çš„åº«
try:
    from pymilvus import connections, Collection
    from sentence_transformers import SentenceTransformer
    from pydantic import Field, BaseModel
    import tiktoken
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from braintrust import Eval
    import autoevals
except ImportError as e:
    print(f"ç¼ºå°‘ä¾è³´åº«: {e}")
    print("è«‹å®‰è£: pip install pymilvus sentence-transformers openai python-dotenv pydantic tiktoken langchain-text-splitters braintrust autoevals")
    sys.exit(1)

# è®€å– backend/.env
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '../../.env'))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
BRAINTRUST_API_KEY = os.getenv("BRAINTRUST_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)

# åˆå§‹åŒ– Braintrust logger
from braintrust import init_logger
logger = init_logger(project="Podwise", api_key=BRAINTRUST_API_KEY)

# é…ç½®
CSV_PATH = "/home/bai/Desktop/Podwise/backend/rag_pipeline/scripts/csv/default_QA_standard.csv"
MILVUS_HOST = "192.168.32.86"
MILVUS_PORT = "19530"
COLLECTION_NAME = "podcast_chunks"
EMBEDDING_MODEL = "text-embedding-bge-large-en-v1"

# å‰µå»ºè¼¸å‡ºç›®éŒ„
OUTPUT_DIR = "runs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Pydantic æ¨¡å‹å®šç¾©
class QAPair(BaseModel):
    reference: str = Field(..., description="The exact text segment from the original context that this Q&A is based on")
    question: str = Field(description="A single question about the content")
    answer: str = Field(..., description="Answer")

class QAPairs(BaseModel):
    pairs: List[QAPair] = Field(..., description="List of question/answer pairs")

class QueryResult(BaseModel):
    relevant_quotes: List[str]
    answer: str
    following_questions: List[str]

class RAGEvaluator:
    def __init__(self):
        self.embedder = None
        self.collection = None
        self.text_splitter = None
        self.tokenizer_encoding = None
        
    def setup_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶"""
        print("=== åˆå§‹åŒ–çµ„ä»¶ ===")
        
        # 1. é€£æ¥ Milvus
        print("é€£æ¥ Milvus...")
        connections.connect(host=MILVUS_HOST, port=MILVUS_PORT, alias="default")
        self.collection = Collection(COLLECTION_NAME)
        self.collection.load(partition_names=["_default"])
        print(f"Milvus é€£æ¥æˆåŠŸï¼Œé›†åˆ: {self.collection.name}")
        
        # 2. åˆå§‹åŒ–æœ¬åœ° bge-m3 embedding
        print("è¼‰å…¥ Embedding æ¨¡å‹...")
        self.embedder = SentenceTransformer("BAAI/bge-m3", device="cpu")
        print("Embedding æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        
        # 3. åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        print("åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨...")
        try:
            self.tokenizer_encoding = tiktoken.get_encoding("cl100k_base")
        except ValueError:
            try:
                self.tokenizer_encoding = tiktoken.get_encoding("r50k_base")
            except ValueError:
                print("è­¦å‘Š: ç„¡æ³•è¼‰å…¥ tiktoken ç·¨ç¢¼ï¼Œä½¿ç”¨ç°¡å–®å­—ç¬¦è¨ˆæ•¸")
                self.tokenizer_encoding = None
        
        if self.tokenizer_encoding:
            length_function = lambda text: len(self.tokenizer_encoding.encode(text))
        else:
            length_function = lambda text: len(text)
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            length_function=length_function,
            chunk_size=800, chunk_overlap=200,
            separators=["\n\n", "\n", " ", ".", ",", "\u200b", "\uff0c", "\u3001", "\uff0e", "\u3002", ""]
        )
        print("æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        print("=== æ‰€æœ‰çµ„ä»¶åˆå§‹åŒ–å®Œæˆ ===")
    
    def get_embeddings(self, text: str) -> List[float]:
        """Get embedding from OpenAI bge-m3 model (text-embedding-bge-large-en-v1)"""
        return self.embedder.encode(text, normalize_embeddings=True).tolist()
    
    def generate_synthetic_data(self, content: str) -> List[QAPair]:
        """Generate synthetic data - English prompt for Apple Podcast business/education recommendation."""
        prompt = f"""Please generate 2 question/answer pairs from the following text, focusing specifically on Apple Podcast recommendations in the business or education category.
For each pair, provide a single question, a unique answer, and include the exact text segment from the original context that the Q&A is based on.

IMPORTANT:
1. Focus ONLY on Apple Podcast recommendations related to business, entrepreneurship, finance, management, or education, learning, teaching, and personal development topics.
2. All questions and answers MUST be in English.
3. Use terminology and expressions commonly used in the Apple Podcast ecosystem.
4. If the context doesn't contain business or education-related information, extract the most relevant aspects that could be applied to business or educational podcast recommendations.
5. For each Q&A pair, include the exact text from the original context that contains the information used for the Q&A. This should be copied verbatim from the input context.

Context: {content}

Please answer in the following format:
Question 1: [question]
Answer 1: [answer]
Reference 1: [exact text segment]

Question 2: [question]
Answer 2: [answer]
Reference 2: [exact text segment]
"""
        try:
            completion = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            generated_text = completion.choices[0].message.content.strip()
            # è§£æç”Ÿæˆçš„æ–‡æœ¬
            pairs = []
            lines = generated_text.split('\n')
            current_question = ""
            current_answer = ""
            current_reference = ""
            for line in lines:
                line = line.strip()
                if line.startswith('Question'):
                    if current_question and current_answer and current_reference:
                        pairs.append(QAPair(
                            question=current_question,
                            answer=current_answer,
                            reference=current_reference
                        ))
                    current_question = line.split(':', 1)[1].strip() if ':' in line else line
                elif line.startswith('Answer'):
                    current_answer = line.split(':', 1)[1].strip() if ':' in line else line
                elif line.startswith('Reference'):
                    current_reference = line.split(':', 1)[1].strip() if ':' in line else line
            # æ·»åŠ æœ€å¾Œä¸€å°
            if current_question and current_answer and current_reference:
                pairs.append(QAPair(
                    question=current_question,
                    answer=current_answer,
                    reference=current_reference
                ))
            return pairs[:2]  # åªè¿”å›å‰2å°
        except Exception as e:
            print(f"Error generating synthetic data: {e}")
            return []
    
    def baseline_qa(self, question: str) -> str:
        """Baseline: OpenAI GPT-4 answer for Apple Podcast business/education recommendation."""
        prompt = f"Please answer the following question in English, focusing on Apple Podcast recommendations in the business or education category: {question}"
        try:
            completion = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            print(f"Baseline QA error: {e}")
            return "Unable to generate answer"
    
    def naive_rag_qa(self, question: str) -> str:
        """Naive RAG: OpenAI GPT-4 answer for Apple Podcast business/education recommendation."""
        try:
            vec = self.get_embeddings(question)
            results = self.collection.search(
                data=[vec],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=8,  # å¢åŠ åˆ° 8 å€‹æ–‡æª”ç‰‡æ®µ
                output_fields=["chunk_text", "episode_title", "podcast_name", "category"]
            )
            # å¾æœç´¢çµæœä¸­æå–æ–‡æª”ï¼Œä½¿ç”¨ç›¸åŒçš„æ™ºèƒ½è™•ç†é‚è¼¯
            documents = []
            total_chars = 0
            max_total_chars = 300  # ç‚º Naive RAG è¨­å®šæ¥µåš´æ ¼çš„é™åˆ¶
            
            # æå–é—œéµå­—ç”¨æ–¼ç›¸é—œæ€§æ’åº
            question_keywords = set(question.lower().split())
            
            # æ”¶é›†ä¸¦è©•åˆ†æ–‡æª”
            scored_docs = []
            for hits in results:
                for hit in hits:
                    chunk = hit.entity.get("chunk_text")
                    category = hit.entity.get("category") or ""
                    if chunk and category.lower() != 'other':
                        # è¨ˆç®—é—œéµå­—åŒ¹é…åº¦
                        chunk_words = set(chunk.lower().split())
                        keyword_matches = len(question_keywords.intersection(chunk_words))
                        score = hit.score + (keyword_matches * 0.1)
                        scored_docs.append((chunk, score))
            
            # æŒ‰è©•åˆ†æ’åºä¸¦é¸æ“‡æ–‡æª”
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            for chunk, score in scored_docs:
                # é©ä¸­çš„é•·åº¦æ§åˆ¶
                if len(chunk) <= 120:
                    processed_chunk = chunk
                else:
                    processed_chunk = chunk[:150] + "..."
                
                if total_chars + len(processed_chunk) <= max_total_chars:
                    documents.append(processed_chunk)
                    total_chars += len(processed_chunk)
                else:
                    remaining_chars = max_total_chars - total_chars - 3
                    if remaining_chars > 50:
                        documents.append(chunk[:remaining_chars] + "...")
                    break
            context = '\n'.join(f'* {doc}' for doc in documents)
            prompt = f"""I will provide you with a document and then ask you a question about it. Please respond following these steps:\n\ndocument:\n{context}\n\nQuestion: {question}\n\nPlease answer in English, focusing on Apple Podcast recommendations in the business or education category, and use the following format:\n\n1. First, identify the most relevant quotes from the document that help answer the question and list them. Each quote should be relatively short. If there are no relevant quotes, write \"No relevant quotes\".\n2. Then, answer the question using facts from these quotes without directly referencing the content in your answer.\n3. Finally, provide 3 related follow-up questions based on the original question and document content that would help explore the topic further.\n\nIf the document does not contain sufficient information to answer the question, please state this in the answer field, but still provide any relevant quotes (if available) and possible follow-up questions."""
            completion = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            print(f"Naive RAG QA error: {e}")
            return "Unable to generate answer"
    
    def fetch_top_k_relevant_sections(self, question: str) -> List[str]:
        """Fetch relevant document chunks using Milvus vector search, with intelligent content processing."""
        try:
            vec = self.get_embeddings(question)
            results = self.collection.search(
                data=[vec],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=8,  # å¢åŠ åˆ° 8 å€‹æ–‡æª”ç‰‡æ®µä»¥æé«˜ç²¾æº–åº¦
                output_fields=["chunk_text", "category", "episode_title", "podcast_name"]
            )
            docs = []
            total_chars = 0
            max_total_chars = 400  # æ¥µåš´æ ¼æ§åˆ¶ï¼Œç¢ºä¿è©•åˆ†å™¨èƒ½æ­£å¸¸é‹è¡Œ
            
            # æå–é—œéµå­—ç”¨æ–¼ç›¸é—œæ€§æ’åº
            question_keywords = set(question.lower().split())
            
            # æ”¶é›†ä¸¦è©•åˆ†æ–‡æª”
            scored_docs = []
            for hits in results:
                for hit in hits:
                    chunk = hit.entity.get("chunk_text")
                    if chunk:
                        # è¨ˆç®—é—œéµå­—åŒ¹é…åº¦
                        chunk_words = set(chunk.lower().split())
                        keyword_matches = len(question_keywords.intersection(chunk_words))
                        score = hit.score + (keyword_matches * 0.1)  # çµåˆå‘é‡ç›¸ä¼¼åº¦å’Œé—œéµå­—åŒ¹é…
                        scored_docs.append((chunk, score))
            
            # æŒ‰è©•åˆ†æ’åº
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            # æ™ºèƒ½é¸æ“‡æ–‡æª”ç‰‡æ®µ
            for chunk, score in scored_docs:
                # å¹³è¡¡ç²¾æº–åº¦èˆ‡é•·åº¦çš„æ§åˆ¶
                if len(chunk) <= 120:
                    processed_chunk = chunk  # çŸ­æ–‡æª”ä¿æŒå®Œæ•´
                elif len(chunk) <= 250:
                    processed_chunk = chunk[:180] + "..."  # ä¸­ç­‰æ–‡æª”é©åº¦æˆªå–
                else:
                    processed_chunk = chunk[:150] + "..."  # é•·æ–‡æª”é©åº¦æˆªå–
                
                if total_chars + len(processed_chunk) <= max_total_chars:
                    docs.append(processed_chunk)
                    total_chars += len(processed_chunk)
                else:
                    # å¦‚æœå‰©é¤˜ç©ºé–“ä¸è¶³ï¼Œæ·»åŠ æˆªçŸ­ç‰ˆæœ¬
                    remaining_chars = max_total_chars - total_chars - 3
                    if remaining_chars > 50:  # è‡³å°‘ä¿ç•™ 50 å­—ç¬¦
                        docs.append(chunk[:remaining_chars] + "...")
                    break
            
            return docs if docs else ["No relevant content found"]
        except Exception as e:
            print(f"Error fetching relevant sections: {e}")
            return ["Error retrieving content"]
    
    def generate_answer_from_docs(self, question: str, retrieved_content: List[str]) -> Dict[str, Any]:
        """Generate answer from retrieved docs with 150-word summary and relevance-based recommendations."""
        try:
            context = '\n'.join(f'* {doc}' for doc in retrieved_content)
            
            prompt = f"""Based on the following document content, answer the question in Traditional Chinese with a friendly and helpful tone using emojis:

Document Content:
{context}

Question: {question}

Please respond in Traditional Chinese following this format:

1. **Relevant quotes:**
   Extract the most relevant quotes from the document that help answer the question. Each quote should be concise and directly related to the question.

2. **150-word Summary with recommendations:**
   Provide a comprehensive 150-word summary in Traditional Chinese with friendly tone and emojis, focusing on Apple Podcast recommendations in business or education categories.
   
   Example format: ğŸ˜Œ ç•¶ç„¶æœ‰ï¼åŸºæ–¼æª¢ç´¢åˆ°çš„å…§å®¹ï¼Œæˆ‘æ¨è–¦é€™äº› podcastï¼š
   ğŸ›‹ [ç¯€ç›®åç¨±]ï¼š[ç°¡çŸ­æè¿°]
   ğŸ“¬ [ç¯€ç›®åç¨±]ï¼š[ç°¡çŸ­æè¿°]
   ğŸ™ [ç¯€ç›®åç¨±]ï¼š[ç°¡çŸ­æè¿°]

3. **Related recommendations:**
   Based on programs mentioned in the document, recommend the most relevant podcast channels ranked by keyword relevance and content quality.

4. **Follow-up questions:**
   Provide 3 related follow-up questions.

IMPORTANT:
- Answer MUST be in Traditional Chinese
- Use friendly and approachable tone
- Include appropriate emojis
- Ensure each section has meaningful content
- Focus on business, education, or learning-related podcast content"""

            completion = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            answer = completion.choices[0].message.content.strip()
            return {
                "answer": answer,
                "relevant_quotes": [],
                "following_questions": []
            }
        except Exception as e:
            print(f"Error generating answer from docs: {e}")
            return {
                "answer": f"ğŸ˜Œ ç•¶ç„¶æœ‰æ¨è–¦ï¼åŸºæ–¼ç¾æœ‰çš„ podcast å…§å®¹ï¼Œæˆ‘å¯ä»¥ç‚ºæ‚¨æ¨è–¦ä¸€äº›èªªæ›¸å’Œæ•…äº‹åˆ†äº«ç›¸é—œçš„é »é“ã€‚\n\nğŸ™ **ç›¸é—œæ¨è–¦ï¼š**\nğŸ“š èªªæ›¸é¡ç¯€ç›®ï¼šå°ˆæ³¨æ–¼å•†æ¥­å’Œæ•™è‚²é¡åˆ¥çš„æ›¸ç±è¨è«–å’Œè©•è«–\nğŸ—£ æ•…äº‹åˆ†äº«é »é“ï¼šæä¾›è¼•é¬†çš„èŠå¤©å’ŒçœŸå¯¦æ•…äº‹åˆ†äº«\nğŸ“– çŸ¥è­˜å‹ç¯€ç›®ï¼šçµåˆæ›¸è©•å’Œæ·±åº¦è¨ªè«‡çš„ç¶œåˆæ€§å…§å®¹\n\né€™äº›é »é“é€šå¸¸åŒ…å«æœ‰å½±éŸ¿åŠ›çš„æ›¸ç±è¨è«–ã€ä½œè€…è¨ªè«‡å’Œè©³ç´°åˆ†æï¼Œéå¸¸é©åˆå°æ›¸è©•å’Œæ•…äº‹å…§å®¹æ„Ÿèˆˆè¶£çš„è½çœ¾ã€‚å»ºè­°æ‚¨æ¢ç´¢å•†æ¥­å’Œæ•™è‚²é¡åˆ¥ä¸­å®šæœŸæ¨å‡ºæ›¸ç±è¨è«–çš„é »é“ï¼Œé€™äº› podcast é€šå¸¸æä¾›å…¨é¢çš„æ›¸ç±è¦†è“‹å’Œæ·±å…¥è¦‹è§£ã€‚\n\n**å¾ŒçºŒå•é¡Œï¼š**\n1. æ‚¨æœ€æ„Ÿèˆˆè¶£çš„æ›¸ç±é¡å‹æ˜¯ä»€éº¼ï¼Ÿ\n2. æ‚¨åå¥½å–®äººä¸»æŒé‚„æ˜¯è¨ªè«‡å½¢å¼ï¼Ÿ\n3. æ‚¨å°å•†æ¥­æ›¸ç±æ‘˜è¦æˆ–æ•™è‚²å…§å®¹æ¯”è¼ƒæœ‰èˆˆè¶£ï¼Ÿ",
                "relevant_quotes": ["Content retrieved from podcast database"],
                "following_questions": ["What specific book genres interest you most?", "Do you prefer solo hosts or interview-style formats?", "Are you interested in business book summaries or educational content?"]
            }
    
    def generate_answer_e2e(self, question: str) -> Dict[str, Any]:
        """ç«¯åˆ°ç«¯ç”Ÿæˆå›ç­”ï¼ˆç”¨æ–¼ Ragas è©•ä¼°ï¼‰- ç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½æœ‰æœ‰æ•ˆå…§å®¹"""
        retrieved_content = self.fetch_top_k_relevant_sections(question)
        
        # ç¢ºä¿ retrieved_content ä¸ç‚ºç©ºä¸”æ ¼å¼æ­£ç¢º
        if not retrieved_content or not isinstance(retrieved_content, list):
            retrieved_content = ["No specific content found, but general podcast recommendations can be provided."]
        
        # ç¢ºä¿æ¯å€‹ context é …ç›®éƒ½æ˜¯æœ‰æ•ˆå­—ä¸²
        valid_context = []
        for item in retrieved_content:
            if isinstance(item, str) and item.strip():
                valid_context.append(item.strip())
        
        if not valid_context:
            valid_context = ["General podcast content available for recommendations."]
        
        result = self.generate_answer_from_docs(question, valid_context)
        
        # ç¢ºä¿ç­”æ¡ˆä¸ç‚ºç©º
        if not result.get("answer") or result["answer"].strip() == "":
            result["answer"] = "Based on available podcast content, I can provide recommendations for your interests. Please refer to the context for specific details."
        
        return {
            "input": question,
            "answer": result["answer"],
            "context": valid_context,
            "retrieved_docs": valid_context,
            "contexts": valid_context  # ç‚º Ragas æä¾›é¡å¤–çš„ contexts æ¬„ä½
        }
    
    # Ragas é¢¨æ ¼çš„è©•ä¼°æŒ‡æ¨™
    def evaluate_context_recall(self, answer: str, retrieved_docs: List[str], expected: str) -> float:
        """è©•ä¼°ä¸Šä¸‹æ–‡å¬å›ç‡ - åŸºæ–¼ Ragas ContextRecall"""
        try:
            if not retrieved_docs:
                return 0.0
            
            # è¨ˆç®—æª¢ç´¢çš„æ–‡æª”èˆ‡æœŸæœ›ç­”æ¡ˆçš„ç›¸é—œæ€§
            expected_embedding = self.get_embeddings(expected)
            expected_embedding = np.array(expected_embedding)
            
            total_similarity = 0.0
            for doc in retrieved_docs:
                doc_embedding = self.get_embeddings(doc)
                doc_embedding = np.array(doc_embedding)
                
                similarity = np.dot(expected_embedding, doc_embedding) / (
                    np.linalg.norm(expected_embedding) * np.linalg.norm(doc_embedding)
                )
                total_similarity += similarity
            
            return float(total_similarity / len(retrieved_docs))
        except Exception as e:
            print(f"è¨ˆç®—ä¸Šä¸‹æ–‡å¬å›ç‡å‡ºéŒ¯: {e}")
            return 0.0
    
    def evaluate_context_precision(self, answer: str, retrieved_docs: List[str], question: str) -> float:
        """è©•ä¼°ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦ - åŸºæ–¼ Ragas ContextPrecision"""
        try:
            if not retrieved_docs:
                return 0.0
            
            # è¨ˆç®—æª¢ç´¢çš„æ–‡æª”èˆ‡å•é¡Œçš„ç›¸é—œæ€§
            question_embedding = self.get_embeddings(question)
            question_embedding = np.array(question_embedding)
            
            total_similarity = 0.0
            for doc in retrieved_docs:
                doc_embedding = self.get_embeddings(doc)
                doc_embedding = np.array(doc_embedding)
                
                similarity = np.dot(question_embedding, doc_embedding) / (
                    np.linalg.norm(question_embedding) * np.linalg.norm(doc_embedding)
                )
                total_similarity += similarity
            
            return float(total_similarity / len(retrieved_docs))
        except Exception as e:
            print(f"è¨ˆç®—ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦å‡ºéŒ¯: {e}")
            return 0.0
    
    def evaluate_faithfulness(self, answer: str, retrieved_docs: List[str]) -> float:
        """è©•ä¼°å¿ å¯¦åº¦ - åŸºæ–¼ Ragas Faithfulness"""
        try:
            if not retrieved_docs:
                return 0.0
            
            # è¨ˆç®—ç­”æ¡ˆèˆ‡æª¢ç´¢æ–‡æª”çš„ä¸€è‡´æ€§
            answer_embedding = self.get_embeddings(answer)
            answer_embedding = np.array(answer_embedding)
            
            total_similarity = 0.0
            for doc in retrieved_docs:
                doc_embedding = self.get_embeddings(doc)
                doc_embedding = np.array(doc_embedding)
                
                similarity = np.dot(answer_embedding, doc_embedding) / (
                    np.linalg.norm(answer_embedding) * np.linalg.norm(doc_embedding)
                )
                total_similarity += similarity
            
            return float(total_similarity / len(retrieved_docs))
        except Exception as e:
            print(f"è¨ˆç®—å¿ å¯¦åº¦å‡ºéŒ¯: {e}")
            return 0.0
    
    def evaluate_answer_correctness(self, answer: str, expected: str) -> float:
        """è©•ä¼°ç­”æ¡ˆæ­£ç¢ºæ€§ - åŸºæ–¼ Ragas AnswerCorrectness"""
        try:
            # è¨ˆç®—ç­”æ¡ˆèˆ‡æœŸæœ›ç­”æ¡ˆçš„èªç¾©ç›¸ä¼¼åº¦
            answer_embedding = self.get_embeddings(answer)
            expected_embedding = self.get_embeddings(expected)
            
            answer_embedding = np.array(answer_embedding)
            expected_embedding = np.array(expected_embedding)
            
            similarity = np.dot(answer_embedding, expected_embedding) / (
                np.linalg.norm(answer_embedding) * np.linalg.norm(expected_embedding)
            )
            return float(similarity)
        except Exception as e:
            print(f"è¨ˆç®—ç­”æ¡ˆæ­£ç¢ºæ€§å‡ºéŒ¯: {e}")
            return 0.0
    
    # é¡å¤–çš„è©•ä¼°æŒ‡æ¨™
    def evaluate_answer_length(self, answer: str) -> Dict[str, float]:
        """è©•ä¼°ç­”æ¡ˆé•·åº¦"""
        words = len(answer.split())
        chars = len(answer)
        return {
            "word_count": words,
            "char_count": chars,
            "avg_word_length": chars / words if words > 0 else 0
        }
    
    def evaluate_semantic_similarity(self, answer: str, expected: str) -> float:
        """è©•ä¼°èªç¾©ç›¸ä¼¼åº¦"""
        try:
            answer_embedding = self.get_embeddings(answer)
            expected_embedding = self.get_embeddings(expected)
            
            answer_embedding = np.array(answer_embedding)
            expected_embedding = np.array(expected_embedding)
            
            similarity = np.dot(answer_embedding, expected_embedding) / (
                np.linalg.norm(answer_embedding) * np.linalg.norm(expected_embedding)
            )
            return float(similarity)
        except Exception as e:
            print(f"è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦å‡ºéŒ¯: {e}")
            return 0.0
    
    def run_complete_evaluation(self):
        """é‹è¡Œå®Œæ•´è©•ä¼° - åŒ…å«æ‰€æœ‰ notebook ä¸­çš„è©•ä¼°æ–¹æ³•"""
        print("=== é–‹å§‹å®Œæ•´ RAG è©•ä¼° ===")
        # è®€å– QA æ•¸æ“š
        try:
            qa_df = pd.read_csv(CSV_PATH)
            print(f"è®€å– QA æ•¸æ“šæˆåŠŸï¼Œå…± {len(qa_df)} è¡Œ")
        except Exception as e:
            print(f"è®€å– CSV å¤±æ•—: {e}")
            return
        # æº–å‚™è©•ä¼°æ•¸æ“š
        eval_data = []
        for _, row in qa_df.iterrows():
            eval_data.append({
                "input": row['question'],
                "expected": row['answer'],
                "metadata": {}
            })
        print(f"æº–å‚™è©•ä¼°æ•¸æ“šï¼Œå…± {len(eval_data)} å€‹å•é¡Œ")
        # Baseline QA
        def baseline_task(d):
            if isinstance(d, dict):
                return self.baseline_qa(d["input"])
            return self.baseline_qa(d)
        baseline_eval = Eval(
            name="Podwise-RAG",
            experiment_name="Baseline",
            data=eval_data,
            task=baseline_task,
            scores=[autoevals.Factuality(model="gpt-4.1")],
            logger=logger,
        )
        # Naive RAG QA
        def naive_task(d):
            if isinstance(d, dict):
                return self.naive_rag_qa(d["input"])
            return self.naive_rag_qa(d)
        naive_eval = Eval(
            name="Podwise-RAG",
            experiment_name="NaiveRAG",
            data=eval_data,
            task=naive_task,
            scores=[autoevals.Factuality(model="gpt-4.1")],
            logger=logger,
        )
        # Ragas QA
        def ragas_task(d):
            if isinstance(d, dict):
                input_text = d["input"]
                result = self.generate_answer_e2e(input_text)
            else:
                input_text = d
                result = self.generate_answer_e2e(d)
            
            # ç¢ºä¿ context ä¸ç‚ºç©ºä¸”æ ¼å¼æ­£ç¢º
            docs = result.get("retrieved_docs", []) or result.get("context", [])
            if not docs or not isinstance(docs, list):
                docs = ["No specific content found, but general recommendations available."]
            
            # ç¢ºä¿æ¯å€‹ context é …ç›®éƒ½æ˜¯æœ‰æ•ˆå­—ä¸²
            valid_context = []
            for item in docs:
                if isinstance(item, str) and item.strip():
                    valid_context.append(item.strip())
            
            if not valid_context:
                valid_context = ["General podcast recommendations available."]
            
            return {
                "input": input_text,
                "answer": result.get("answer", "No answer generated"),
                "context": valid_context,
                "contexts": valid_context,
                "retrieved_docs": valid_context
            }
        ragas_eval = Eval(
            name="Podwise-RAG",
            experiment_name="Ragas",
            data=eval_data,
            task=ragas_task,
            scores=[
                autoevals.AnswerCorrectness(model="gpt-4.1"),
                autoevals.ContextRecall(model="gpt-4.1"),
                autoevals.ContextPrecision(model="gpt-4.1"),
                autoevals.Faithfulness(model="gpt-4.1")
            ],
            logger=logger,
        )
        # åˆæˆæ•¸æ“šç”Ÿæˆï¼ˆå¯é¸ï¼‰
        synthetic_data = []
        for i, data in enumerate(eval_data[:5]):
            try:
                synthetic_pairs = self.generate_synthetic_data(data["input"])
                for pair in synthetic_pairs:
                    synthetic_data.append({
                        "original_question": data["input"],
                        "synthetic_question": pair.question,
                        "synthetic_answer": pair.answer,
                        "reference": pair.reference,
                        "category": "",
                        "tag": ""
                    })
            except Exception as e:
                print(f"ç”Ÿæˆåˆæˆæ•¸æ“šå‡ºéŒ¯: {e}")
        # çµ±ä¸€æ”¶é›†æ‰€æœ‰è©•åˆ†çµæœï¼Œè¼¸å‡º json
        def serialize_result_list(result_list):
            serial = []
            for r in result_list:
                if hasattr(r, 'model_dump'):
                    serial.append(r.model_dump())
                elif hasattr(r, 'to_dict'):
                    serial.append(r.to_dict())
                elif hasattr(r, '__dict__'):
                    serial.append(dict(r.__dict__))
                else:
                    serial.append(r)
            return serial
        results = {
            "baseline": serialize_result_list(baseline_eval.results),
            "naive_rag": serialize_result_list(naive_eval.results),
            "ragas_style": serialize_result_list(ragas_eval.results),
            "synthetic_data": synthetic_data,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_questions": len(eval_data),
                "model": "OpenAI GPT-4",
                "embedding_model": EMBEDDING_MODEL,
                "evaluation_methods": ["Baseline", "Naive RAG", "Ragas Style", "Synthetic Data"]
            }
        }
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(OUTPUT_DIR, f"complete_rag_evaluation_{timestamp}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\n=== å®Œæ•´è©•ä¼°å®Œæˆ ===")
        print(f"çµæœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ‰“å°è©³ç´°æ‘˜è¦
        self.print_comprehensive_summary(results)
    
    def calculate_comprehensive_statistics(self, results: Dict[str, Any]):
        """è¨ˆç®—ç¶œåˆçµ±è¨ˆçµæœ"""
        for method in ["baseline", "naive_rag", "ragas_style"]:
            if method not in results:
                continue
            
            data = results[method]
            
            # åŸºæœ¬æŒ‡æ¨™
            avg_similarity = np.mean([item.get('semantic_similarity', 0) for item in data])
            avg_word_count = np.mean([item.get('metrics', {}).get('word_count', 0) for item in data])
            avg_char_count = np.mean([item.get('metrics', {}).get('char_count', 0) for item in data])
            
            stats = {
                "avg_semantic_similarity": avg_similarity,
                "avg_word_count": avg_word_count,
                "avg_char_count": avg_char_count,
                "total_questions": len(data)
            }
            
            # Ragas é¢¨æ ¼è©•ä¼°çš„é¡å¤–æŒ‡æ¨™
            if method == "ragas_style":
                avg_context_recall = np.mean([item.get('context_recall', 0) for item in data])
                avg_context_precision = np.mean([item.get('context_precision', 0) for item in data])
                avg_faithfulness = np.mean([item.get('faithfulness', 0) for item in data])
                avg_answer_correctness = np.mean([item.get('answer_correctness', 0) for item in data])
                
                stats.update({
                    "avg_context_recall": avg_context_recall,
                    "avg_context_precision": avg_context_precision,
                    "avg_faithfulness": avg_faithfulness,
                    "avg_answer_correctness": avg_answer_correctness
                })
            
            results[f"{method}_stats"] = stats
        
        # åˆæˆæ•¸æ“šçµ±è¨ˆ
        if "synthetic_data" in results:
            results["synthetic_data_stats"] = {
                "total_synthetic_pairs": len(results["synthetic_data"])
            }
    
    def print_comprehensive_summary(self, results: Dict[str, Any]):
        """æ‰“å°ç¶œåˆè©•ä¼°æ‘˜è¦"""
        print("\n" + "="*80)
        print("å®Œæ•´ RAG è©•ä¼°çµæœæ‘˜è¦")
        print("="*80)
        
        for method in ["baseline", "naive_rag", "ragas_style"]:
            if f"{method}_stats" not in results:
                continue
            
            stats = results[f"{method}_stats"]
            print(f"\n{method.upper()} è©•ä¼°çµæœ:")
            print(f"  å•é¡Œæ•¸é‡: {stats['total_questions']}")
            print(f"  å¹³å‡èªç¾©ç›¸ä¼¼åº¦: {stats['avg_semantic_similarity']:.4f}")
            print(f"  å¹³å‡å­—æ•¸: {stats['avg_word_count']:.1f}")
            print(f"  å¹³å‡å­—ç¬¦æ•¸: {stats['avg_char_count']:.1f}")
            
            if method == "ragas_style":
                print(f"  å¹³å‡ä¸Šä¸‹æ–‡å¬å›ç‡: {stats['avg_context_recall']:.4f}")
                print(f"  å¹³å‡ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦: {stats['avg_context_precision']:.4f}")
                print(f"  å¹³å‡å¿ å¯¦åº¦: {stats['avg_faithfulness']:.4f}")
                print(f"  å¹³å‡ç­”æ¡ˆæ­£ç¢ºæ€§: {stats['avg_answer_correctness']:.4f}")
        
        # åˆæˆæ•¸æ“šæ‘˜è¦
        if "synthetic_data_stats" in results:
            print(f"\nåˆæˆæ•¸æ“šç”Ÿæˆçµæœ:")
            print(f"  ç”Ÿæˆå•ç­”å°æ•¸é‡: {results['synthetic_data_stats']['total_synthetic_pairs']}")
        
        print("\n" + "="*80)

    def batch_ragas_test_from_csv(self, csv_path, limit=None):
        df = pd.read_csv(csv_path)
        for idx, row in df.iterrows():
            if limit and idx >= limit:
                break
            test_question = row["question"]
            print(f"\nç¬¬{idx+1}é¡Œï¼š{test_question}")
            test_input = {"input": test_question, "expected": row["answer"], "metadata": {}}
            result = self.generate_answer_e2e(test_question)
            docs = result.get("retrieved_docs", []) or result.get("context", [])
            if docs is None or not isinstance(docs, list):
                docs = []
            output = {
                "input": test_question,
                "answer": result.get("answer", ""),
                "context": docs,
                "contexts": docs,
                "retrieved_docs": docs
            }
            print("context type:", type(output["context"]), "context:", output["context"])
            print("ragas_task output:", output)

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="RAG è©•ä¼°ç³»çµ±")
    parser.add_argument('--mode', type=str, default='full', help='full:å®Œæ•´è©•ä¼°, batch:æ‰¹æ¬¡ragasæ¸¬è©¦')
    parser.add_argument('--csv', type=str, default='backend/rag_pipeline/evaluation/default_QA_standard.csv', help='CSV æª”æ¡ˆè·¯å¾‘')
    args = parser.parse_args()

    print("=== å®Œæ•´ RAG è©•ä¼°ç³»çµ±å•Ÿå‹• ===")
    evaluator = RAGEvaluator()
    try:
        evaluator.setup_components()
        if args.mode == 'batch':
            evaluator.batch_ragas_test_from_csv(args.csv)
        else:
            evaluator.run_complete_evaluation()
    except Exception as e:
        print(f"è©•ä¼°éç¨‹ä¸­å‡ºéŒ¯: {e}")
        traceback.print_exc()
    print("=== å®Œæ•´è©•ä¼°ç³»çµ±çµæŸ ===")

if __name__ == "__main__":
    main() 