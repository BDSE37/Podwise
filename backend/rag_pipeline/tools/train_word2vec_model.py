#!/usr/bin/env python3
"""
Word2Vec æ¨¡å‹è¨“ç·´å·¥å…·

æ­¤è…³æœ¬ç”¨æ–¼è¨“ç·´ Word2Vec æ¨¡å‹ï¼ŒåŸºæ–¼ Podcast å…§å®¹é€²è¡Œèªç¾©å­¸ç¿’ï¼Œ
ç‚ºæ™ºèƒ½ TAG æå–æä¾›èªç¾©ç›¸ä¼¼åº¦è¨ˆç®—èƒ½åŠ›ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- è¼‰å…¥ Podcast æ–‡æœ¬æ•¸æ“š
- æ–‡æœ¬é è™•ç†
- Word2Vec æ¨¡å‹è¨“ç·´
- æ¨¡å‹è©•ä¼°èˆ‡ä¿å­˜

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 1.0.0
"""

import logging
import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
from datetime import datetime

# æ©Ÿå™¨å­¸ç¿’ç›¸é—œå°å…¥
try:
    import jieba
    from gensim.models import Word2Vec
    from gensim.models.word2vec import LineSentence
    from sklearn.metrics.pairwise import cosine_similarity
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    logging.error("æ©Ÿå™¨å­¸ç¿’å¥—ä»¶æœªå®‰è£ï¼Œç„¡æ³•è¨“ç·´ Word2Vec æ¨¡å‹")

logger = logging.getLogger(__name__)


class Word2VecTrainer:
    """Word2Vec æ¨¡å‹è¨“ç·´å™¨"""
    
    def __init__(self, 
                 data_dir: str = "../../../data/raw",
                 model_save_path: str = "models/word2vec_podcast.model",
                 min_count: int = 2,
                 vector_size: int = 100,
                 window: int = 5,
                 workers: int = 4):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Args:
            data_dir: æ•¸æ“šç›®éŒ„è·¯å¾‘
            model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾‘
            min_count: æœ€å°è©é »
            vector_size: å‘é‡ç¶­åº¦
            window: çª—å£å¤§å°
            workers: å·¥ä½œé€²ç¨‹æ•¸
        """
        self.data_dir = Path(data_dir)
        self.model_save_path = Path(model_save_path)
        self.min_count = min_count
        self.vector_size = vector_size
        self.window = window
        self.workers = workers
        
        # ç¢ºä¿æ¨¡å‹ä¿å­˜ç›®éŒ„å­˜åœ¨
        self.model_save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– jieba
        if ML_AVAILABLE:
            jieba.initialize()
    
    def load_podcast_data(self) -> List[str]:
        """
        è¼‰å…¥ Podcast æ•¸æ“š
        
        Returns:
            List[str]: æ–‡æœ¬åˆ—è¡¨
        """
        texts = []
        
        # æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
        supported_formats = ['.json', '.txt', '.csv']
        
        for file_path in self.data_dir.rglob('*'):
            if file_path.suffix.lower() in supported_formats:
                try:
                    if file_path.suffix.lower() == '.json':
                        texts.extend(self._load_json_data(file_path))
                    elif file_path.suffix.lower() == '.txt':
                        texts.extend(self._load_txt_data(file_path))
                    elif file_path.suffix.lower() == '.csv':
                        texts.extend(self._load_csv_data(file_path))
                    
                    logger.info(f"è¼‰å…¥æª”æ¡ˆ: {file_path}")
                    
                except Exception as e:
                    logger.error(f"è¼‰å…¥æª”æ¡ˆå¤±æ•— {file_path}: {str(e)}")
        
        logger.info(f"ç¸½å…±è¼‰å…¥ {len(texts)} å€‹æ–‡æœ¬")
        return texts
    
    def _load_json_data(self, file_path: Path) -> List[str]:
        """è¼‰å…¥ JSON æ•¸æ“š"""
        texts = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        # æå–å¯èƒ½çš„æ–‡æœ¬æ¬„ä½
                        text_fields = ['content', 'text', 'description', 'title', 'summary', 'transcript']
                        for field in text_fields:
                            if field in item and item[field]:
                                texts.append(str(item[field]))
            elif isinstance(data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œéæ­¸è™•ç†
                texts.extend(self._extract_text_from_dict(data))
        
        return texts
    
    def _load_txt_data(self, file_path: Path) -> List[str]:
        """è¼‰å…¥ TXT æ•¸æ“š"""
        texts = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    texts.append(line)
        
        return texts
    
    def _load_csv_data(self, file_path: Path) -> List[str]:
        """è¼‰å…¥ CSV æ•¸æ“š"""
        texts = []
        
        try:
            import pandas as pd
            df = pd.read_csv(file_path)
            
            # æå–å¯èƒ½çš„æ–‡æœ¬æ¬„ä½
            text_columns = [col for col in df.columns if any(keyword in col.lower() 
                           for keyword in ['content', 'text', 'description', 'title', 'summary'])]
            
            for col in text_columns:
                texts.extend(df[col].dropna().astype(str).tolist())
                
        except ImportError:
            logger.warning("pandas æœªå®‰è£ï¼Œè·³é CSV æª”æ¡ˆ")
        except Exception as e:
            logger.error(f"è¼‰å…¥ CSV æª”æ¡ˆå¤±æ•—: {str(e)}")
        
        return texts
    
    def _extract_text_from_dict(self, data: Dict[str, Any]) -> List[str]:
        """å¾å­—å…¸ä¸­æå–æ–‡æœ¬"""
        texts = []
        
        for key, value in data.items():
            if isinstance(value, str) and value.strip():
                texts.append(value)
            elif isinstance(value, dict):
                texts.extend(self._extract_text_from_dict(value))
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, str) and item.strip():
                        texts.append(item)
                    elif isinstance(item, dict):
                        texts.extend(self._extract_text_from_dict(item))
        
        return texts
    
    def preprocess_texts(self, texts: List[str]) -> List[List[str]]:
        """
        é è™•ç†æ–‡æœ¬
        
        Args:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[List[str]]: åˆ†è©å¾Œçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not ML_AVAILABLE:
            logger.error("jieba æœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œåˆ†è©")
            return []
        
        processed_texts = []
        
        for text in texts:
            if not text or not text.strip():
                continue
            
            # åˆ†è©
            words = jieba.lcut(text)
            
            # éæ¿¾åœç”¨è©å’ŒçŸ­è©
            filtered_words = []
            for word in words:
                word = word.strip()
                if len(word) >= 2 and not self._is_stopword(word):
                    filtered_words.append(word)
            
            if filtered_words:
                processed_texts.append(filtered_words)
        
        logger.info(f"é è™•ç†å®Œæˆï¼Œå…± {len(processed_texts)} å€‹æ–‡æœ¬")
        return processed_texts
    
    def _is_stopword(self, word: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºåœç”¨è©"""
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'å¥¹å€‘', 'å®ƒå€‘',
            'é€™å€‹', 'é‚£å€‹', 'é€™äº›', 'é‚£äº›', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å“ªè£¡', 'ä»€éº¼æ™‚å€™', 'æ€éº¼æ¨£',
            'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'æƒ³è¦', 'å¸Œæœ›', 'è¦ºå¾—', 'èªç‚º', 'çŸ¥é“', 'äº†è§£', 'å­¸ç¿’',
            'å·¥ä½œ', 'ç”Ÿæ´»', 'æ™‚é–“', 'å•é¡Œ', 'äº‹æƒ…', 'æ±è¥¿', 'åœ°æ–¹', 'æ™‚å€™', 'æ–¹å¼', 'æ–¹æ³•', 'çµæœ',
            'å› ç‚º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'è€Œä¸”', 'æˆ–è€…', 'å¦‚æœ', 'é›–ç„¶', 'å„˜ç®¡', 'é™¤äº†', 'é—œæ–¼', 'å°æ–¼',
            'é€šé', 'æ ¹æ“š', 'ç”±æ–¼', 'ç‚ºäº†', 'ç”±æ–¼', 'é—œæ–¼', 'å°æ–¼', 'åœ¨', 'å¾', 'åˆ°', 'å‘', 'å°', 'çµ¦',
            'è¢«', 'è®“', 'ä½¿', 'å¾—', 'åœ°', 'å¾—', 'çš„', 'åœ°', 'å¾—', 'è‘—', 'äº†', 'é', 'ä¾†', 'å»', 'ä¸Š', 'ä¸‹',
            'é€²', 'å‡º', 'å›', 'ä¾†', 'å»', 'åˆ°', 'åœ¨', 'å¾', 'å‘', 'å°', 'çµ¦', 'è¢«', 'è®“', 'ä½¿', 'å¾—'
        }
        return word in stopwords
    
    def train_word2vec_model(self, processed_texts: List[List[str]]) -> Optional[Word2Vec]:
        """
        è¨“ç·´ Word2Vec æ¨¡å‹
        
        Args:
            processed_texts: é è™•ç†å¾Œçš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            Optional[Word2Vec]: è¨“ç·´å¥½çš„æ¨¡å‹
        """
        if not ML_AVAILABLE:
            logger.error("gensim æœªå®‰è£ï¼Œç„¡æ³•è¨“ç·´ Word2Vec æ¨¡å‹")
            return None
        
        if not processed_texts:
            logger.error("æ²’æœ‰æ–‡æœ¬æ•¸æ“šå¯ç”¨æ–¼è¨“ç·´")
            return None
        
        logger.info("é–‹å§‹è¨“ç·´ Word2Vec æ¨¡å‹...")
        logger.info(f"æ–‡æœ¬æ•¸é‡: {len(processed_texts)}")
        logger.info(f"æœ€å°è©é »: {self.min_count}")
        logger.info(f"å‘é‡ç¶­åº¦: {self.vector_size}")
        logger.info(f"çª—å£å¤§å°: {self.window}")
        logger.info(f"å·¥ä½œé€²ç¨‹: {self.workers}")
        
        try:
            # è¨“ç·´æ¨¡å‹
            model = Word2Vec(
                sentences=processed_texts,
                vector_size=self.vector_size,
                window=self.window,
                min_count=self.min_count,
                workers=self.workers,
                sg=1,  # Skip-gram
                epochs=10
            )
            
            logger.info("Word2Vec æ¨¡å‹è¨“ç·´å®Œæˆ")
            logger.info(f"è©å½™è¡¨å¤§å°: {len(model.wv.key_to_index)}")
            
            return model
            
        except Exception as e:
            logger.error(f"è¨“ç·´ Word2Vec æ¨¡å‹å¤±æ•—: {str(e)}")
            return None
    
    def evaluate_model(self, model: Word2Vec) -> Dict[str, Any]:
        """
        è©•ä¼°æ¨¡å‹
        
        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            
        Returns:
            Dict[str, Any]: è©•ä¼°çµæœ
        """
        if not model:
            return {}
        
        evaluation_results = {
            'vocabulary_size': len(model.wv.key_to_index),
            'vector_size': model.vector_size,
            'window': model.window,
            'min_count': model.min_count
        }
        
        # æ¸¬è©¦ç›¸ä¼¼è©
        test_words = ['æŠ•è³‡', 'ç§‘æŠ€', 'å­¸ç¿’', 'å·¥ä½œ', 'å¥åº·']
        similar_words = {}
        
        for word in test_words:
            if word in model.wv:
                try:
                    similar = model.wv.most_similar(word, topn=5)
                    similar_words[word] = similar
                except Exception as e:
                    logger.warning(f"è¨ˆç®— {word} çš„ç›¸ä¼¼è©å¤±æ•—: {str(e)}")
        
        evaluation_results['similar_words'] = similar_words
        
        logger.info("æ¨¡å‹è©•ä¼°å®Œæˆ")
        logger.info(f"è©å½™è¡¨å¤§å°: {evaluation_results['vocabulary_size']}")
        
        return evaluation_results
    
    def save_model(self, model: Word2Vec, evaluation_results: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ¨¡å‹å’Œè©•ä¼°çµæœ
        
        Args:
            model: è¨“ç·´å¥½çš„æ¨¡å‹
            evaluation_results: è©•ä¼°çµæœ
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not model:
            return False
        
        try:
            # ä¿å­˜æ¨¡å‹
            model.save(str(self.model_save_path))
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_save_path}")
            
            # ä¿å­˜è©•ä¼°çµæœ
            eval_path = self.model_save_path.with_suffix('.eval.json')
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            logger.info(f"è©•ä¼°çµæœå·²ä¿å­˜åˆ°: {eval_path}")
            
            # ä¿å­˜è©å½™è¡¨
            vocab_path = self.model_save_path.with_suffix('.vocab.txt')
            with open(vocab_path, 'w', encoding='utf-8') as f:
                for word in sorted(model.wv.key_to_index.keys()):
                    f.write(f"{word}\n")
            logger.info(f"è©å½™è¡¨å·²ä¿å­˜åˆ°: {vocab_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±æ•—: {str(e)}")
            return False
    
    def train_and_save(self) -> bool:
        """
        å®Œæ•´çš„è¨“ç·´å’Œä¿å­˜æµç¨‹
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        logger.info("ğŸš€ é–‹å§‹ Word2Vec æ¨¡å‹è¨“ç·´æµç¨‹")
        
        try:
            # 1. è¼‰å…¥æ•¸æ“š
            logger.info("æ­¥é©Ÿ 1: è¼‰å…¥ Podcast æ•¸æ“š")
            texts = self.load_podcast_data()
            
            if not texts:
                logger.error("æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡æœ¬æ•¸æ“š")
                return False
            
            # 2. é è™•ç†
            logger.info("æ­¥é©Ÿ 2: é è™•ç†æ–‡æœ¬")
            processed_texts = self.preprocess_texts(texts)
            
            if not processed_texts:
                logger.error("é è™•ç†å¾Œæ²’æœ‰å¯ç”¨çš„æ–‡æœ¬")
                return False
            
            # 3. è¨“ç·´æ¨¡å‹
            logger.info("æ­¥é©Ÿ 3: è¨“ç·´ Word2Vec æ¨¡å‹")
            model = self.train_word2vec_model(processed_texts)
            
            if not model:
                logger.error("æ¨¡å‹è¨“ç·´å¤±æ•—")
                return False
            
            # 4. è©•ä¼°æ¨¡å‹
            logger.info("æ­¥é©Ÿ 4: è©•ä¼°æ¨¡å‹")
            evaluation_results = self.evaluate_model(model)
            
            # 5. ä¿å­˜æ¨¡å‹
            logger.info("æ­¥é©Ÿ 5: ä¿å­˜æ¨¡å‹")
            success = self.save_model(model, evaluation_results)
            
            if success:
                logger.info("âœ… Word2Vec æ¨¡å‹è¨“ç·´å®Œæˆ")
                return True
            else:
                logger.error("âŒ æ¨¡å‹ä¿å­˜å¤±æ•—")
                return False
                
        except Exception as e:
            logger.error(f"âŒ è¨“ç·´æµç¨‹å¤±æ•—: {str(e)}")
            return False


def main():
    """ä¸»å‡½æ•¸"""
    # è¨­å®šæ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = Word2VecTrainer(
        data_dir="../../../data/raw",
        model_save_path="models/word2vec_podcast.model",
        min_count=2,
        vector_size=100,
        window=5,
        workers=4
    )
    
    # åŸ·è¡Œè¨“ç·´
    success = trainer.train_and_save()
    
    if success:
        print("âœ… Word2Vec æ¨¡å‹è¨“ç·´æˆåŠŸ")
    else:
        print("âŒ Word2Vec æ¨¡å‹è¨“ç·´å¤±æ•—")


if __name__ == "__main__":
    main() 