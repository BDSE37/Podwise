#!/usr/bin/env python3
"""
çµ±ä¸€ LLM å·¥å…·æ¨¡çµ„

æ­¤æ¨¡çµ„æä¾›çµ±ä¸€çš„ LLM å·¥å…·ä»‹é¢ï¼Œæ•´åˆå¤šç¨® LLM æ¨¡å‹ï¼Œ
æ”¯æ´ CrewAI å’Œ LangChain ç”Ÿæ…‹ç³»çµ±ï¼Œæä¾›ä¸€è‡´çš„æ–‡æœ¬ç”ŸæˆåŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- å¤šæ¨¡å‹æ”¯æ´ (Qwen3, DeepSeek, ç­‰)
- LangChain å·¥å…·æ•´åˆ
- CrewAI ä»£ç†äººæ”¯æ´
- ç•°æ­¥è™•ç†
- æ¨¡å‹å¥åº·æª¢æŸ¥
- è‡ªå‹•å›é€€æ©Ÿåˆ¶

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 2.0.0
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from datetime import datetime
import time

from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain.tools import BaseTool
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.schema import BaseMessage, HumanMessage, AIMessage

# å°å…¥é…ç½®
from config.integrated_config import get_config

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class ModelConfig:
    """
    æ¨¡å‹é…ç½®æ•¸æ“šé¡åˆ¥
    
    æ­¤é¡åˆ¥å°è£äº† LLM æ¨¡å‹çš„åŸºæœ¬é…ç½®è³‡è¨Šï¼ŒåŒ…æ‹¬æ¨¡å‹åç¨±ã€
    è·¯å¾‘ã€åƒæ•¸è¨­å®šç­‰ã€‚
    
    Attributes:
        name: æ¨¡å‹åç¨±
        path: æ¨¡å‹è·¯å¾‘æˆ–æ¨™è­˜ç¬¦
        model_type: æ¨¡å‹é¡å‹
        max_length: æœ€å¤§ç”Ÿæˆé•·åº¦
        temperature: æº«åº¦åƒæ•¸
        device: è¨­å‚™é…ç½®
        trust_remote_code: æ˜¯å¦ä¿¡ä»»é ç¨‹ä»£ç¢¼
    """
    name: str
    path: str
    model_type: str
    max_length: int = 2048
    temperature: float = 0.7
    device: str = "auto"
    trust_remote_code: bool = True


@dataclass
class GenerationResult:
    """
    ç”Ÿæˆçµæœæ•¸æ“šé¡åˆ¥
    
    æ­¤é¡åˆ¥å°è£äº†æ–‡æœ¬ç”Ÿæˆçš„çµæœè³‡è¨Šï¼ŒåŒ…æ‹¬ç”Ÿæˆçš„æ–‡æœ¬ã€
    è™•ç†æ™‚é–“ã€æ¨¡å‹è³‡è¨Šç­‰ã€‚
    
    Attributes:
        text: ç”Ÿæˆçš„æ–‡æœ¬
        model_type: ä½¿ç”¨çš„æ¨¡å‹é¡å‹
        processing_time: è™•ç†æ™‚é–“
        tokens_generated: ç”Ÿæˆçš„ token æ•¸é‡
        metadata: é¡å¤–å…ƒæ•¸æ“š
    """
    text: str
    model_type: str
    processing_time: float
    tokens_generated: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)


class LLMInput(BaseModel):
    """
    LLM è¼¸å…¥æ¨¡å‹
    
    æ­¤æ¨¡å‹å®šç¾©äº† LLM å·¥å…·çš„è¼¸å…¥åƒæ•¸çµæ§‹ï¼Œæ”¯æ´
    LangChain çš„æ¨™æº–åŒ–è¼¸å…¥æ ¼å¼ã€‚
    
    Attributes:
        prompt: è¼¸å…¥æç¤ºæ–‡æœ¬
        context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
        max_length: æœ€å¤§ç”Ÿæˆé•·åº¦
        temperature: æº«åº¦åƒæ•¸
        model_type: æ¨¡å‹é¡å‹
        system_prompt: ç³»çµ±æç¤º
    """
    prompt: str = Field(description="è¼¸å…¥æç¤ºæ–‡æœ¬")
    context: Optional[List[str]] = Field(default=None, description="ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨")
    max_length: int = Field(default=2048, description="æœ€å¤§ç”Ÿæˆé•·åº¦")
    temperature: float = Field(default=0.7, description="æº«åº¦åƒæ•¸")
    model_type: str = Field(default="qwen3", description="æ¨¡å‹é¡å‹: qwen3, deepseek")
    system_prompt: Optional[str] = Field(default=None, description="ç³»çµ±æç¤º")


class BaseLLMProvider(ABC):
    """
    LLM æä¾›è€…åŸºç¤æŠ½è±¡é¡åˆ¥
    
    æ­¤é¡åˆ¥å®šç¾©äº†æ‰€æœ‰ LLM æä¾›è€…çš„åŸºæœ¬ä»‹é¢ï¼Œç¢ºä¿
    ä¸åŒæ¨¡å‹æä¾›è€…çš„ä¸€è‡´æ€§ã€‚
    """
    
    def __init__(self, config: ModelConfig) -> None:
        """
        åˆå§‹åŒ–åŸºç¤ LLM æä¾›è€…
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        self.config = config
        self.model: Optional[Any] = None
        self.tokenizer: Optional[Any] = None
        self.is_loaded = False
    
    @abstractmethod
    async def load_model(self) -> bool:
        """
        è¼‰å…¥æ¨¡å‹
        
        Returns:
            bool: è¼‰å…¥æ˜¯å¦æˆåŠŸ
        """
        raise NotImplementedError("å­é¡åˆ¥å¿…é ˆå¯¦ä½œ load_model æ–¹æ³•")
    
    @abstractmethod
    async def generate_text(self, prompt: str, **kwargs) -> GenerationResult:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¼¸å…¥æç¤º
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            GenerationResult: ç”Ÿæˆçµæœ
        """
        raise NotImplementedError("å­é¡åˆ¥å¿…é ˆå¯¦ä½œ generate_text æ–¹æ³•")
    
    @abstractmethod
    def format_prompt(self, prompt: str, context: Optional[List[str]] = None) -> str:
        """
        æ ¼å¼åŒ–æç¤ºæ–‡æœ¬
        
        Args:
            prompt: è¼¸å…¥æç¤º
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–å¾Œçš„æç¤ºæ–‡æœ¬
        """
        raise NotImplementedError("å­é¡åˆ¥å¿…é ˆå¯¦ä½œ format_prompt æ–¹æ³•")
    
    async def health_check(self) -> bool:
        """
        å¥åº·æª¢æŸ¥
        
        Returns:
            bool: æ¨¡å‹æ˜¯å¦å¥åº·
        """
        try:
            if not self.is_loaded:
                await self.load_model()
            
            test_prompt = "ä½ å¥½"
            result = await self.generate_text(test_prompt)
            return len(result.text) > 0 and "éŒ¯èª¤" not in result.text
            
        except Exception as e:
            logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {str(e)}")
            return False


class Qwen3Provider(BaseLLMProvider):
    """
    Qwen3 æ¨¡å‹æä¾›è€…
    
    æ­¤é¡åˆ¥å¯¦ä½œäº† Qwen3 æ¨¡å‹çš„è¼‰å…¥å’Œæ–‡æœ¬ç”ŸæˆåŠŸèƒ½ï¼Œ
    æ”¯æ´å¤šç¨® Qwen3 è®Šé«”ã€‚
    """
    
    async def load_model(self) -> bool:
        """
        è¼‰å…¥ Qwen3 æ¨¡å‹
        
        Returns:
            bool: è¼‰å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"è¼‰å…¥ Qwen3 æ¨¡å‹: {self.config.path}")
            
            # è¼‰å…¥åˆ†è©å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.path,
                trust_remote_code=self.config.trust_remote_code
            )
            
            # è¼‰å…¥æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.path,
                trust_remote_code=self.config.trust_remote_code,
                device_map=self.config.device
            )
            
            self.is_loaded = True
            logger.info(f"âœ… Qwen3 æ¨¡å‹è¼‰å…¥æˆåŠŸ: {self.config.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Qwen3 æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
            return False
    
    async def generate_text(self, prompt: str, **kwargs) -> GenerationResult:
        """
        ä½¿ç”¨ Qwen3 ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¼¸å…¥æç¤º
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            GenerationResult: ç”Ÿæˆçµæœ
        """
        start_time = time.time()
        
        try:
            if not self.is_loaded:
                await self.load_model()
            
            # æª¢æŸ¥æ¨¡å‹å’Œåˆ†è©å™¨æ˜¯å¦å·²è¼‰å…¥
            if self.model is None or self.tokenizer is None:
                return GenerationResult(
                    text="æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥",
                    model_type=self.config.model_type,
                    processing_time=time.time() - start_time
                )
            
            # æ ¼å¼åŒ–æç¤º
            formatted_prompt = self.format_prompt(prompt, kwargs.get('context'))
            
            # æº–å‚™è¼¸å…¥
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆæ–‡æœ¬
            outputs = self.model.generate(
                **inputs,
                max_length=kwargs.get('max_length', self.config.max_length),
                temperature=kwargs.get('temperature', self.config.temperature),
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
            
            # è§£ç¢¼è¼¸å‡º
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–å›ç­”éƒ¨åˆ†
            answer = self._extract_answer(generated_text, formatted_prompt)
            
            processing_time = time.time() - start_time
            tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])
            
            return GenerationResult(
                text=answer,
                model_type=self.config.model_type,
                processing_time=processing_time,
                tokens_generated=tokens_generated,
                metadata={"model_name": self.config.name}
            )
            
        except Exception as e:
            logger.error(f"âŒ Qwen3 ç”Ÿæˆå¤±æ•—: {str(e)}")
            return GenerationResult(
                text=f"ç”Ÿæˆå¤±æ•—: {str(e)}",
                model_type=self.config.model_type,
                processing_time=time.time() - start_time
            )
    
    def format_prompt(self, prompt: str, context: Optional[List[str]] = None) -> str:
        """
        æ ¼å¼åŒ– Qwen3 æç¤ºæ–‡æœ¬
        
        Args:
            prompt: è¼¸å…¥æç¤º
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–å¾Œçš„æç¤ºæ–‡æœ¬
        """
        if context:
            formatted_prompt = "è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œï¼š\n\n"
            for i, ctx in enumerate(context, 1):
                formatted_prompt += f"ä¸Šä¸‹æ–‡ {i}:\n{ctx}\n\n"
            formatted_prompt += f"å•é¡Œï¼š{prompt}\n\nå›ç­”ï¼š"
        else:
            formatted_prompt = prompt
            
        return formatted_prompt
    
    def _extract_answer(self, generated_text: str, original_prompt: str) -> str:
        """
        å¾ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å›ç­”éƒ¨åˆ†
        
        Args:
            generated_text: ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
            original_prompt: åŸå§‹æç¤º
            
        Returns:
            str: æå–çš„å›ç­”
        """
        if "å›ç­”ï¼š" in generated_text:
            return generated_text.split("å›ç­”ï¼š")[-1].strip()
        elif len(generated_text) > len(original_prompt):
            return generated_text[len(original_prompt):].strip()
        else:
            return generated_text


class DeepSeekProvider(BaseLLMProvider):
    """
    DeepSeek æ¨¡å‹æä¾›è€…
    
    æ­¤é¡åˆ¥å¯¦ä½œäº† DeepSeek æ¨¡å‹çš„è¼‰å…¥å’Œæ–‡æœ¬ç”ŸæˆåŠŸèƒ½ã€‚
    """
    
    async def load_model(self) -> bool:
        """
        è¼‰å…¥ DeepSeek æ¨¡å‹
        
        Returns:
            bool: è¼‰å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"è¼‰å…¥ DeepSeek æ¨¡å‹: {self.config.path}")
            
            # è¼‰å…¥åˆ†è©å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.path,
                trust_remote_code=self.config.trust_remote_code
            )
            
            # è¼‰å…¥æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.path,
                trust_remote_code=self.config.trust_remote_code,
                device_map=self.config.device
            )
            
            self.is_loaded = True
            logger.info(f"âœ… DeepSeek æ¨¡å‹è¼‰å…¥æˆåŠŸ: {self.config.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ DeepSeek æ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
            return False
    
    async def generate_text(self, prompt: str, **kwargs) -> GenerationResult:
        """
        ä½¿ç”¨ DeepSeek ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¼¸å…¥æç¤º
            **kwargs: é¡å¤–åƒæ•¸
            
        Returns:
            GenerationResult: ç”Ÿæˆçµæœ
        """
        start_time = time.time()
        
        try:
            if not self.is_loaded:
                await self.load_model()
            
            # æª¢æŸ¥æ¨¡å‹å’Œåˆ†è©å™¨æ˜¯å¦å·²è¼‰å…¥
            if self.model is None or self.tokenizer is None:
                return GenerationResult(
                    text="æ¨¡å‹æœªæ­£ç¢ºè¼‰å…¥",
                    model_type=self.config.model_type,
                    processing_time=time.time() - start_time
                )
            
            # æ ¼å¼åŒ–æç¤º
            formatted_prompt = self.format_prompt(prompt, kwargs.get('context'))
            
            # æº–å‚™è¼¸å…¥
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆæ–‡æœ¬
            outputs = self.model.generate(
                **inputs,
                max_length=kwargs.get('max_length', self.config.max_length),
                temperature=kwargs.get('temperature', self.config.temperature),
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
            
            # è§£ç¢¼è¼¸å‡º
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–å›ç­”éƒ¨åˆ†
            answer = self._extract_answer(generated_text, formatted_prompt)
            
            processing_time = time.time() - start_time
            tokens_generated = len(outputs[0]) - len(inputs['input_ids'][0])
            
            return GenerationResult(
                text=answer,
                model_type=self.config.model_type,
                processing_time=processing_time,
                tokens_generated=tokens_generated,
                metadata={"model_name": self.config.name}
            )
            
            except Exception as e:
            logger.error(f"âŒ DeepSeek ç”Ÿæˆå¤±æ•—: {str(e)}")
            return GenerationResult(
                text=f"ç”Ÿæˆå¤±æ•—: {str(e)}",
                model_type=self.config.model_type,
                processing_time=time.time() - start_time
            )
    
    def format_prompt(self, prompt: str, context: Optional[List[str]] = None) -> str:
        """
        æ ¼å¼åŒ– DeepSeek æç¤ºæ–‡æœ¬
        
        Args:
            prompt: è¼¸å…¥æç¤º
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–å¾Œçš„æç¤ºæ–‡æœ¬
        """
        if context:
            formatted_prompt = "è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œï¼š\n\n"
            for i, ctx in enumerate(context, 1):
                formatted_prompt += f"ä¸Šä¸‹æ–‡ {i}:\n{ctx}\n\n"
            formatted_prompt += f"å•é¡Œï¼š{prompt}\n\nå›ç­”ï¼š"
        else:
            formatted_prompt = prompt
            
        return formatted_prompt
    
    def _extract_answer(self, generated_text: str, original_prompt: str) -> str:
        """
        å¾ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å›ç­”éƒ¨åˆ†
        
        Args:
            generated_text: ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬
            original_prompt: åŸå§‹æç¤º
            
        Returns:
            str: æå–çš„å›ç­”
        """
        if "å›ç­”ï¼š" in generated_text:
            return generated_text.split("å›ç­”ï¼š")[-1].strip()
        elif len(generated_text) > len(original_prompt):
            return generated_text[len(original_prompt):].strip()
        else:
            return generated_text


class UnifiedLLMTool(BaseTool):
    """
    çµ±ä¸€ LLM å·¥å…·
    
    æ­¤å·¥å…·æ•´åˆäº†å¤šç¨® LLM æ¨¡å‹ï¼Œæä¾›çµ±ä¸€çš„ä»‹é¢ï¼Œ
    æ”¯æ´ LangChain å’Œ CrewAI ç”Ÿæ…‹ç³»çµ±ã€‚
    
    Attributes:
        name: å·¥å…·åç¨±
        description: å·¥å…·æè¿°
        args_schema: è¼¸å…¥åƒæ•¸æ¨¡å¼
    """
    
    name: str = "unified_llm"
    description: str = "ä½¿ç”¨å¤šç¨® LLM æ¨¡å‹é€²è¡Œæ–‡æœ¬ç”Ÿæˆå’Œè™•ç†ï¼Œæ”¯æ´ Qwen3ã€DeepSeek ç­‰æ¨¡å‹"
    args_schema = LLMInput
    
    def __init__(self, model_configs: Optional[Dict[str, ModelConfig]] = None) -> None:
        """
        åˆå§‹åŒ–çµ±ä¸€ LLM å·¥å…·
        
        Args:
            model_configs: æ¨¡å‹é…ç½®å­—å…¸
        """
        super().__init__(
            name="unified_llm",
            description="ä½¿ç”¨å¤šç¨® LLM æ¨¡å‹é€²è¡Œæ–‡æœ¬ç”Ÿæˆå’Œè™•ç†ï¼Œæ”¯æ´ Qwen3ã€DeepSeek ç­‰æ¨¡å‹",
            args_schema=LLMInput
        )
        
        # è¨­å®šé è¨­æ¨¡å‹é…ç½®
        self.model_configs = model_configs or self._get_default_configs()
        
        # åˆå§‹åŒ–æä¾›è€…
        self.providers: Dict[str, BaseLLMProvider] = {}
        self.current_provider: Optional[str] = None
        
        # è¼‰å…¥é…ç½®
        self.config = get_config()
        
        # åˆå§‹åŒ–æä¾›è€…
        self._initialize_providers()
    
    def _get_default_configs(self) -> Dict[str, ModelConfig]:
        """
        ç²å–é è¨­æ¨¡å‹é…ç½®
        
        Returns:
            Dict[str, ModelConfig]: é è¨­é…ç½®å­—å…¸
        """
        return {
            "qwen3": ModelConfig(
                name="Qwen3-7B-Instruct",
                path="Qwen/Qwen3-7B-Instruct",
                model_type="qwen3",
                max_length=2048,
                temperature=0.7
            ),
            "deepseek": ModelConfig(
                name="DeepSeek-R1",
                path="deepseek-ai/deepseek-r1",
                model_type="deepseek",
                max_length=2048,
                temperature=0.7
            )
        }
    
    def _initialize_providers(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹æä¾›è€…"""
        try:
            for model_type, config in self.model_configs.items():
                if model_type == "qwen3":
                    self.providers[model_type] = Qwen3Provider(config)
                elif model_type == "deepseek":
                    self.providers[model_type] = DeepSeekProvider(config)
                else:
                    logger.warning(f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {model_type}")
            
            # è¨­å®šé è¨­æä¾›è€…
            if "qwen3" in self.providers:
                self.current_provider = "qwen3"
            elif self.providers:
                self.current_provider = list(self.providers.keys())[0]
            
            logger.info(f"âœ… åˆå§‹åŒ– {len(self.providers)} å€‹æ¨¡å‹æä¾›è€…")
            
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æä¾›è€…å¤±æ•—: {str(e)}")
            raise
    
    def _run(
        self, 
        prompt: str, 
        context: Optional[List[str]] = None, 
        max_length: int = 2048, 
        temperature: float = 0.7,
        model_type: str = "qwen3",
        system_prompt: Optional[str] = None
    ) -> str:
        """
        åŒæ­¥åŸ·è¡Œæ–‡æœ¬ç”Ÿæˆ
        
        Args:
            prompt: è¼¸å…¥æç¤º
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            max_length: æœ€å¤§ç”Ÿæˆé•·åº¦
            temperature: æº«åº¦åƒæ•¸
            model_type: æ¨¡å‹é¡å‹
            system_prompt: ç³»çµ±æç¤º
            
        Returns:
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            # ä½¿ç”¨ç•°æ­¥æ–¹æ³•
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # å¦‚æœå·²ç¶“åœ¨äº‹ä»¶å¾ªç’°ä¸­ï¼Œå‰µå»ºæ–°ç·šç¨‹
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run,
                        self._agenerate_text(prompt, context, max_length, temperature, model_type, system_prompt)
                    )
                    result = future.result()
            else:
                result = loop.run_until_complete(
                    self._agenerate_text(prompt, context, max_length, temperature, model_type, system_prompt)
                )
            
            return result.text
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ç”Ÿæˆå¤±æ•—: {str(e)}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}"
    
    async def _arun(
        self, 
        prompt: str, 
        context: Optional[List[str]] = None, 
        max_length: int = 2048, 
        temperature: float = 0.7,
        model_type: str = "qwen3",
        system_prompt: Optional[str] = None
    ) -> str:
        """
        ç•°æ­¥åŸ·è¡Œæ–‡æœ¬ç”Ÿæˆ
        
        Args:
            prompt: è¼¸å…¥æç¤º
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            max_length: æœ€å¤§ç”Ÿæˆé•·åº¦
            temperature: æº«åº¦åƒæ•¸
            model_type: æ¨¡å‹é¡å‹
            system_prompt: ç³»çµ±æç¤º
            
        Returns:
            str: ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            result = await self._agenerate_text(prompt, context, max_length, temperature, model_type, system_prompt)
            return result.text
            
        except Exception as e:
            logger.error(f"âŒ ç•°æ­¥ç”Ÿæˆå¤±æ•—: {str(e)}")
            return f"ç”Ÿæˆå¤±æ•—: {str(e)}"
    
    async def _agenerate_text(
        self,
        prompt: str,
        context: Optional[List[str]] = None,
        max_length: int = 2048,
        temperature: float = 0.7,
        model_type: str = "qwen3",
        system_prompt: Optional[str] = None
    ) -> GenerationResult:
        """
        ç•°æ­¥ç”Ÿæˆæ–‡æœ¬çš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            prompt: è¼¸å…¥æç¤º
            context: ä¸Šä¸‹æ–‡æ–‡æœ¬åˆ—è¡¨
            max_length: æœ€å¤§ç”Ÿæˆé•·åº¦
            temperature: æº«åº¦åƒæ•¸
            model_type: æ¨¡å‹é¡å‹
            system_prompt: ç³»çµ±æç¤º
            
        Returns:
            GenerationResult: ç”Ÿæˆçµæœ
        """
        # é¸æ“‡æä¾›è€…
        provider = self._get_provider(model_type)
        if not provider:
            return GenerationResult(
                text=f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {model_type}",
                model_type=model_type,
                processing_time=0.0
            )
        
        # æº–å‚™åƒæ•¸
        kwargs = {
            'max_length': max_length,
            'temperature': temperature,
            'context': context
        }
        
        if system_prompt:
            kwargs['system_prompt'] = system_prompt
        
        # ç”Ÿæˆæ–‡æœ¬
        return await provider.generate_text(prompt, **kwargs)
    
    def _get_provider(self, model_type: str) -> Optional[BaseLLMProvider]:
        """
        ç²å–æŒ‡å®šçš„æ¨¡å‹æä¾›è€…
        
        Args:
            model_type: æ¨¡å‹é¡å‹
            
        Returns:
            Optional[BaseLLMProvider]: æ¨¡å‹æä¾›è€…
        """
        if model_type in self.providers:
            return self.providers[model_type]
        elif self.current_provider:
            logger.warning(f"æ¨¡å‹ {model_type} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­æ¨¡å‹: {self.current_provider}")
            return self.providers[self.current_provider]
        else:
            return None
    
    def get_available_models(self) -> List[str]:
        """
        ç²å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            List[str]: å¯ç”¨æ¨¡å‹åˆ—è¡¨
        """
        return list(self.providers.keys())
    
    def switch_model(self, model_type: str) -> bool:
        """
        åˆ‡æ›åˆ°æŒ‡å®šæ¨¡å‹
        
        Args:
            model_type: æ¨¡å‹é¡å‹
            
        Returns:
            bool: åˆ‡æ›æ˜¯å¦æˆåŠŸ
        """
        if model_type in self.providers:
            self.current_provider = model_type
            logger.info(f"âœ… åˆ‡æ›åˆ°æ¨¡å‹: {model_type}")
            return True
        else:
            logger.error(f"âŒ æ¨¡å‹ {model_type} ä¸å­˜åœ¨")
            return False
    
    async def health_check(self, model_type: Optional[str] = None) -> Dict[str, Any]:
        """
        å¥åº·æª¢æŸ¥
        
        Args:
            model_type: æ¨¡å‹é¡å‹ï¼Œå¦‚æœç‚º None å‰‡æª¢æŸ¥æ‰€æœ‰æ¨¡å‹
            
        Returns:
            Dict[str, Any]: å¥åº·æª¢æŸ¥çµæœ
        """
        results = {}
        
        if model_type:
            providers_to_check = {model_type: self.providers.get(model_type)}
        else:
            providers_to_check = self.providers
        
        for mt, provider in providers_to_check.items():
            if provider:
                is_healthy = await provider.health_check()
                results[mt] = {
                    "healthy": is_healthy,
                    "model_name": provider.config.name,
                    "is_loaded": provider.is_loaded
                }
            else:
                results[mt] = {
                    "healthy": False,
                    "error": "Provider not found"
                }
        
        return results


# å…¨åŸŸå·¥å…·å¯¦ä¾‹
unified_llm_tool = UnifiedLLMTool()


def get_unified_llm_tool() -> UnifiedLLMTool:
    """
    ç²å–çµ±ä¸€ LLM å·¥å…·å¯¦ä¾‹
    
    Returns:
        UnifiedLLMTool: å·¥å…·å¯¦ä¾‹
    """
    return unified_llm_tool


if __name__ == "__main__":
    # æ¸¬è©¦çµ±ä¸€ LLM å·¥å…·
    async def test_unified_llm():
        """æ¸¬è©¦çµ±ä¸€ LLM å·¥å…·"""
        print("ğŸ”§ çµ±ä¸€ LLM å·¥å…·æ¸¬è©¦")
        print("=" * 50)
        
        tool = get_unified_llm_tool()
        
        # é¡¯ç¤ºå¯ç”¨æ¨¡å‹
        print(f"å¯ç”¨æ¨¡å‹: {tool.get_available_models()}")
        print(f"ç•¶å‰æ¨¡å‹: {tool.current_provider}")
        
        # å¥åº·æª¢æŸ¥
        print("\nğŸ¥ æ¨¡å‹å¥åº·æª¢æŸ¥:")
        health_results = await tool.health_check()
        for model_type, result in health_results.items():
            status = "âœ…" if result["healthy"] else "âŒ"
            print(f"  {model_type}: {status} - {result}")
        
        # æ¸¬è©¦æ–‡æœ¬ç”Ÿæˆ
        print("\nğŸ¤– æ–‡æœ¬ç”Ÿæˆæ¸¬è©¦:")
        test_prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±"
        response = await tool._arun(test_prompt)
        print(f"å›æ‡‰: {response}")
        
        print("\nâœ… æ¸¬è©¦å®Œæˆ")
    
    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(test_unified_llm()) 