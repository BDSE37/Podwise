"""
LLM å„ªå…ˆç´šå’Œå‚™æ´æ©Ÿåˆ¶æ¸¬è©¦
ç¢ºä¿ Qwen2.5-Taiwan å’Œ Qwen3:8b å„ªå…ˆä½¿ç”¨ï¼Œåªæœ‰åœ¨é€™å…©å€‹éƒ½ä¸è¡Œæ™‚æ‰å•Ÿå‹• OpenAI å‚™æ´
"""

import asyncio
import logging
import sys
import os
from typing import Dict, Any, List
from datetime import datetime

# æ·»åŠ è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.qwen3_llm_manager import get_qwen3_llm_manager
from config.integrated_config import get_config

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMFallbackTest:
    """LLM å‚™æ´æ©Ÿåˆ¶æ¸¬è©¦é¡åˆ¥"""
    
    def __init__(self):
        self.config = get_config()
        self.llm_manager = get_qwen3_llm_manager()
        self.test_results: Dict[str, Any] = {}
        
    def test_model_availability(self) -> Dict[str, bool]:
        """æ¸¬è©¦æ‰€æœ‰æ¨¡å‹çš„å¯ç”¨æ€§"""
        logger.info("ğŸ” æ¸¬è©¦æ¨¡å‹å¯ç”¨æ€§...")
        
        available_models = self.llm_manager.get_available_models()
        model_health = {}
        
        for model_name in available_models:
            logger.info(f"æ¸¬è©¦æ¨¡å‹: {model_name}")
            is_healthy = self.llm_manager.test_model_health(model_name)
            model_health[model_name] = is_healthy
            status = "âœ… å¥åº·" if is_healthy else "âŒ ä¸å¯ç”¨"
            logger.info(f"  {model_name}: {status}")
        
        return model_health
    
    def test_priority_order(self) -> Dict[str, Any]:
        """æ¸¬è©¦æ¨¡å‹å„ªå…ˆç´šé †åº"""
        logger.info("ğŸ“‹ æ¸¬è©¦æ¨¡å‹å„ªå…ˆç´šé †åº...")
        
        priority_models = self.config.models.llm_priority or []
        logger.info(f"é…ç½®çš„å„ªå…ˆç´šé †åº: {priority_models}")
        
        # æª¢æŸ¥å„ªå…ˆç´šæ¨¡å‹æ˜¯å¦éƒ½å­˜åœ¨
        available_models = self.llm_manager.get_available_models()
        missing_models = [model for model in priority_models if model not in available_models]
        
        if missing_models:
            logger.warning(f"âš ï¸  ç¼ºå°‘å„ªå…ˆç´šæ¨¡å‹: {missing_models}")
        
        return {
            "priority_order": priority_models,
            "available_models": available_models,
            "missing_models": missing_models
        }
    
    def test_fallback_mechanism(self) -> Dict[str, Any]:
        """æ¸¬è©¦å‚™æ´æ©Ÿåˆ¶"""
        logger.info("ğŸ”„ æ¸¬è©¦å‚™æ´æ©Ÿåˆ¶...")
        
        test_prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šä½ å¥½ï¼Œè«‹ç°¡å–®ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±"
        
        # æ¸¬è©¦æœ€ä½³æ¨¡å‹é¸æ“‡
        best_model = self.llm_manager.get_best_model()
        logger.info(f"é¸æ“‡çš„æœ€ä½³æ¨¡å‹: {best_model}")
        
        # æ¸¬è©¦æ¨¡å‹å‘¼å«
        try:
            response = self.llm_manager.call_with_fallback(test_prompt)
            logger.info(f"æ¨¡å‹å›æ‡‰: {response[:100]}...")
            
            success = "éŒ¯èª¤" not in response and len(response) > 10
            logger.info(f"å‘¼å«çµæœ: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹å‘¼å«ç•°å¸¸: {str(e)}")
            success = False
            response = f"éŒ¯èª¤: {str(e)}"
        
        return {
            "best_model": best_model,
            "response": response,
            "success": success
        }
    
    def test_openai_fallback(self) -> Dict[str, Any]:
        """æ¸¬è©¦ OpenAI å‚™æ´åŠŸèƒ½"""
        logger.info("ğŸ¤– æ¸¬è©¦ OpenAI å‚™æ´åŠŸèƒ½...")
        
        # æª¢æŸ¥ OpenAI API Key
        openai_configured = bool(self.config.api.openai_api_key)
        logger.info(f"OpenAI API Key é…ç½®: {'âœ… å·²é…ç½®' if openai_configured else 'âŒ æœªé…ç½®'}")
        
        if not openai_configured:
            return {
                "configured": False,
                "message": "OpenAI API Key æœªé…ç½®ï¼Œç„¡æ³•æ¸¬è©¦å‚™æ´åŠŸèƒ½"
            }
        
        # æª¢æŸ¥ OpenAI æ¨¡å‹æ˜¯å¦å¯ç”¨
        openai_models = ["openai:gpt-3.5", "openai:gpt-4"]
        available_openai_models = []
        
        for model_name in openai_models:
            if model_name in self.llm_manager.models:
                is_healthy = self.llm_manager.test_model_health(model_name)
                if is_healthy:
                    available_openai_models.append(model_name)
                logger.info(f"  {model_name}: {'âœ… å¯ç”¨' if is_healthy else 'âŒ ä¸å¯ç”¨'}")
        
        return {
            "configured": True,
            "available_models": available_openai_models,
            "total_openai_models": len(openai_models)
        }
    
    def test_taiwan_model_priority(self) -> Dict[str, Any]:
        """æ¸¬è©¦å°ç£æ¨¡å‹å„ªå…ˆç´š"""
        logger.info("ğŸ‡¹ğŸ‡¼ æ¸¬è©¦å°ç£æ¨¡å‹å„ªå…ˆç´š...")
        
        # æª¢æŸ¥å°ç£æ¨¡å‹æ˜¯å¦ç‚ºç¬¬ä¸€å„ªå…ˆ
        priority_models = self.config.models.llm_priority or []
        taiwan_model = "qwen2.5:taiwan"
        
        is_first_priority = priority_models and priority_models[0] == taiwan_model
        logger.info(f"å°ç£æ¨¡å‹æ˜¯å¦ç‚ºç¬¬ä¸€å„ªå…ˆ: {'âœ… æ˜¯' if is_first_priority else 'âŒ å¦'}")
        
        # æª¢æŸ¥å°ç£æ¨¡å‹æ˜¯å¦å¯ç”¨
        taiwan_available = taiwan_model in self.llm_manager.models
        if taiwan_available:
            taiwan_healthy = self.llm_manager.test_model_health(taiwan_model)
            logger.info(f"å°ç£æ¨¡å‹å¯ç”¨æ€§: {'âœ… å¥åº·' if taiwan_healthy else 'âŒ ä¸å¯ç”¨'}")
        else:
            logger.warning("âš ï¸  å°ç£æ¨¡å‹æœªé…ç½®")
            taiwan_healthy = False
        
        return {
            "is_first_priority": is_first_priority,
            "available": taiwan_available,
            "healthy": taiwan_healthy
        }
    
    def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹ LLM å‚™æ´æ©Ÿåˆ¶æ¸¬è©¦")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        # åŸ·è¡Œå„é …æ¸¬è©¦
        self.test_results["model_availability"] = self.test_model_availability()
        self.test_results["priority_order"] = self.test_priority_order()
        self.test_results["fallback_mechanism"] = self.test_fallback_mechanism()
        self.test_results["openai_fallback"] = self.test_openai_fallback()
        self.test_results["taiwan_priority"] = self.test_taiwan_model_priority()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # è¨ˆç®—æ¸¬è©¦çµæœ
        self.test_results["summary"] = self._calculate_summary()
        self.test_results["duration"] = duration
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦:")
        self._print_summary()
        
        return self.test_results
    
    def _calculate_summary(self) -> Dict[str, Any]:
        """è¨ˆç®—æ¸¬è©¦æ‘˜è¦"""
        # æ¨¡å‹å¯ç”¨æ€§
        available_models = sum(self.test_results["model_availability"].values())
        total_models = len(self.test_results["model_availability"])
        availability_rate = available_models / total_models if total_models > 0 else 0
        
        # å„ªå…ˆç´šé…ç½®
        priority_correct = self.test_results["taiwan_priority"]["is_first_priority"]
        
        # å‚™æ´æ©Ÿåˆ¶
        fallback_success = self.test_results["fallback_mechanism"]["success"]
        
        # OpenAI å‚™æ´
        openai_configured = self.test_results["openai_fallback"]["configured"]
        
        return {
            "availability_rate": availability_rate,
            "priority_correct": priority_correct,
            "fallback_success": fallback_success,
            "openai_configured": openai_configured,
            "overall_success": availability_rate > 0 and fallback_success
        }
    
    def _print_summary(self):
        """åˆ—å°æ¸¬è©¦æ‘˜è¦"""
        summary = self.test_results["summary"]
        
        print(f"\nğŸ“ˆ æ¸¬è©¦æ‘˜è¦:")
        print(f"  æ¨¡å‹å¯ç”¨ç‡: {summary['availability_rate']:.2%}")
        print(f"  å„ªå…ˆç´šé…ç½®æ­£ç¢º: {'âœ…' if summary['priority_correct'] else 'âŒ'}")
        print(f"  å‚™æ´æ©Ÿåˆ¶æ­£å¸¸: {'âœ…' if summary['fallback_success'] else 'âŒ'}")
        print(f"  OpenAI å‚™æ´é…ç½®: {'âœ…' if summary['openai_configured'] else 'âŒ'}")
        print(f"  æ•´é«”æ¸¬è©¦çµæœ: {'âœ… é€šé' if summary['overall_success'] else 'âŒ å¤±æ•—'}")
        
        print(f"\nâ±ï¸  æ¸¬è©¦è€—æ™‚: {self.test_results['duration']:.2f} ç§’")
        
        # è©³ç´°è³‡è¨Š
        print(f"\nğŸ” è©³ç´°è³‡è¨Š:")
        print(f"  å¯ç”¨æ¨¡å‹: {list(self.test_results['model_availability'].keys())}")
        print(f"  å„ªå…ˆç´šé †åº: {self.test_results['priority_order']['priority_order']}")
        print(f"  æœ€ä½³æ¨¡å‹: {self.test_results['fallback_mechanism']['best_model']}")
        
        if self.test_results['openai_fallback']['configured']:
            print(f"  OpenAI å¯ç”¨æ¨¡å‹: {self.test_results['openai_fallback']['available_models']}")


async def main():
    """ä¸»å‡½æ•¸"""
    try:
        # å‰µå»ºæ¸¬è©¦å¯¦ä¾‹
        test_suite = LLMFallbackTest()
        
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        results = test_suite.run_all_tests()
        
        # è¿”å›æ¸¬è©¦çµæœ
        return results
        
    except Exception as e:
        logger.error(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
        return {
            "error": str(e),
            "success": False
        }


if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    results = asyncio.run(main())
    
    # æª¢æŸ¥æ¸¬è©¦çµæœ
    if results.get("success", True):
        print("\nğŸ‰ LLM å‚™æ´æ©Ÿåˆ¶æ¸¬è©¦å®Œæˆï¼")
        sys.exit(0)
    else:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {results.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        sys.exit(1) 