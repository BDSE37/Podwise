#!/usr/bin/env python3
"""
ç›£æ§æ¨¡çµ„

æ•´åˆæ€§èƒ½ç›£æ§ã€ç³»çµ±ç‹€æ…‹è¿½è¹¤ç­‰åŠŸèƒ½ã€‚

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 1.0.0
"""

import time
import logging
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, deque

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    timestamp: datetime
    query_id: str
    service_name: str
    response_time: float
    success: bool
    error_message: Optional[str] = None
    confidence_score: Optional[float] = None
    agent_used: Optional[str] = None
    fallback_used: bool = False
    memory_usage: Optional[float] = None
    cpu_usage: Optional[float] = None


class PerformanceMonitor:
    """æ€§èƒ½ç›£æ§å™¨"""
    
    def __init__(self, 
                 metrics_file: str = "performance_metrics.json",
                 max_history: int = 10000,
                 enable_realtime: bool = True):
        """åˆå§‹åŒ–æ€§èƒ½ç›£æ§å™¨"""
        self.metrics_file = metrics_file
        self.max_history = max_history
        self.enable_realtime = enable_realtime
        
        # æ€§èƒ½æŒ‡æ¨™å­˜å„²
        self.metrics_history: deque = deque(maxlen=max_history)
        self.realtime_stats: Dict[str, Any] = defaultdict(dict)
        
        # çµ±è¨ˆè³‡è¨Š
        self.total_queries = 0
        self.successful_queries = 0
        self.failed_queries = 0
        self.total_response_time = 0.0
        self.avg_confidence = 0.0
        
        logger.info("âœ… æ€§èƒ½ç›£æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_query_timer(self, query_id: str, service_name: str) -> float:
        """
        é–‹å§‹æŸ¥è©¢è¨ˆæ™‚
        
        Args:
            query_id: æŸ¥è©¢ ID
            service_name: æœå‹™åç¨±
            
        Returns:
            é–‹å§‹æ™‚é–“æˆ³
        """
        start_time = time.time()
        
        if self.enable_realtime:
            self.realtime_stats[service_name]["active_queries"] = \
                self.realtime_stats[service_name].get("active_queries", 0) + 1
        
        return start_time
    
    def end_query_timer(self, 
                       start_time: float,
                       query_id: str,
                       service_name: str,
                       success: bool,
                       confidence_score: Optional[float] = None,
                       agent_used: Optional[str] = None,
                       fallback_used: bool = False,
                       error_message: Optional[str] = None) -> PerformanceMetrics:
        """
        çµæŸæŸ¥è©¢è¨ˆæ™‚
        
        Args:
            start_time: é–‹å§‹æ™‚é–“
            query_id: æŸ¥è©¢ ID
            service_name: æœå‹™åç¨±
            success: æ˜¯å¦æˆåŠŸ
            confidence_score: ä¿¡å¿ƒåº¦åˆ†æ•¸
            agent_used: ä½¿ç”¨çš„ä»£ç†äºº
            fallback_used: æ˜¯å¦ä½¿ç”¨å‚™æ´
            error_message: éŒ¯èª¤è¨Šæ¯
            
        Returns:
            æ€§èƒ½æŒ‡æ¨™
        """
        end_time = time.time()
        response_time = end_time - start_time
        
        # å‰µå»ºæ€§èƒ½æŒ‡æ¨™
        metric = PerformanceMetrics(
            timestamp=datetime.now(),
            query_id=query_id,
            service_name=service_name,
            response_time=response_time,
            success=success,
            error_message=error_message,
            confidence_score=confidence_score,
            agent_used=agent_used,
            fallback_used=fallback_used
        )
        
        # æ·»åŠ åˆ°æ­·å²è¨˜éŒ„
        self.metrics_history.append(metric)
        
        # æ›´æ–°çµ±è¨ˆè³‡è¨Š
        self.total_queries += 1
        self.total_response_time += response_time
        
        if success:
            self.successful_queries += 1
            if confidence_score is not None:
                self.avg_confidence = (
                    (self.avg_confidence * (self.successful_queries - 1) + confidence_score) / 
                    self.successful_queries
                )
        else:
            self.failed_queries += 1
        
        # æ›´æ–°å³æ™‚çµ±è¨ˆ
        if self.enable_realtime:
            self._update_realtime_stats(metric)
        
        return metric
    
    def _update_realtime_stats(self, metric: PerformanceMetrics) -> None:
        """æ›´æ–°å³æ™‚çµ±è¨ˆ"""
        service_name = metric.service_name
        
        # æ›´æ–°æ´»èºæŸ¥è©¢æ•¸
        self.realtime_stats[service_name]["active_queries"] = \
            max(0, self.realtime_stats[service_name].get("active_queries", 1) - 1)
        
        # æ›´æ–°å¹³å‡å›æ‡‰æ™‚é–“
        current_avg = self.realtime_stats[service_name].get("avg_response_time", 0.0)
        query_count = self.realtime_stats[service_name].get("query_count", 0) + 1
        
        new_avg = (current_avg * (query_count - 1) + metric.response_time) / query_count
        
        self.realtime_stats[service_name].update({
            "avg_response_time": new_avg,
            "query_count": query_count,
            "last_query_time": metric.timestamp.isoformat(),
            "success_rate": (
                self.realtime_stats[service_name].get("successful_queries", 0) + 
                (1 if metric.success else 0)
            ) / query_count
        })
        
        if metric.success:
            self.realtime_stats[service_name]["successful_queries"] = \
                self.realtime_stats[service_name].get("successful_queries", 0) + 1
    
    def get_service_performance(self, service_name: str, hours: int = 24) -> Dict[str, Any]:
        """
        ç²å–æœå‹™æ€§èƒ½çµ±è¨ˆ
        
        Args:
            service_name: æœå‹™åç¨±
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            
        Returns:
            æ€§èƒ½çµ±è¨ˆ
        """
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # ç¯©é¸æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„æŒ‡æ¨™
        recent_metrics = [
            m for m in self.metrics_history 
            if m.service_name == service_name and m.timestamp >= cutoff_time
        ]
        
        if not recent_metrics:
            return {
                "service_name": service_name,
                "total_queries": 0,
                "success_rate": 0.0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "error_rate": 0.0
            }
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        total_queries = len(recent_metrics)
        successful_queries = sum(1 for m in recent_metrics if m.success)
        total_response_time = sum(m.response_time for m in recent_metrics)
        avg_response_time = total_response_time / total_queries
        success_rate = successful_queries / total_queries
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåº¦
        confidence_scores = [m.confidence_score for m in recent_metrics if m.confidence_score is not None]
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        return {
            "service_name": service_name,
            "total_queries": total_queries,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "avg_confidence": avg_confidence,
            "error_rate": 1.0 - success_rate,
            "time_range_hours": hours
        }
    
    def get_overall_performance(self, hours: int = 24) -> Dict[str, Any]:
        """
        ç²å–æ•´é«”æ€§èƒ½çµ±è¨ˆ
        
        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            
        Returns:
            æ•´é«”æ€§èƒ½çµ±è¨ˆ
        """
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # ç¯©é¸æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„æŒ‡æ¨™
        recent_metrics = [
            m for m in self.metrics_history 
            if m.timestamp >= cutoff_time
        ]
        
        if not recent_metrics:
            return {
                "total_queries": 0,
                "success_rate": 0.0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "services": {}
            }
        
        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        total_queries = len(recent_metrics)
        successful_queries = sum(1 for m in recent_metrics if m.success)
        total_response_time = sum(m.response_time for m in recent_metrics)
        avg_response_time = total_response_time / total_queries
        success_rate = successful_queries / total_queries
        
        # è¨ˆç®—å¹³å‡ä¿¡å¿ƒåº¦
        confidence_scores = [m.confidence_score for m in recent_metrics if m.confidence_score is not None]
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
        
        # æŒ‰æœå‹™åˆ†çµ„çµ±è¨ˆ
        services = {}
        for metric in recent_metrics:
            if metric.service_name not in services:
                services[metric.service_name] = {
                    "total_queries": 0,
                    "successful_queries": 0,
                    "total_response_time": 0.0,
                    "confidence_scores": []
                }
            
            services[metric.service_name]["total_queries"] += 1
            services[metric.service_name]["total_response_time"] += metric.response_time
            
            if metric.success:
                services[metric.service_name]["successful_queries"] += 1
            
            if metric.confidence_score is not None:
                services[metric.service_name]["confidence_scores"].append(metric.confidence_score)
        
        # è¨ˆç®—æ¯å€‹æœå‹™çš„çµ±è¨ˆ
        for service_name, stats in services.items():
            stats["success_rate"] = stats["successful_queries"] / stats["total_queries"]
            stats["avg_response_time"] = stats["total_response_time"] / stats["total_queries"]
            stats["avg_confidence"] = (
                sum(stats["confidence_scores"]) / len(stats["confidence_scores"])
                if stats["confidence_scores"] else 0.0
            )
            del stats["confidence_scores"]
        
        return {
            "total_queries": total_queries,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "avg_confidence": avg_confidence,
            "services": services,
            "time_range_hours": hours
        }
    
    def get_performance_alerts(self) -> list:
        """
        ç²å–æ€§èƒ½è­¦å ±
        
        Returns:
            è­¦å ±åˆ—è¡¨
        """
        alerts = []
        
        # æª¢æŸ¥æ•´é«”æˆåŠŸç‡
        if self.total_queries > 0:
            success_rate = self.successful_queries / self.total_queries
            if success_rate < 0.8:
                alerts.append({
                    "type": "low_success_rate",
                    "message": f"æ•´é«”æˆåŠŸç‡éä½: {success_rate:.2%}",
                    "severity": "high" if success_rate < 0.6 else "medium",
                    "timestamp": datetime.now().isoformat()
                })
        
        # æª¢æŸ¥å¹³å‡å›æ‡‰æ™‚é–“
        if self.total_queries > 0:
            avg_response_time = self.total_response_time / self.total_queries
            if avg_response_time > 10.0:
                alerts.append({
                    "type": "high_response_time",
                    "message": f"å¹³å‡å›æ‡‰æ™‚é–“éé«˜: {avg_response_time:.2f}ç§’",
                    "severity": "high" if avg_response_time > 30.0 else "medium",
                    "timestamp": datetime.now().isoformat()
                })
        
        # æª¢æŸ¥å³æ™‚çµ±è¨ˆ
        for service_name, stats in self.realtime_stats.items():
            if stats.get("active_queries", 0) > 100:
                alerts.append({
                    "type": "high_concurrency",
                    "message": f"{service_name} æœå‹™ä¸¦ç™¼é‡éé«˜: {stats['active_queries']}",
                    "severity": "medium",
                    "timestamp": datetime.now().isoformat()
                })
        
        return alerts
    
    def export_metrics(self, format: str = "json", filepath: Optional[str] = None) -> str:
        """
        åŒ¯å‡ºæ€§èƒ½æŒ‡æ¨™
        
        Args:
            format: åŒ¯å‡ºæ ¼å¼
            filepath: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åŒ¯å‡ºæª”æ¡ˆè·¯å¾‘
        """
        import json
        from datetime import datetime
        
        if not filepath:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"performance_metrics_{timestamp}.{format}"
        
        try:
            if format.lower() == "json":
                # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                export_data = {
                    "export_timestamp": datetime.now().isoformat(),
                    "total_queries": self.total_queries,
                    "successful_queries": self.successful_queries,
                    "failed_queries": self.failed_queries,
                    "avg_response_time": self.total_response_time / max(self.total_queries, 1),
                    "avg_confidence": self.avg_confidence,
                    "metrics": [
                        {
                            "timestamp": m.timestamp.isoformat(),
                            "query_id": m.query_id,
                            "service_name": m.service_name,
                            "response_time": m.response_time,
                            "success": m.success,
                            "confidence_score": m.confidence_score,
                            "agent_used": m.agent_used,
                            "fallback_used": m.fallback_used
                        }
                        for m in self.metrics_history
                    ]
                }
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ€§èƒ½æŒ‡æ¨™å·²åŒ¯å‡ºåˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"âŒ åŒ¯å‡ºæ€§èƒ½æŒ‡æ¨™å¤±æ•—: {e}")
            raise
    
    def cleanup_old_metrics(self, days: int = 30) -> None:
        """
        æ¸…ç†èˆŠçš„æ€§èƒ½æŒ‡æ¨™
        
        Args:
            days: ä¿ç•™å¤©æ•¸
        """
        cutoff_time = datetime.now() - timedelta(days=days)
        
        # ç§»é™¤èˆŠæŒ‡æ¨™
        original_count = len(self.metrics_history)
        self.metrics_history = deque(
            [m for m in self.metrics_history if m.timestamp >= cutoff_time],
            maxlen=self.max_history
        )
        
        removed_count = original_count - len(self.metrics_history)
        if removed_count > 0:
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {removed_count} å€‹èˆŠæ€§èƒ½æŒ‡æ¨™")


def get_performance_monitor() -> PerformanceMonitor:
    """ç²å–æ€§èƒ½ç›£æ§å™¨å¯¦ä¾‹"""
    return PerformanceMonitor() 