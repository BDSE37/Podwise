#!/usr/bin/env python3
"""
å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG ç³»çµ±ç›£æ§é¢æ¿
ç”¨æ–¼å¯¦æ™‚ç›£æ§å„å±¤ç´šçš„æ•ˆèƒ½æŒ‡æ¨™å’Œç³»çµ±ç‹€æ…‹
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any
import yaml

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="å±¤ç´šåŒ– RAG ç³»çµ±ç›£æ§é¢æ¿",
    page_icon="ğŸŒ³",
    layout="wide",
    initial_sidebar_state="expanded"
)

class HierarchicalRAGMonitor:
    """å±¤ç´šåŒ– RAG ç³»çµ±ç›£æ§å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç›£æ§å™¨"""
        self.levels = [
            "æŸ¥è©¢é‡å¯«è½‰æ›æ‹“å±•",
            "æ··åˆæœå°‹", 
            "æª¢ç´¢å¢å¼·",
            "é‡æ–°æ’åº",
            "ä¸Šä¸‹æ–‡å£“ç¸®éæ¿¾",
            "æ··åˆå¼RAG"
        ]
        
        self.metrics_history = []
        self.ml_pipeline_metrics = {}
        self.load_mock_data()
        self._initialize_ml_pipeline_monitoring()
    
    def _initialize_ml_pipeline_monitoring(self):
        """åˆå§‹åŒ– ML Pipeline ç›£æ§"""
        try:
            import sys
            import os
            
            # æ·»åŠ  ML Pipeline è·¯å¾‘
            ml_pipeline_path = os.path.join(
                os.path.dirname(__file__), 
                '..', '..', 'ml_pipeline'
            )
            if ml_pipeline_path not in sys.path:
                sys.path.insert(0, ml_pipeline_path)
            
            # å˜—è©¦å°å…¥ ML Pipeline æœå‹™
            try:
                from services import RecommendationService
                from config.recommender_config import get_recommender_config
                
                # åˆå§‹åŒ–æ¨è–¦æœå‹™ç”¨æ–¼ç›£æ§
                config = get_recommender_config()
                db_url = os.getenv("DATABASE_URL", config.get("database_url", ""))
                
                if db_url:
                    self.ml_pipeline_service = RecommendationService(db_url, config)
                    print("ML Pipeline ç›£æ§åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("æœªè¨­å®š DATABASE_URLï¼ŒML Pipeline ç›£æ§å°‡ä¸å¯ç”¨")
                    self.ml_pipeline_service = None
                    
            except ImportError:
                print("ML Pipeline æ¨¡çµ„ä¸å¯ç”¨ï¼Œç›£æ§åŠŸèƒ½å°‡å—é™")
                self.ml_pipeline_service = None
                
        except Exception as e:
            print(f"ML Pipeline ç›£æ§åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            self.ml_pipeline_service = None
    
    def load_mock_data(self):
        """è¼‰å…¥æ¨¡æ“¬æ•¸æ“š"""
        # æ¨¡æ“¬æ­·å²æ•¸æ“š
        for i in range(100):
            timestamp = datetime.now() - timedelta(minutes=i)
            self.metrics_history.append({
                'timestamp': timestamp,
                'level_1_confidence': 0.85 + (i % 10) * 0.01,
                'level_2_confidence': 0.78 + (i % 8) * 0.015,
                'level_3_confidence': 0.82 + (i % 12) * 0.012,
                'level_4_confidence': 0.88 + (i % 6) * 0.02,
                'level_5_confidence': 0.90 + (i % 4) * 0.025,
                'level_6_confidence': 0.92 + (i % 5) * 0.015,
                'response_time': 2.5 + (i % 15) * 0.1,
                'throughput': 50 + (i % 20) * 2,
                'error_rate': 0.02 + (i % 10) * 0.005,
                'user_satisfaction': 0.85 + (i % 8) * 0.02
            })
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æŒ‡æ¨™"""
        return {
            'level_1_confidence': 0.87,
            'level_2_confidence': 0.81,
            'level_3_confidence': 0.84,
            'level_4_confidence': 0.89,
            'level_5_confidence': 0.91,
            'level_6_confidence': 0.93,
            'response_time': 2.8,
            'throughput': 52,
            'error_rate': 0.025,
            'user_satisfaction': 0.87
        }
    
    def get_service_status(self) -> Dict[str, str]:
        """ç²å–æœå‹™ç‹€æ…‹"""
        status = {
            'RAG Pipeline': 'ğŸŸ¢ é‹è¡Œä¸­',
            'CrewAI': 'ğŸŸ¢ é‹è¡Œä¸­',
            'AnythingLLM': 'ğŸŸ¢ é‹è¡Œä¸­',
            'LLM Service': 'ğŸŸ¢ é‹è¡Œä¸­',
            'Ollama': 'ğŸŸ¢ é‹è¡Œä¸­',
            'PostgreSQL': 'ğŸŸ¢ é‹è¡Œä¸­',
            'MongoDB': 'ğŸŸ¢ é‹è¡Œä¸­',
            'Milvus': 'ğŸŸ¢ é‹è¡Œä¸­',
            'MinIO': 'ğŸŸ¢ é‹è¡Œä¸­'
        }
        
        # æ·»åŠ  ML Pipeline ç‹€æ…‹
        if hasattr(self, 'ml_pipeline_service') and self.ml_pipeline_service:
            try:
                ml_status = self.ml_pipeline_service.get_system_status()
                status['ML Pipeline'] = 'ğŸŸ¢ é‹è¡Œä¸­' if ml_status.get('status') == 'healthy' else 'ğŸŸ¡ è­¦å‘Š'
            except:
                status['ML Pipeline'] = 'ğŸ”´ éŒ¯èª¤'
        else:
            status['ML Pipeline'] = 'âšª æœªé€£æ¥'
        
        return status
    
    def get_ml_pipeline_metrics(self) -> Dict[str, Any]:
        """ç²å– ML Pipeline æŒ‡æ¨™"""
        if not hasattr(self, 'ml_pipeline_service') or not self.ml_pipeline_service:
            return {
                'recommendation_accuracy': 0.0,
                'user_satisfaction': 0.0,
                'diversity_score': 0.0,
                'response_time': 0.0,
                'throughput': 0
            }
        
        try:
            # æ¨¡æ“¬ ML Pipeline æŒ‡æ¨™
            return {
                'recommendation_accuracy': 0.85,
                'user_satisfaction': 0.87,
                'diversity_score': 0.78,
                'response_time': 1.2,
                'throughput': 45
            }
        except Exception as e:
            print(f"ç²å– ML Pipeline æŒ‡æ¨™å¤±æ•—: {str(e)}")
            return {
                'recommendation_accuracy': 0.0,
                'user_satisfaction': 0.0,
                'diversity_score': 0.0,
                'response_time': 0.0,
                'throughput': 0
            }

def main():
    """ä¸»å‡½æ•¸"""
    st.title("ğŸŒ³ å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG ç³»çµ±ç›£æ§é¢æ¿")
    st.markdown("---")
    
    # åˆå§‹åŒ–ç›£æ§å™¨
    monitor = HierarchicalRAGMonitor()
    
    # å´é‚Šæ¬„é…ç½®
    st.sidebar.title("ğŸ›ï¸ ç›£æ§é…ç½®")
    
    # æ™‚é–“ç¯„åœé¸æ“‡
    time_range = st.sidebar.selectbox(
        "æ™‚é–“ç¯„åœ",
        ["æœ€è¿‘1å°æ™‚", "æœ€è¿‘6å°æ™‚", "æœ€è¿‘24å°æ™‚", "æœ€è¿‘7å¤©"]
    )
    
    # æŒ‡æ¨™é¸æ“‡
    selected_metrics = st.sidebar.multiselect(
        "ç›£æ§æŒ‡æ¨™",
        ["ä¿¡å¿ƒå€¼", "å›æ‡‰æ™‚é–“", "ååé‡", "éŒ¯èª¤ç‡", "ç”¨æˆ¶æ»¿æ„åº¦"],
        default=["ä¿¡å¿ƒå€¼", "å›æ‡‰æ™‚é–“"]
    )
    
    # è‡ªå‹•åˆ·æ–°
    auto_refresh = st.sidebar.checkbox("è‡ªå‹•åˆ·æ–°", value=True)
    if auto_refresh:
        time.sleep(1)
        st.rerun()
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“Š å±¤ç´šåŒ–æ•ˆèƒ½æŒ‡æ¨™")
        
        # ç²å–ç•¶å‰æŒ‡æ¨™
        current_metrics = monitor.get_current_metrics()
        
        # ä¿¡å¿ƒå€¼åœ–è¡¨
        if "ä¿¡å¿ƒå€¼" in selected_metrics:
            confidence_data = {
                'å±¤ç´š': monitor.levels,
                'ä¿¡å¿ƒå€¼': [
                    current_metrics['level_1_confidence'],
                    current_metrics['level_2_confidence'],
                    current_metrics['level_3_confidence'],
                    current_metrics['level_4_confidence'],
                    current_metrics['level_5_confidence'],
                    current_metrics['level_6_confidence']
                ]
            }
            
            df_confidence = pd.DataFrame(confidence_data)
            
            fig_confidence = px.bar(
                df_confidence,
                x='å±¤ç´š',
                y='ä¿¡å¿ƒå€¼',
                title="å„å±¤ç´šä¿¡å¿ƒå€¼",
                color='ä¿¡å¿ƒå€¼',
                color_continuous_scale='RdYlGn',
                range_color=[0.5, 1.0]
            )
            
            fig_confidence.update_layout(
                height=400,
                showlegend=False
            )
            
            st.plotly_chart(fig_confidence, use_container_width=True)
        
        # å›æ‡‰æ™‚é–“åœ–è¡¨
        if "å›æ‡‰æ™‚é–“" in selected_metrics:
            # æ¨¡æ“¬å„å±¤ç´šå›æ‡‰æ™‚é–“
            response_times = [0.5, 1.2, 0.8, 0.6, 0.4, 2.0]
            
            fig_response = px.line(
                x=monitor.levels,
                y=response_times,
                title="å„å±¤ç´šå›æ‡‰æ™‚é–“ (ç§’)",
                markers=True
            )
            
            fig_response.update_layout(
                height=300,
                xaxis_title="å±¤ç´š",
                yaxis_title="å›æ‡‰æ™‚é–“ (ç§’)"
            )
            
            st.plotly_chart(fig_response, use_container_width=True)
    
    with col2:
        st.subheader("ğŸ”§ ç³»çµ±ç‹€æ…‹")
        
        # æœå‹™ç‹€æ…‹
        service_status = monitor.get_service_status()
        
        for service, status in service_status.items():
            st.markdown(f"**{service}**: {status}")
        
        st.markdown("---")
        
        # é—œéµæŒ‡æ¨™å¡ç‰‡
        st.subheader("ğŸ“ˆ é—œéµæŒ‡æ¨™")
        
        col_a, col_b = st.columns(2)
        
        with col_a:
            st.metric(
                label="å¹³å‡ä¿¡å¿ƒå€¼",
                value=f"{current_metrics['level_6_confidence']:.3f}",
                delta="+0.02"
            )
            
            st.metric(
                label="å›æ‡‰æ™‚é–“",
                value=f"{current_metrics['response_time']:.1f}s",
                delta="-0.2s"
            )
        
        with col_b:
            st.metric(
                label="ååé‡",
                value=f"{current_metrics['throughput']} req/min",
                delta="+3"
            )
            
            st.metric(
                label="éŒ¯èª¤ç‡",
                value=f"{current_metrics['error_rate']:.3f}",
                delta="-0.005"
            )
    
    # è©³ç´°ç›£æ§å€åŸŸ
    st.markdown("---")
    
    col3, col4 = st.columns(2)
    
    with col3:
        st.subheader("ğŸ”„ å±¤ç´šé–“æ•¸æ“šæµ")
        
        # å±¤ç´šé–“æ•¸æ“šæµåœ–
        fig_flow = go.Figure()
        
        # æ·»åŠ ç¯€é»
        node_x = [0, 0, 0, 0, 0, 0]
        node_y = [0, 1, 2, 3, 4, 5]
        node_text = monitor.levels
        
        fig_flow.add_trace(go.Scatter(
            x=node_x,
            y=node_y,
            mode='markers+text',
            marker=dict(size=20, color='lightblue'),
            text=node_text,
            textposition="middle right",
            name="å±¤ç´š"
        ))
        
        # æ·»åŠ é€£æ¥ç·š
        for i in range(len(node_y) - 1):
            fig_flow.add_trace(go.Scatter(
                x=[0, 0],
                y=[node_y[i], node_y[i + 1]],
                mode='lines',
                line=dict(color='gray', width=2),
                showlegend=False
            ))
        
        fig_flow.update_layout(
            title="å±¤ç´šåŒ–æ•¸æ“šæµ",
            height=400,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            showlegend=False
        )
        
        st.plotly_chart(fig_flow, use_container_width=True)
    
    with col4:
        st.subheader("ğŸ“‹ å±¤ç´šè©³ç´°ä¿¡æ¯")
        
        # å±¤ç´šè©³ç´°ä¿¡æ¯è¡¨æ ¼
        level_details = []
        for i, level in enumerate(monitor.levels, 1):
            level_details.append({
                'å±¤ç´š': f"ç¬¬{i}å±¤",
                'åç¨±': level,
                'ä¿¡å¿ƒå€¼': current_metrics[f'level_{i}_confidence'],
                'ç‹€æ…‹': 'ğŸŸ¢ æ­£å¸¸' if current_metrics[f'level_{i}_confidence'] > 0.7 else 'ğŸŸ¡ è­¦å‘Š',
                'è™•ç†æ™‚é–“': f"{0.5 + i * 0.2:.1f}s"
            })
        
        df_details = pd.DataFrame(level_details)
        st.dataframe(df_details, use_container_width=True)
    
    # æ­·å²è¶¨å‹¢åœ–
    st.markdown("---")
    st.subheader("ğŸ“ˆ æ­·å²è¶¨å‹¢")
    
    # è½‰æ›æ­·å²æ•¸æ“šç‚º DataFrame
    df_history = pd.DataFrame(monitor.metrics_history)
    
    # é¸æ“‡è¦é¡¯ç¤ºçš„æŒ‡æ¨™
    if "ä¿¡å¿ƒå€¼" in selected_metrics:
        fig_trend = make_subplots(
            rows=2, cols=1,
            subplot_titles=("ä¿¡å¿ƒå€¼è¶¨å‹¢", "å›æ‡‰æ™‚é–“è¶¨å‹¢"),
            vertical_spacing=0.1
        )
        
        # æ·»åŠ ä¿¡å¿ƒå€¼è¶¨å‹¢
        for i in range(1, 7):
            fig_trend.add_trace(
                go.Scatter(
                    x=df_history['timestamp'],
                    y=df_history[f'level_{i}_confidence'],
                    name=f"ç¬¬{i}å±¤",
                    mode='lines'
                ),
                row=1, col=1
            )
        
        # æ·»åŠ å›æ‡‰æ™‚é–“è¶¨å‹¢
        fig_trend.add_trace(
            go.Scatter(
                x=df_history['timestamp'],
                y=df_history['response_time'],
                name="å›æ‡‰æ™‚é–“",
                mode='lines',
                line=dict(color='red')
            ),
            row=2, col=1
        )
        
        fig_trend.update_layout(height=600, showlegend=True)
        st.plotly_chart(fig_trend, use_container_width=True)
    
    # å‘Šè­¦å’Œå»ºè­°
    st.markdown("---")
    
    col5, col6 = st.columns(2)
    
    with col5:
        st.subheader("âš ï¸ ç³»çµ±å‘Šè­¦")
        
        alerts = []
        if current_metrics['error_rate'] > 0.05:
            alerts.append("ğŸ”´ éŒ¯èª¤ç‡éé«˜ (>5%)")
        if current_metrics['response_time'] > 5:
            alerts.append("ğŸŸ¡ å›æ‡‰æ™‚é–“éé•· (>5s)")
        if current_metrics['level_1_confidence'] < 0.7:
            alerts.append("ğŸŸ¡ ç¬¬ä¸€å±¤ä¿¡å¿ƒå€¼åä½")
        
        if alerts:
            for alert in alerts:
                st.error(alert)
        else:
            st.success("âœ… ç³»çµ±é‹è¡Œæ­£å¸¸ï¼Œç„¡å‘Šè­¦")
    
    with col6:
        st.subheader("ğŸ’¡ å„ªåŒ–å»ºè­°")
        
        suggestions = []
        if current_metrics['level_2_confidence'] < 0.8:
            suggestions.append("è€ƒæ…®å„ªåŒ–æ··åˆæœå°‹ç­–ç•¥")
        if current_metrics['throughput'] < 30:
            suggestions.append("å¢åŠ ç³»çµ±è³‡æºæˆ–å„ªåŒ–æ€§èƒ½")
        if current_metrics['user_satisfaction'] < 0.8:
            suggestions.append("æ”¹é€²å›æ‡‰è³ªé‡å’Œç›¸é—œæ€§")
        
        if suggestions:
            for suggestion in suggestions:
                st.info(suggestion)
        else:
            st.success("âœ… ç³»çµ±é‹è¡Œè‰¯å¥½ï¼Œç„¡éœ€å„ªåŒ–")
    
    # é è…³
    st.markdown("---")
    st.markdown(
        f"<div style='text-align: center; color: gray;'>"
        f"æœ€å¾Œæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | "
        f"å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG ç³»çµ±ç›£æ§é¢æ¿"
        f"</div>",
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main() 