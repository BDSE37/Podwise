#!/usr/bin/env python3
"""
Â±§Á¥öÂåñÊ®πÁãÄÁµêÊßã RAG Á≥ªÁµ±Áõ£ÊéßÈù¢Êùø
Êï¥ÂêàÊïàËÉΩÁõ£ÊéßÂíåÂÑÄË°®ÊùøÂäüËÉΩÔºåÁî®ÊñºÂØ¶ÊôÇÁõ£ÊéßÂêÑÂ±§Á¥öÁöÑÊïàËÉΩÊåáÊ®ôÂíåÁ≥ªÁµ±ÁãÄÊÖã
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import asyncio
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import statistics
import os
import yaml

# Ë®≠ÂÆöÊó•Ë™å
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ÊïàËÉΩÊåáÊ®ôÊï∏ÊìöÁµêÊßã"""
    timestamp: datetime
    query_id: str
    service_name: str
    response_time: float
    success: bool
    error_message: Optional[str] = None
    confidence_score: Optional[float] = None
    agent_used: Optional[str] = None
    fallback_used: bool = False
    memory_usage: Optional[float] = None
    cpu_usage: Optional[float] = None

# Ë®≠ÂÆöÈ†ÅÈù¢ÈÖçÁΩÆ
st.set_page_config(
    page_title="Â±§Á¥öÂåñ RAG Á≥ªÁµ±Áõ£ÊéßÈù¢Êùø",
    page_icon="üå≥",
    layout="wide",
    initial_sidebar_state="expanded"
)

class HierarchicalRAGMonitor:
    """Â±§Á¥öÂåñ RAG Á≥ªÁµ±Áõ£ÊéßÂô®"""
    
    def __init__(self):
        """ÂàùÂßãÂåñÁõ£ÊéßÂô®"""
        self.levels = [
            "Êü•Ë©¢ÈáçÂØ´ËΩâÊèõÊãìÂ±ï",
            "Ê∑∑ÂêàÊêúÂ∞ã", 
            "Ê™¢Á¥¢Â¢ûÂº∑",
            "ÈáçÊñ∞ÊéíÂ∫è",
            "‰∏ä‰∏ãÊñáÂ£ìÁ∏ÆÈÅéÊøæ",
            "Ê∑∑ÂêàÂºèRAG"
        ]
        
        # ÊïàËÉΩÁõ£ÊéßÁõ∏Èóú
        self.metrics_file = "performance_metrics.json"
        self.max_history = 10000
        self.metrics_history = deque(maxlen=self.max_history)
        self.service_metrics = defaultdict(list)
        self.realtime_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "current_confidence_avg": 0.0,
            "fallback_rate": 0.0
        }
        
        # Ê≠∑Âè≤Êï∏Êìö
        self.metrics_history_display = []
        self.ml_pipeline_metrics = {}
        
        # ÂàùÂßãÂåñ
        self.load_mock_data()
        self._initialize_ml_pipeline_monitoring()
        self._load_metrics()
    
    def _load_metrics(self):
        """ËºâÂÖ•Ê≠∑Âè≤ÊïàËÉΩÊåáÊ®ô"""
        try:
            if os.path.exists(self.metrics_file):
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for metric_data in data.get('metrics', []):
                        metric = PerformanceMetrics(
                            timestamp=datetime.fromisoformat(metric_data['timestamp']),
                            query_id=metric_data['query_id'],
                            service_name=metric_data['service_name'],
                            response_time=metric_data['response_time'],
                            success=metric_data['success'],
                            error_message=metric_data.get('error_message'),
                            confidence_score=metric_data.get('confidence_score'),
                            agent_used=metric_data.get('agent_used'),
                            fallback_used=metric_data.get('fallback_used', False),
                            memory_usage=metric_data.get('memory_usage'),
                            cpu_usage=metric_data.get('cpu_usage')
                        )
                        self.metrics_history.append(metric)
                        self.service_metrics[metric.service_name].append(metric)
                
                logger.info(f"‚úÖ ËºâÂÖ• {len(self.metrics_history)} Ê¢ùÊ≠∑Âè≤ÊïàËÉΩÊåáÊ®ô")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ËºâÂÖ•Ê≠∑Âè≤ÊåáÊ®ôÂ§±Êïó: {e}")
    
    def _save_metrics(self):
        """ÂÑ≤Â≠òÊïàËÉΩÊåáÊ®ôÂà∞Ê™îÊ°à"""
        try:
            data = {
                'last_updated': datetime.now().isoformat(),
                'total_metrics': len(self.metrics_history),
                'metrics': [asdict(metric) for metric in self.metrics_history]
            }
            
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"‚ùå ÂÑ≤Â≠òÊïàËÉΩÊåáÊ®ôÂ§±Êïó: {e}")
    
    def record_performance_metric(self, 
                                query_id: str,
                                service_name: str,
                                response_time: float,
                                success: bool,
                                confidence_score: Optional[float] = None,
                                agent_used: Optional[str] = None,
                                fallback_used: bool = False,
                                error_message: Optional[str] = None) -> PerformanceMetrics:
        """Ë®òÈåÑÊïàËÉΩÊåáÊ®ô"""
        metric = PerformanceMetrics(
            timestamp=datetime.now(),
            query_id=query_id,
            service_name=service_name,
            response_time=response_time,
            success=success,
            error_message=error_message,
            confidence_score=confidence_score,
            agent_used=agent_used,
            fallback_used=fallback_used
        )
        
        self.metrics_history.append(metric)
        self.service_metrics[service_name].append(metric)
        self._update_realtime_stats(metric)
        
        # ÂÆöÊúüÂÑ≤Â≠ò
        if len(self.metrics_history) % 100 == 0:
            self._save_metrics()
        
        return metric
    
    def _update_realtime_stats(self, metric: PerformanceMetrics):
        """Êõ¥Êñ∞Âç≥ÊôÇÁµ±Ë®àÊï∏Êìö"""
        self.realtime_stats["total_queries"] += 1
        
        if metric.success:
            self.realtime_stats["successful_queries"] += 1
        else:
            self.realtime_stats["failed_queries"] += 1
        
        # Êõ¥Êñ∞Âπ≥ÂùáÈüøÊáâÊôÇÈñì
        if self.metrics_history:
            total_time = sum(m.response_time for m in self.metrics_history)
            self.realtime_stats["average_response_time"] = total_time / len(self.metrics_history)
        
        # Êõ¥Êñ∞Âπ≥Âùá‰ø°ÂøÉÂÄº
        confidence_scores = [m.confidence_score for m in self.metrics_history if m.confidence_score is not None]
        if confidence_scores:
            self.realtime_stats["current_confidence_avg"] = statistics.mean(confidence_scores)
        
        # Êõ¥Êñ∞ÂÇôÊè¥Áéá
        fallback_count = sum(1 for m in self.metrics_history if m.fallback_used)
        self.realtime_stats["fallback_rate"] = fallback_count / len(self.metrics_history) if self.metrics_history else 0.0
    
    def get_service_performance(self, service_name: str, hours: int = 24) -> Dict[str, Any]:
        """Áç≤ÂèñÊúçÂãôÊïàËÉΩÁµ±Ë®à"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.service_metrics[service_name] if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {
                "total_queries": 0,
                "success_rate": 0.0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "fallback_rate": 0.0
            }
        
        success_count = sum(1 for m in recent_metrics if m.success)
        confidence_scores = [m.confidence_score for m in recent_metrics if m.confidence_score is not None]
        fallback_count = sum(1 for m in recent_metrics if m.fallback_used)
        
        return {
            "total_queries": len(recent_metrics),
            "success_rate": success_count / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in recent_metrics),
            "avg_confidence": statistics.mean(confidence_scores) if confidence_scores else 0.0,
            "fallback_rate": fallback_count / len(recent_metrics)
        }
    
    def get_overall_performance(self, hours: int = 24) -> Dict[str, Any]:
        """Áç≤ÂèñÊï¥È´îÊïàËÉΩÁµ±Ë®à"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {
                "total_queries": 0,
                "success_rate": 0.0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "fallback_rate": 0.0,
                "service_breakdown": {}
            }
        
        success_count = sum(1 for m in recent_metrics if m.success)
        confidence_scores = [m.confidence_score for m in recent_metrics if m.confidence_score is not None]
        fallback_count = sum(1 for m in recent_metrics if m.fallback_used)
        
        # ÊúçÂãôÁ¥∞ÂàÜ
        service_breakdown = {}
        for service_name in set(m.service_name for m in recent_metrics):
            service_metrics = [m for m in recent_metrics if m.service_name == service_name]
            service_breakdown[service_name] = {
                "queries": len(service_metrics),
                "success_rate": sum(1 for m in service_metrics if m.success) / len(service_metrics),
                "avg_response_time": statistics.mean(m.response_time for m in service_metrics)
            }
        
        return {
            "total_queries": len(recent_metrics),
            "success_rate": success_count / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in recent_metrics),
            "avg_confidence": statistics.mean(confidence_scores) if confidence_scores else 0.0,
            "fallback_rate": fallback_count / len(recent_metrics),
            "service_breakdown": service_breakdown
        }
    
    def _initialize_ml_pipeline_monitoring(self):
        """ÂàùÂßãÂåñ ML Pipeline Áõ£Êéß"""
        try:
            import sys
            import os
            
            # Ê∑ªÂä† ML Pipeline Ë∑ØÂæë
            ml_pipeline_path = os.path.join(
                os.path.dirname(__file__), 
                '..', '..', 'ml_pipeline'
            )
            if ml_pipeline_path not in sys.path:
                sys.path.insert(0, ml_pipeline_path)
            
            # ÂòóË©¶Â∞éÂÖ• ML Pipeline ÊúçÂãô
            try:
                from backend.ml_pipeline.services import RecommendationService
                from backend.ml_pipeline.config.recommender_config import get_recommender_config
                
                # ÂàùÂßãÂåñÊé®Ëñ¶ÊúçÂãôÁî®ÊñºÁõ£Êéß
                config = get_recommender_config()
                db_url = os.getenv("DATABASE_URL", config.get("database_url", ""))
                
                if db_url:
                    self.ml_pipeline_service = RecommendationService(db_url, config)
                    print("ML Pipeline Áõ£ÊéßÂàùÂßãÂåñÊàêÂäü")
                else:
                    print("Êú™Ë®≠ÂÆö DATABASE_URLÔºåML Pipeline Áõ£ÊéßÂ∞á‰∏çÂèØÁî®")
                    self.ml_pipeline_service = None
                    
            except ImportError:
                print("ML Pipeline Ê®°ÁµÑ‰∏çÂèØÁî®ÔºåÁõ£ÊéßÂäüËÉΩÂ∞áÂèóÈôê")
                self.ml_pipeline_service = None
                
        except Exception as e:
            print(f"ML Pipeline Áõ£ÊéßÂàùÂßãÂåñÂ§±Êïó: {str(e)}")
            self.ml_pipeline_service = None
    
    def load_mock_data(self):
        """ËºâÂÖ•Ê®°Êì¨Êï∏Êìö"""
        # Ê®°Êì¨Ê≠∑Âè≤Êï∏Êìö
        for i in range(100):
            timestamp = datetime.now() - timedelta(minutes=i)
            self.metrics_history_display.append({
                'timestamp': timestamp,
                'level_1_confidence': 0.85 + (i % 10) * 0.01,
                'level_2_confidence': 0.78 + (i % 8) * 0.015,
                'level_3_confidence': 0.82 + (i % 12) * 0.012,
                'level_4_confidence': 0.88 + (i % 6) * 0.02,
                'level_5_confidence': 0.90 + (i % 4) * 0.025,
                'level_6_confidence': 0.92 + (i % 5) * 0.015,
                'response_time': 2.5 + (i % 15) * 0.1,
                'throughput': 50 + (i % 20) * 2,
                'error_rate': 0.02 + (i % 10) * 0.005,
                'user_satisfaction': 0.85 + (i % 8) * 0.02
            })
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """Áç≤ÂèñÁï∂ÂâçÊåáÊ®ô"""
        return {
            'level_1_confidence': 0.87,
            'level_2_confidence': 0.81,
            'level_3_confidence': 0.84,
            'level_4_confidence': 0.89,
            'level_5_confidence': 0.91,
            'level_6_confidence': 0.93,
            'response_time': 2.8,
            'throughput': 52,
            'error_rate': 0.025,
            'user_satisfaction': 0.87
        }
    
    def get_service_status(self) -> Dict[str, str]:
        """Áç≤ÂèñÊúçÂãôÁãÄÊÖã"""
        status = {
            'RAG Pipeline': 'üü¢ ÈÅãË°å‰∏≠',
            'CrewAI': 'üü¢ ÈÅãË°å‰∏≠',
            'AnythingLLM': 'üü¢ ÈÅãË°å‰∏≠',
            'LLM Service': 'üü¢ ÈÅãË°å‰∏≠',
            'Ollama': 'üü¢ ÈÅãË°å‰∏≠',
            'PostgreSQL': 'üü¢ ÈÅãË°å‰∏≠',
            'MongoDB': 'üü¢ ÈÅãË°å‰∏≠',
            'Milvus': 'üü¢ ÈÅãË°å‰∏≠',
            'MinIO': 'üü¢ ÈÅãË°å‰∏≠'
        }
        
        # Ê∑ªÂä† ML Pipeline ÁãÄÊÖã
        if hasattr(self, 'ml_pipeline_service') and self.ml_pipeline_service:
            try:
                ml_status = self.ml_pipeline_service.get_system_status()
                status['ML Pipeline'] = 'üü¢ ÈÅãË°å‰∏≠' if ml_status.get('status') == 'healthy' else 'üü° Ë≠¶Âëä'
            except:
                status['ML Pipeline'] = 'üî¥ ÈåØË™§'
        else:
            status['ML Pipeline'] = '‚ö™ Êú™ÈÄ£Êé•'
        
        return status
    
    def get_ml_pipeline_metrics(self) -> Dict[str, Any]:
        """Áç≤Âèñ ML Pipeline ÊåáÊ®ô"""
        if not hasattr(self, 'ml_pipeline_service') or not self.ml_pipeline_service:
            return {
                'recommendation_accuracy': 0.0,
                'user_satisfaction': 0.0,
                'diversity_score': 0.0,
                'response_time': 0.0,
                'throughput': 0
            }
        
        try:
            # Ê®°Êì¨ ML Pipeline ÊåáÊ®ô
            return {
                'recommendation_accuracy': 0.85,
                'user_satisfaction': 0.87,
                'diversity_score': 0.78,
                'response_time': 1.2,
                'throughput': 45
            }
        except Exception as e:
            print(f"Áç≤Âèñ ML Pipeline ÊåáÊ®ôÂ§±Êïó: {str(e)}")
            return {
                'recommendation_accuracy': 0.0,
                'user_satisfaction': 0.0,
                'diversity_score': 0.0,
                'response_time': 0.0,
                'throughput': 0
            }

def main():
    """‰∏ªÂáΩÊï∏"""
    st.title("üå≥ Â±§Á¥öÂåñÊ®πÁãÄÁµêÊßã RAG Á≥ªÁµ±Áõ£ÊéßÈù¢Êùø")
    st.markdown("---")
    
    # ÂàùÂßãÂåñÁõ£ÊéßÂô®
    monitor = HierarchicalRAGMonitor()
    
    # ÂÅ¥ÈÇäÊ¨ÑÈÖçÁΩÆ
    st.sidebar.title("üéõÔ∏è Áõ£ÊéßÈÖçÁΩÆ")
    
    # ÊôÇÈñìÁØÑÂúçÈÅ∏Êìá
    time_range = st.sidebar.selectbox(
        "ÊôÇÈñìÁØÑÂúç",
        ["ÊúÄËøë1Â∞èÊôÇ", "ÊúÄËøë6Â∞èÊôÇ", "ÊúÄËøë24Â∞èÊôÇ", "ÊúÄËøë7Â§©"]
    )
    
    # ÊåáÊ®ôÈÅ∏Êìá
    selected_metrics = st.sidebar.multiselect(
        "Áõ£ÊéßÊåáÊ®ô",
        ["‰ø°ÂøÉÂÄº", "ÂõûÊáâÊôÇÈñì", "ÂêûÂêêÈáè", "ÈåØË™§Áéá", "Áî®Êà∂ÊªøÊÑèÂ∫¶"],
        default=["‰ø°ÂøÉÂÄº", "ÂõûÊáâÊôÇÈñì"]
    )
    
    # Ëá™ÂãïÂà∑Êñ∞
    auto_refresh = st.sidebar.checkbox("Ëá™ÂãïÂà∑Êñ∞", value=True)
    if auto_refresh:
        time.sleep(1)
        st.rerun()
    
    # ‰∏ªË¶ÅÂÖßÂÆπÂçÄÂüü
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("üìä Á≥ªÁµ±ÊïàËÉΩÊåáÊ®ô")
        
        # Áç≤ÂèñÊï¥È´îÊïàËÉΩÁµ±Ë®à
        overall_perf = monitor.get_overall_performance(hours=24)
        
        # ÊïàËÉΩÊåáÊ®ôÂç°Áâá
        perf_col1, perf_col2, perf_col3, perf_col4 = st.columns(4)
        
        with perf_col1:
            st.metric(
                "Á∏ΩÊü•Ë©¢Êï∏",
                overall_perf["total_queries"],
                delta=f"{overall_perf['success_rate']:.1%} ÊàêÂäüÁéá"
            )
        
        with perf_col2:
            st.metric(
                "Âπ≥ÂùáÂõûÊáâÊôÇÈñì",
                f"{overall_perf['avg_response_time']:.2f}s",
                delta=f"{overall_perf['fallback_rate']:.1%} ÂÇôÊè¥Áéá"
            )
        
        with perf_col3:
            st.metric(
                "Âπ≥Âùá‰ø°ÂøÉÂÄº",
                f"{overall_perf['avg_confidence']:.2f}",
                delta="‰ø°ÂøÉÂÄº"
            )
        
        with perf_col4:
            st.metric(
                "Âç≥ÊôÇÊü•Ë©¢",
                monitor.realtime_stats["total_queries"],
                delta="Ê¥ªË∫çÊü•Ë©¢"
            )
    
    with col2:
        st.subheader("üîß ÊúçÂãôÁãÄÊÖã")
        service_status = monitor.get_service_status()
        
        for service, status in service_status.items():
            st.write(f"{service}: {status}")
    
    # ÂúñË°®ÂçÄÂüü
    st.subheader("üìà ÊïàËÉΩË∂®Âã¢Âúñ")
    
    # ËΩâÊèõÊ≠∑Âè≤Êï∏ÊìöÁÇ∫ DataFrame
    if monitor.metrics_history_display:
        df = pd.DataFrame(monitor.metrics_history_display)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # ‰ø°ÂøÉÂÄºË∂®Âã¢Âúñ
        if "‰ø°ÂøÉÂÄº" in selected_metrics:
            fig_confidence = go.Figure()
            for i, level in enumerate(monitor.levels, 1):
                col_name = f'level_{i}_confidence'
                if col_name in df.columns:
                    fig_confidence.add_trace(go.Scatter(
                        x=df['timestamp'],
                        y=df[col_name],
                        mode='lines',
                        name=level,
                        line=dict(width=2)
                    ))
            
            fig_confidence.update_layout(
                title="ÂêÑÂ±§Á¥ö‰ø°ÂøÉÂÄºË∂®Âã¢",
                xaxis_title="ÊôÇÈñì",
                yaxis_title="‰ø°ÂøÉÂÄº",
                height=400
            )
            st.plotly_chart(fig_confidence, use_container_width=True)
        
        # ÂõûÊáâÊôÇÈñìË∂®Âã¢Âúñ
        if "ÂõûÊáâÊôÇÈñì" in selected_metrics:
            fig_response = px.line(
                df, 
                x='timestamp', 
                y='response_time',
                title="ÂõûÊáâÊôÇÈñìË∂®Âã¢"
            )
            fig_response.update_layout(height=400)
            st.plotly_chart(fig_response, use_container_width=True)
    
    # ÊúçÂãôÊïàËÉΩÁ¥∞ÂàÜ
    st.subheader("üîç ÊúçÂãôÊïàËÉΩÁ¥∞ÂàÜ")
    service_perf = overall_perf.get("service_breakdown", {})
    
    if service_perf:
        service_df = pd.DataFrame([
            {
                "ÊúçÂãô": service,
                "Êü•Ë©¢Êï∏": data["queries"],
                "ÊàêÂäüÁéá": data["success_rate"],
                "Âπ≥ÂùáÂõûÊáâÊôÇÈñì": data["avg_response_time"]
            }
            for service, data in service_perf.items()
        ])
        
        st.dataframe(service_df, use_container_width=True)
    
    # ML Pipeline ÊåáÊ®ô
    st.subheader("ü§ñ ML Pipeline ÊåáÊ®ô")
    ml_metrics = monitor.get_ml_pipeline_metrics()
    
    ml_col1, ml_col2, ml_col3, ml_col4 = st.columns(4)
    
    with ml_col1:
        st.metric("Êé®Ëñ¶Ê∫ñÁ¢∫Áéá", f"{ml_metrics['recommendation_accuracy']:.2f}")
    
    with ml_col2:
        st.metric("Áî®Êà∂ÊªøÊÑèÂ∫¶", f"{ml_metrics['user_satisfaction']:.2f}")
    
    with ml_col3:
        st.metric("Â§öÊ®£ÊÄßÂàÜÊï∏", f"{ml_metrics['diversity_score']:.2f}")
    
    with ml_col4:
        st.metric("ML ÂõûÊáâÊôÇÈñì", f"{ml_metrics['response_time']:.2f}s")

if __name__ == "__main__":
    main() 