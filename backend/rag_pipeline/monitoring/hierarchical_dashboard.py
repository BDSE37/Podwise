#!/usr/bin/env python3
"""
å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG ç³»çµ±ç›£æ§é¢æ¿
æ•´åˆæ•ˆèƒ½ç›£æ§å’Œå„€è¡¨æ¿åŠŸèƒ½ï¼Œç”¨æ–¼å¯¦æ™‚ç›£æ§å„å±¤ç´šçš„æ•ˆèƒ½æŒ‡æ¨™å’Œç³»çµ±ç‹€æ…‹
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import asyncio
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import statistics
import os
import yaml

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ•ˆèƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    timestamp: datetime
    query_id: str
    service_name: str
    response_time: float
    success: bool
    error_message: Optional[str] = None
    confidence_score: Optional[float] = None
    agent_used: Optional[str] = None
    fallback_used: bool = False
    memory_usage: Optional[float] = None
    cpu_usage: Optional[float] = None

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="å±¤ç´šåŒ– RAG ç³»çµ±ç›£æ§é¢æ¿",
    page_icon="ğŸŒ³",
    layout="wide",
    initial_sidebar_state="expanded"
)

class HierarchicalRAGMonitor:
    """å±¤ç´šåŒ– RAG ç³»çµ±ç›£æ§å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç›£æ§å™¨"""
        self.levels = [
            "æŸ¥è©¢é‡å¯«è½‰æ›æ‹“å±•",
            "æ··åˆæœå°‹", 
            "æª¢ç´¢å¢å¼·",
            "é‡æ–°æ’åº",
            "ä¸Šä¸‹æ–‡å£“ç¸®éæ¿¾",
            "æ··åˆå¼RAG"
        ]
        
        # æ•ˆèƒ½ç›£æ§ç›¸é—œ
        self.metrics_file = "performance_metrics.json"
        self.max_history = 10000
        self.metrics_history = deque(maxlen=self.max_history)
        self.service_metrics = defaultdict(list)
        self.realtime_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "current_confidence_avg": 0.0,
            "fallback_rate": 0.0
        }
        
        # æ­·å²æ•¸æ“š
        self.metrics_history_display = []
        self.ml_pipeline_metrics = {}
        
        # åˆå§‹åŒ–
        self.load_mock_data()
        self._initialize_ml_pipeline_monitoring()
        self._load_metrics()
    
    def _load_metrics(self):
        """è¼‰å…¥æ­·å²æ•ˆèƒ½æŒ‡æ¨™"""
        try:
            if os.path.exists(self.metrics_file):
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for metric_data in data.get('metrics', []):
                        metric = PerformanceMetrics(
                            timestamp=datetime.fromisoformat(metric_data['timestamp']),
                            query_id=metric_data['query_id'],
                            service_name=metric_data['service_name'],
                            response_time=metric_data['response_time'],
                            success=metric_data['success'],
                            error_message=metric_data.get('error_message'),
                            confidence_score=metric_data.get('confidence_score'),
                            agent_used=metric_data.get('agent_used'),
                            fallback_used=metric_data.get('fallback_used', False),
                            memory_usage=metric_data.get('memory_usage'),
                            cpu_usage=metric_data.get('cpu_usage')
                        )
                        self.metrics_history.append(metric)
                        self.service_metrics[metric.service_name].append(metric)
                
                logger.info(f"âœ… è¼‰å…¥ {len(self.metrics_history)} æ¢æ­·å²æ•ˆèƒ½æŒ‡æ¨™")
        except Exception as e:
            logger.warning(f"âš ï¸ è¼‰å…¥æ­·å²æŒ‡æ¨™å¤±æ•—: {e}")
    
    def _save_metrics(self):
        """å„²å­˜æ•ˆèƒ½æŒ‡æ¨™åˆ°æª”æ¡ˆ"""
        try:
            data = {
                'last_updated': datetime.now().isoformat(),
                'total_metrics': len(self.metrics_history),
                'metrics': [asdict(metric) for metric in self.metrics_history]
            }
            
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ å„²å­˜æ•ˆèƒ½æŒ‡æ¨™å¤±æ•—: {e}")
    
    def record_performance_metric(self, 
                                query_id: str,
                                service_name: str,
                                response_time: float,
                                success: bool,
                                confidence_score: Optional[float] = None,
                                agent_used: Optional[str] = None,
                                fallback_used: bool = False,
                                error_message: Optional[str] = None) -> PerformanceMetrics:
        """è¨˜éŒ„æ•ˆèƒ½æŒ‡æ¨™"""
        metric = PerformanceMetrics(
            timestamp=datetime.now(),
            query_id=query_id,
            service_name=service_name,
            response_time=response_time,
            success=success,
            error_message=error_message,
            confidence_score=confidence_score,
            agent_used=agent_used,
            fallback_used=fallback_used
        )
        
        self.metrics_history.append(metric)
        self.service_metrics[service_name].append(metric)
        self._update_realtime_stats(metric)
        
        # å®šæœŸå„²å­˜
        if len(self.metrics_history) % 100 == 0:
            self._save_metrics()
        
        return metric
    
    def _update_realtime_stats(self, metric: PerformanceMetrics):
        """æ›´æ–°å³æ™‚çµ±è¨ˆæ•¸æ“š"""
        self.realtime_stats["total_queries"] += 1
        
        if metric.success:
            self.realtime_stats["successful_queries"] += 1
        else:
            self.realtime_stats["failed_queries"] += 1
        
        # æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
        if self.metrics_history:
            total_time = sum(m.response_time for m in self.metrics_history)
            self.realtime_stats["average_response_time"] = total_time / len(self.metrics_history)
        
        # æ›´æ–°å¹³å‡ä¿¡å¿ƒå€¼
        confidence_scores = [m.confidence_score for m in self.metrics_history if m.confidence_score is not None]
        if confidence_scores:
            self.realtime_stats["current_confidence_avg"] = statistics.mean(confidence_scores)
        
        # æ›´æ–°å‚™æ´ç‡
        fallback_count = sum(1 for m in self.metrics_history if m.fallback_used)
        self.realtime_stats["fallback_rate"] = fallback_count / len(self.metrics_history) if self.metrics_history else 0.0
    
    def get_service_performance(self, service_name: str, hours: int = 24) -> Dict[str, Any]:
        """ç²å–æœå‹™æ•ˆèƒ½çµ±è¨ˆ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.service_metrics[service_name] if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {
                "total_queries": 0,
                "success_rate": 0.0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "fallback_rate": 0.0
            }
        
        success_count = sum(1 for m in recent_metrics if m.success)
        confidence_scores = [m.confidence_score for m in recent_metrics if m.confidence_score is not None]
        fallback_count = sum(1 for m in recent_metrics if m.fallback_used)
        
        return {
            "total_queries": len(recent_metrics),
            "success_rate": success_count / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in recent_metrics),
            "avg_confidence": statistics.mean(confidence_scores) if confidence_scores else 0.0,
            "fallback_rate": fallback_count / len(recent_metrics)
        }
    
    def get_overall_performance(self, hours: int = 24) -> Dict[str, Any]:
        """ç²å–æ•´é«”æ•ˆèƒ½çµ±è¨ˆ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {
                "total_queries": 0,
                "success_rate": 0.0,
                "avg_response_time": 0.0,
                "avg_confidence": 0.0,
                "fallback_rate": 0.0,
                "service_breakdown": {}
            }
        
        success_count = sum(1 for m in recent_metrics if m.success)
        confidence_scores = [m.confidence_score for m in recent_metrics if m.confidence_score is not None]
        fallback_count = sum(1 for m in recent_metrics if m.fallback_used)
        
        # æœå‹™ç´°åˆ†
        service_breakdown = {}
        for service_name in set(m.service_name for m in recent_metrics):
            service_metrics = [m for m in recent_metrics if m.service_name == service_name]
            service_breakdown[service_name] = {
                "queries": len(service_metrics),
                "success_rate": sum(1 for m in service_metrics if m.success) / len(service_metrics),
                "avg_response_time": statistics.mean(m.response_time for m in service_metrics)
            }
        
        return {
            "total_queries": len(recent_metrics),
            "success_rate": success_count / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in recent_metrics),
            "avg_confidence": statistics.mean(confidence_scores) if confidence_scores else 0.0,
            "fallback_rate": fallback_count / len(recent_metrics),
            "service_breakdown": service_breakdown
        }
    
    def _initialize_ml_pipeline_monitoring(self):
        """åˆå§‹åŒ– ML Pipeline ç›£æ§"""
        try:
            import sys
            import os
            
            # æ·»åŠ  ML Pipeline è·¯å¾‘
            ml_pipeline_path = os.path.join(
                os.path.dirname(__file__), 
                '..', '..', 'ml_pipeline'
            )
            if ml_pipeline_path not in sys.path:
                sys.path.insert(0, ml_pipeline_path)
            
            # å˜—è©¦å°å…¥ ML Pipeline æœå‹™
            try:
                from backend.ml_pipeline.services import RecommendationService
                from backend.ml_pipeline.config.recommender_config import get_recommender_config
                
                # åˆå§‹åŒ–æ¨è–¦æœå‹™ç”¨æ–¼ç›£æ§
                config = get_recommender_config()
                db_url = os.getenv("DATABASE_URL", config.get("database_url", ""))
                
                if db_url:
                    self.ml_pipeline_service = RecommendationService(db_url, config)
                    print("ML Pipeline ç›£æ§åˆå§‹åŒ–æˆåŠŸ")
                else:
                    print("æœªè¨­å®š DATABASE_URLï¼ŒML Pipeline ç›£æ§å°‡ä¸å¯ç”¨")
                    self.ml_pipeline_service = None
                    
            except ImportError:
                print("ML Pipeline æ¨¡çµ„ä¸å¯ç”¨ï¼Œç›£æ§åŠŸèƒ½å°‡å—é™")
                self.ml_pipeline_service = None
                
        except Exception as e:
            print(f"ML Pipeline ç›£æ§åˆå§‹åŒ–å¤±æ•—: {str(e)}")
            self.ml_pipeline_service = None
    
    def load_mock_data(self):
        """è¼‰å…¥æ¨¡æ“¬æ•¸æ“š"""
        # æ¨¡æ“¬æ­·å²æ•¸æ“š
        for i in range(100):
            timestamp = datetime.now() - timedelta(minutes=i)
            self.metrics_history_display.append({
                'timestamp': timestamp,
                'level_1_confidence': 0.85 + (i % 10) * 0.01,
                'level_2_confidence': 0.78 + (i % 8) * 0.015,
                'level_3_confidence': 0.82 + (i % 12) * 0.012,
                'level_4_confidence': 0.88 + (i % 6) * 0.02,
                'level_5_confidence': 0.90 + (i % 4) * 0.025,
                'level_6_confidence': 0.92 + (i % 5) * 0.015,
                'response_time': 2.5 + (i % 15) * 0.1,
                'throughput': 50 + (i % 20) * 2,
                'error_rate': 0.02 + (i % 10) * 0.005,
                'user_satisfaction': 0.85 + (i % 8) * 0.02
            })
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æŒ‡æ¨™"""
        return {
            'level_1_confidence': 0.87,
            'level_2_confidence': 0.81,
            'level_3_confidence': 0.84,
            'level_4_confidence': 0.89,
            'level_5_confidence': 0.91,
            'level_6_confidence': 0.93,
            'response_time': 2.8,
            'throughput': 52,
            'error_rate': 0.025,
            'user_satisfaction': 0.87
        }
    
    def get_service_status(self) -> Dict[str, str]:
        """ç²å–æœå‹™ç‹€æ…‹"""
        status = {
            'RAG Pipeline': 'ğŸŸ¢ é‹è¡Œä¸­',
            'CrewAI': 'ğŸŸ¢ é‹è¡Œä¸­',
            'AnythingLLM': 'ğŸŸ¢ é‹è¡Œä¸­',
            'LLM Service': 'ğŸŸ¢ é‹è¡Œä¸­',
            'Ollama': 'ğŸŸ¢ é‹è¡Œä¸­',
            'PostgreSQL': 'ğŸŸ¢ é‹è¡Œä¸­',
            'MongoDB': 'ğŸŸ¢ é‹è¡Œä¸­',
            'Milvus': 'ğŸŸ¢ é‹è¡Œä¸­',
            'MinIO': 'ğŸŸ¢ é‹è¡Œä¸­'
        }
        
        # æ·»åŠ  ML Pipeline ç‹€æ…‹
        if hasattr(self, 'ml_pipeline_service') and self.ml_pipeline_service:
            try:
                ml_status = self.ml_pipeline_service.get_system_status()
                status['ML Pipeline'] = 'ğŸŸ¢ é‹è¡Œä¸­' if ml_status.get('status') == 'healthy' else 'ğŸŸ¡ è­¦å‘Š'
            except:
                status['ML Pipeline'] = 'ğŸ”´ éŒ¯èª¤'
        else:
            status['ML Pipeline'] = 'âšª æœªé€£æ¥'
        
        return status
    
    def get_ml_pipeline_metrics(self) -> Dict[str, Any]:
        """ç²å– ML Pipeline æŒ‡æ¨™"""
        if not hasattr(self, 'ml_pipeline_service') or not self.ml_pipeline_service:
            return {
                'recommendation_accuracy': 0.0,
                'user_satisfaction': 0.0,
                'diversity_score': 0.0,
                'response_time': 0.0,
                'throughput': 0
            }
        
        try:
            # æ¨¡æ“¬ ML Pipeline æŒ‡æ¨™
            return {
                'recommendation_accuracy': 0.85,
                'user_satisfaction': 0.87,
                'diversity_score': 0.78,
                'response_time': 1.2,
                'throughput': 45
            }
        except Exception as e:
            print(f"ç²å– ML Pipeline æŒ‡æ¨™å¤±æ•—: {str(e)}")
            return {
                'recommendation_accuracy': 0.0,
                'user_satisfaction': 0.0,
                'diversity_score': 0.0,
                'response_time': 0.0,
                'throughput': 0
            }

def main():
    """ä¸»å‡½æ•¸"""
    st.title("ğŸŒ³ å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG ç³»çµ±ç›£æ§é¢æ¿")
    st.markdown("---")
    
    # åˆå§‹åŒ–ç›£æ§å™¨
    monitor = HierarchicalRAGMonitor()
    
    # å´é‚Šæ¬„é…ç½®
    st.sidebar.title("ğŸ›ï¸ ç›£æ§é…ç½®")
    
    # æ™‚é–“ç¯„åœé¸æ“‡
    time_range = st.sidebar.selectbox(
        "æ™‚é–“ç¯„åœ",
        ["æœ€è¿‘1å°æ™‚", "æœ€è¿‘6å°æ™‚", "æœ€è¿‘24å°æ™‚", "æœ€è¿‘7å¤©"]
    )
    
    # æŒ‡æ¨™é¸æ“‡
    selected_metrics = st.sidebar.multiselect(
        "ç›£æ§æŒ‡æ¨™",
        ["ä¿¡å¿ƒå€¼", "å›æ‡‰æ™‚é–“", "ååé‡", "éŒ¯èª¤ç‡", "ç”¨æˆ¶æ»¿æ„åº¦"],
        default=["ä¿¡å¿ƒå€¼", "å›æ‡‰æ™‚é–“"]
    )
    
    # è‡ªå‹•åˆ·æ–°
    auto_refresh = st.sidebar.checkbox("è‡ªå‹•åˆ·æ–°", value=True)
    if auto_refresh:
        time.sleep(1)
        st.rerun()
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“Š ç³»çµ±æ•ˆèƒ½æŒ‡æ¨™")
        
        # ç²å–æ•´é«”æ•ˆèƒ½çµ±è¨ˆ
        overall_perf = monitor.get_overall_performance(hours=24)
        
        # æ•ˆèƒ½æŒ‡æ¨™å¡ç‰‡
        perf_col1, perf_col2, perf_col3, perf_col4 = st.columns(4)
        
        with perf_col1:
            st.metric(
                "ç¸½æŸ¥è©¢æ•¸",
                overall_perf["total_queries"],
                delta=f"{overall_perf['success_rate']:.1%} æˆåŠŸç‡"
            )
        
        with perf_col2:
            st.metric(
                "å¹³å‡å›æ‡‰æ™‚é–“",
                f"{overall_perf['avg_response_time']:.2f}s",
                delta=f"{overall_perf['fallback_rate']:.1%} å‚™æ´ç‡"
            )
        
        with perf_col3:
            st.metric(
                "å¹³å‡ä¿¡å¿ƒå€¼",
                f"{overall_perf['avg_confidence']:.2f}",
                delta="ä¿¡å¿ƒå€¼"
            )
        
        with perf_col4:
            st.metric(
                "å³æ™‚æŸ¥è©¢",
                monitor.realtime_stats["total_queries"],
                delta="æ´»èºæŸ¥è©¢"
            )
    
    with col2:
        st.subheader("ğŸ”§ æœå‹™ç‹€æ…‹")
        service_status = monitor.get_service_status()
        
        for service, status in service_status.items():
            st.write(f"{service}: {status}")
    
    # åœ–è¡¨å€åŸŸ
    st.subheader("ğŸ“ˆ æ•ˆèƒ½è¶¨å‹¢åœ–")
    
    # è½‰æ›æ­·å²æ•¸æ“šç‚º DataFrame
    if monitor.metrics_history_display:
        df = pd.DataFrame(monitor.metrics_history_display)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # ä¿¡å¿ƒå€¼è¶¨å‹¢åœ–
        if "ä¿¡å¿ƒå€¼" in selected_metrics:
            fig_confidence = go.Figure()
            for i, level in enumerate(monitor.levels, 1):
                col_name = f'level_{i}_confidence'
                if col_name in df.columns:
                    fig_confidence.add_trace(go.Scatter(
                        x=df['timestamp'],
                        y=df[col_name],
                        mode='lines',
                        name=level,
                        line=dict(width=2)
                    ))
            
            fig_confidence.update_layout(
                title="å„å±¤ç´šä¿¡å¿ƒå€¼è¶¨å‹¢",
                xaxis_title="æ™‚é–“",
                yaxis_title="ä¿¡å¿ƒå€¼",
                height=400
            )
            st.plotly_chart(fig_confidence, use_container_width=True)
        
        # å›æ‡‰æ™‚é–“è¶¨å‹¢åœ–
        if "å›æ‡‰æ™‚é–“" in selected_metrics:
            fig_response = px.line(
                df, 
                x='timestamp', 
                y='response_time',
                title="å›æ‡‰æ™‚é–“è¶¨å‹¢"
            )
            fig_response.update_layout(height=400)
            st.plotly_chart(fig_response, use_container_width=True)
    
    # æœå‹™æ•ˆèƒ½ç´°åˆ†
    st.subheader("ğŸ” æœå‹™æ•ˆèƒ½ç´°åˆ†")
    service_perf = overall_perf.get("service_breakdown", {})
    
    if service_perf:
        service_df = pd.DataFrame([
            {
                "æœå‹™": service,
                "æŸ¥è©¢æ•¸": data["queries"],
                "æˆåŠŸç‡": data["success_rate"],
                "å¹³å‡å›æ‡‰æ™‚é–“": data["avg_response_time"]
            }
            for service, data in service_perf.items()
        ])
        
        st.dataframe(service_df, use_container_width=True)
    
    # ML Pipeline æŒ‡æ¨™
    st.subheader("ğŸ¤– ML Pipeline æŒ‡æ¨™")
    ml_metrics = monitor.get_ml_pipeline_metrics()
    
    ml_col1, ml_col2, ml_col3, ml_col4 = st.columns(4)
    
    with ml_col1:
        st.metric("æ¨è–¦æº–ç¢ºç‡", f"{ml_metrics['recommendation_accuracy']:.2f}")
    
    with ml_col2:
        st.metric("ç”¨æˆ¶æ»¿æ„åº¦", f"{ml_metrics['user_satisfaction']:.2f}")
    
    with ml_col3:
        st.metric("å¤šæ¨£æ€§åˆ†æ•¸", f"{ml_metrics['diversity_score']:.2f}")
    
    with ml_col4:
        st.metric("ML å›æ‡‰æ™‚é–“", f"{ml_metrics['response_time']:.2f}s")

if __name__ == "__main__":
    main() 