#!/usr/bin/env python3
"""
æ•ˆèƒ½ç›£æ§æ¨¡çµ„
è¿½è¹¤ RAG Pipeline çš„å„ç¨®æ•ˆèƒ½æŒ‡æ¨™ï¼ŒåŒ…å«éŸ¿æ‡‰æ™‚é–“ã€æˆåŠŸç‡ã€è³‡æºä½¿ç”¨ç­‰
"""

import time
import logging
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from collections import defaultdict, deque
import statistics
import json
import os

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ•ˆèƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    timestamp: datetime
    query_id: str
    service_name: str
    response_time: float
    success: bool
    error_message: Optional[str] = None
    confidence_score: Optional[float] = None
    agent_used: Optional[str] = None
    fallback_used: bool = False
    memory_usage: Optional[float] = None
    cpu_usage: Optional[float] = None

class PerformanceMonitor:
    """
    æ•ˆèƒ½ç›£æ§å™¨
    è¿½è¹¤å’Œè¨˜éŒ„ç³»çµ±çš„å„ç¨®æ•ˆèƒ½æŒ‡æ¨™
    """
    
    def __init__(self, 
                 metrics_file: str = "performance_metrics.json",
                 max_history: int = 10000,
                 enable_realtime: bool = True):
        """
        åˆå§‹åŒ–æ•ˆèƒ½ç›£æ§å™¨
        
        Args:
            metrics_file: æŒ‡æ¨™å„²å­˜æª”æ¡ˆè·¯å¾‘
            max_history: æœ€å¤§æ­·å²è¨˜éŒ„æ•¸é‡
            enable_realtime: æ˜¯å¦å•Ÿç”¨å³æ™‚ç›£æ§
        """
        self.metrics_file = metrics_file
        self.max_history = max_history
        self.enable_realtime = enable_realtime
        
        # æ•ˆèƒ½æŒ‡æ¨™å„²å­˜
        self.metrics_history = deque(maxlen=max_history)
        self.service_metrics = defaultdict(list)
        self.realtime_stats = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "average_response_time": 0.0,
            "current_confidence_avg": 0.0,
            "fallback_rate": 0.0
        }
        
        # è¼‰å…¥æ­·å²æ•¸æ“š
        self._load_metrics()
        
        logger.info("âœ… æ•ˆèƒ½ç›£æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_metrics(self):
        """è¼‰å…¥æ­·å²æ•ˆèƒ½æŒ‡æ¨™"""
        try:
            if os.path.exists(self.metrics_file):
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for metric_data in data.get('metrics', []):
                        metric = PerformanceMetrics(
                            timestamp=datetime.fromisoformat(metric_data['timestamp']),
                            query_id=metric_data['query_id'],
                            service_name=metric_data['service_name'],
                            response_time=metric_data['response_time'],
                            success=metric_data['success'],
                            error_message=metric_data.get('error_message'),
                            confidence_score=metric_data.get('confidence_score'),
                            agent_used=metric_data.get('agent_used'),
                            fallback_used=metric_data.get('fallback_used', False),
                            memory_usage=metric_data.get('memory_usage'),
                            cpu_usage=metric_data.get('cpu_usage')
                        )
                        self.metrics_history.append(metric)
                        self.service_metrics[metric.service_name].append(metric)
                
                logger.info(f"âœ… è¼‰å…¥ {len(self.metrics_history)} æ¢æ­·å²æ•ˆèƒ½æŒ‡æ¨™")
        except Exception as e:
            logger.warning(f"âš ï¸ è¼‰å…¥æ­·å²æŒ‡æ¨™å¤±æ•—: {e}")
    
    def _save_metrics(self):
        """å„²å­˜æ•ˆèƒ½æŒ‡æ¨™åˆ°æª”æ¡ˆ"""
        try:
            data = {
                'last_updated': datetime.now().isoformat(),
                'total_metrics': len(self.metrics_history),
                'metrics': [asdict(metric) for metric in self.metrics_history]
            }
            
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"âŒ å„²å­˜æ•ˆèƒ½æŒ‡æ¨™å¤±æ•—: {e}")
    
    def start_query_timer(self, query_id: str, service_name: str) -> float:
        """
        é–‹å§‹æŸ¥è©¢è¨ˆæ™‚å™¨
        
        Args:
            query_id: æŸ¥è©¢ ID
            service_name: æœå‹™åç¨±
            
        Returns:
            é–‹å§‹æ™‚é–“æˆ³è¨˜
        """
        return time.time()
    
    def end_query_timer(self, 
                       start_time: float,
                       query_id: str,
                       service_name: str,
                       success: bool,
                       confidence_score: Optional[float] = None,
                       agent_used: Optional[str] = None,
                       fallback_used: bool = False,
                       error_message: Optional[str] = None) -> PerformanceMetrics:
        """
        çµæŸæŸ¥è©¢è¨ˆæ™‚å™¨ä¸¦è¨˜éŒ„æŒ‡æ¨™
        
        Args:
            start_time: é–‹å§‹æ™‚é–“æˆ³è¨˜
            query_id: æŸ¥è©¢ ID
            service_name: æœå‹™åç¨±
            success: æ˜¯å¦æˆåŠŸ
            confidence_score: ä¿¡å¿ƒå€¼åˆ†æ•¸
            agent_used: ä½¿ç”¨çš„ä»£ç†
            fallback_used: æ˜¯å¦ä½¿ç”¨å‚™æ´
            error_message: éŒ¯èª¤è¨Šæ¯
            
        Returns:
            æ•ˆèƒ½æŒ‡æ¨™ç‰©ä»¶
        """
        response_time = time.time() - start_time
        
        # å‰µå»ºæ•ˆèƒ½æŒ‡æ¨™
        metric = PerformanceMetrics(
            timestamp=datetime.now(),
            query_id=query_id,
            service_name=service_name,
            response_time=response_time,
            success=success,
            error_message=error_message,
            confidence_score=confidence_score,
            agent_used=agent_used,
            fallback_used=fallback_used
        )
        
        # å„²å­˜æŒ‡æ¨™
        self.metrics_history.append(metric)
        self.service_metrics[service_name].append(metric)
        
        # æ›´æ–°å³æ™‚çµ±è¨ˆ
        self._update_realtime_stats(metric)
        
        # å®šæœŸå„²å­˜åˆ°æª”æ¡ˆ
        if len(self.metrics_history) % 100 == 0:
            self._save_metrics()
        
        logger.info(f"ğŸ“Š æ•ˆèƒ½æŒ‡æ¨™è¨˜éŒ„: {service_name} - {response_time:.3f}s - {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        return metric
    
    def _update_realtime_stats(self, metric: PerformanceMetrics):
        """æ›´æ–°å³æ™‚çµ±è¨ˆæ•¸æ“š"""
        self.realtime_stats["total_queries"] += 1
        
        if metric.success:
            self.realtime_stats["successful_queries"] += 1
        else:
            self.realtime_stats["failed_queries"] += 1
        
        # æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
        total_time = sum(m.response_time for m in self.metrics_history)
        self.realtime_stats["average_response_time"] = total_time / len(self.metrics_history)
        
        # æ›´æ–°å¹³å‡ä¿¡å¿ƒå€¼
        confidence_scores = [m.confidence_score for m in self.metrics_history if m.confidence_score is not None]
        if confidence_scores:
            self.realtime_stats["current_confidence_avg"] = statistics.mean(confidence_scores)
        
        # æ›´æ–°å‚™æ´ç‡
        fallback_count = sum(1 for m in self.metrics_history if m.fallback_used)
        self.realtime_stats["fallback_rate"] = fallback_count / len(self.metrics_history)
    
    def get_service_performance(self, service_name: str, hours: int = 24) -> Dict[str, Any]:
        """
        ç²å–ç‰¹å®šæœå‹™çš„æ•ˆèƒ½çµ±è¨ˆ
        
        Args:
            service_name: æœå‹™åç¨±
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            
        Returns:
            æ•ˆèƒ½çµ±è¨ˆæ•¸æ“š
        """
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.service_metrics[service_name]
            if m.timestamp >= cutoff_time
        ]
        
        if not recent_metrics:
            return {
                "service_name": service_name,
                "total_queries": 0,
                "success_rate": 0.0,
                "average_response_time": 0.0,
                "error_count": 0
            }
        
        success_count = sum(1 for m in recent_metrics if m.success)
        response_times = [m.response_time for m in recent_metrics]
        
        return {
            "service_name": service_name,
            "total_queries": len(recent_metrics),
            "success_rate": success_count / len(recent_metrics),
            "average_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "error_count": len(recent_metrics) - success_count,
            "confidence_avg": statistics.mean([m.confidence_score for m in recent_metrics if m.confidence_score is not None])
        }
    
    def get_overall_performance(self, hours: int = 24) -> Dict[str, Any]:
        """
        ç²å–æ•´é«”æ•ˆèƒ½çµ±è¨ˆ
        
        Args:
            hours: çµ±è¨ˆæ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰
            
        Returns:
            æ•´é«”æ•ˆèƒ½çµ±è¨ˆæ•¸æ“š
        """
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_metrics = [
            m for m in self.metrics_history
            if m.timestamp >= cutoff_time
        ]
        
        if not recent_metrics:
            return {
                "total_queries": 0,
                "success_rate": 0.0,
                "average_response_time": 0.0,
                "service_breakdown": {}
            }
        
        # æ•´é«”çµ±è¨ˆ
        success_count = sum(1 for m in recent_metrics if m.success)
        response_times = [m.response_time for m in recent_metrics]
        
        # æœå‹™åˆ†è§£çµ±è¨ˆ
        service_breakdown = {}
        for service_name in set(m.service_name for m in recent_metrics):
            service_breakdown[service_name] = self.get_service_performance(service_name, hours)
        
        return {
            "total_queries": len(recent_metrics),
            "success_rate": success_count / len(recent_metrics),
            "average_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "fallback_rate": sum(1 for m in recent_metrics if m.fallback_used) / len(recent_metrics),
            "confidence_avg": statistics.mean([m.confidence_score for m in recent_metrics if m.confidence_score is not None]),
            "service_breakdown": service_breakdown
        }
    
    def get_performance_alerts(self) -> List[Dict[str, Any]]:
        """
        ç²å–æ•ˆèƒ½è­¦å ±
        
        Returns:
            è­¦å ±åˆ—è¡¨
        """
        alerts = []
        
        # æª¢æŸ¥æœ€è¿‘ 1 å°æ™‚çš„æ•ˆèƒ½
        recent_performance = self.get_overall_performance(hours=1)
        
        # éŸ¿æ‡‰æ™‚é–“è­¦å ±
        if recent_performance["average_response_time"] > 5.0:  # è¶…é 5 ç§’
            alerts.append({
                "type": "high_response_time",
                "severity": "warning",
                "message": f"å¹³å‡éŸ¿æ‡‰æ™‚é–“éé«˜: {recent_performance['average_response_time']:.2f}ç§’",
                "timestamp": datetime.now().isoformat()
            })
        
        # æˆåŠŸç‡è­¦å ±
        if recent_performance["success_rate"] < 0.8:  # ä½æ–¼ 80%
            alerts.append({
                "type": "low_success_rate",
                "severity": "error",
                "message": f"æˆåŠŸç‡éä½: {recent_performance['success_rate']:.2%}",
                "timestamp": datetime.now().isoformat()
            })
        
        # å‚™æ´ç‡è­¦å ±
        if recent_performance["fallback_rate"] > 0.3:  # è¶…é 30%
            alerts.append({
                "type": "high_fallback_rate",
                "severity": "warning",
                "message": f"å‚™æ´ç‡éé«˜: {recent_performance['fallback_rate']:.2%}",
                "timestamp": datetime.now().isoformat()
            })
        
        return alerts
    
    def export_metrics(self, format: str = "json", filepath: Optional[str] = None) -> str:
        """
        åŒ¯å‡ºæ•ˆèƒ½æŒ‡æ¨™
        
        Args:
            format: åŒ¯å‡ºæ ¼å¼ (json/csv)
            filepath: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åŒ¯å‡ºæª”æ¡ˆè·¯å¾‘
        """
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"performance_metrics_{timestamp}.{format}"
        
        if format.lower() == "json":
            data = {
                'export_timestamp': datetime.now().isoformat(),
                'metrics': [asdict(metric) for metric in self.metrics_history]
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        elif format.lower() == "csv":
            import csv
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                # å¯«å…¥æ¨™é¡Œ
                writer.writerow([
                    'timestamp', 'query_id', 'service_name', 'response_time',
                    'success', 'confidence_score', 'agent_used', 'fallback_used'
                ])
                # å¯«å…¥æ•¸æ“š
                for metric in self.metrics_history:
                    writer.writerow([
                        metric.timestamp.isoformat(),
                        metric.query_id,
                        metric.service_name,
                        metric.response_time,
                        metric.success,
                        metric.confidence_score,
                        metric.agent_used,
                        metric.fallback_used
                    ])
        
        logger.info(f"âœ… æ•ˆèƒ½æŒ‡æ¨™å·²åŒ¯å‡ºåˆ°: {filepath}")
        return filepath
    
    def cleanup_old_metrics(self, days: int = 30):
        """
        æ¸…ç†èˆŠçš„æ•ˆèƒ½æŒ‡æ¨™
        
        Args:
            days: ä¿ç•™å¤©æ•¸
        """
        cutoff_time = datetime.now() - timedelta(days=days)
        original_count = len(self.metrics_history)
        
        # éæ¿¾èˆŠæŒ‡æ¨™
        self.metrics_history = deque(
            [m for m in self.metrics_history if m.timestamp >= cutoff_time],
            maxlen=self.max_history
        )
        
        # é‡æ–°æ•´ç†æœå‹™æŒ‡æ¨™
        self.service_metrics.clear()
        for metric in self.metrics_history:
            self.service_metrics[metric.service_name].append(metric)
        
        cleaned_count = original_count - len(self.metrics_history)
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} æ¢èˆŠæ•ˆèƒ½æŒ‡æ¨™")
        
        # å„²å­˜æ›´æ–°å¾Œçš„æŒ‡æ¨™
        self._save_metrics() 