#!/usr/bin/env python3
"""
æª¢æŸ¥ Ollama æ¨¡å‹ç‹€æ…‹è…³æœ¬

æª¢æŸ¥ Ollama æœå‹™ä¸­çš„æ¨¡å‹å¯ç”¨æ€§ï¼Œä¸¦æä¾›ä¿®å¾©å»ºè­°
"""

import os
import asyncio
import aiohttp
import json
import logging
from typing import List, Dict, Any

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OllamaModelChecker:
    """Ollama æ¨¡å‹æª¢æŸ¥å™¨"""
    
    def __init__(self, ollama_host: str = "http://localhost:11434"):
        self.ollama_host = ollama_host
        self.required_models = {
            "llm": [
                "qwen2.5-taiwan-7b-instruct",
                "qwen2.5:8b", 
                "qwen3:8b",
                "gpt-3.5-turbo"
            ],
            "embedding": [
                "bge-m3",
                "nomic-embed-text",
                "all-minilm"
            ]
        }
    
    async def check_ollama_service(self) -> bool:
        """æª¢æŸ¥ Ollama æœå‹™ç‹€æ…‹"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_host}/api/tags") as response:
                    if response.status == 200:
                        logger.info("âœ… Ollama æœå‹™æ­£å¸¸é‹è¡Œ")
                        return True
                    else:
                        logger.error(f"âŒ Ollama æœå‹™ç•°å¸¸: {response.status}")
                        return False
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™: {e}")
            return False
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.ollama_host}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("models", [])
                    else:
                        return []
        except Exception as e:
            logger.error(f"ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    async def check_model(self, model_name: str) -> Dict[str, Any]:
        """æª¢æŸ¥å–®å€‹æ¨¡å‹ç‹€æ…‹"""
        try:
            async with aiohttp.ClientSession() as session:
                # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
                async with session.post(
                    f"{self.ollama_host}/api/show",
                    json={"name": model_name}
                ) as response:
                    if response.status == 200:
                        model_info = await response.json()
                        return {
                            "name": model_name,
                            "status": "available",
                            "info": model_info
                        }
                    else:
                        return {
                            "name": model_name,
                            "status": "not_found",
                            "error": f"HTTP {response.status}"
                        }
        except Exception as e:
            return {
                "name": model_name,
                "status": "error",
                "error": str(e)
            }
    
    async def test_embedding_model(self, model_name: str) -> Dict[str, Any]:
        """æ¸¬è©¦åµŒå…¥æ¨¡å‹"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_host}/api/embeddings",
                    json={
                        "model": model_name,
                        "prompt": "æ¸¬è©¦æ–‡æœ¬"
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        embedding = data.get("embedding", [])
                        return {
                            "name": model_name,
                            "status": "working",
                            "dimension": len(embedding)
                        }
                    else:
                        return {
                            "name": model_name,
                            "status": "failed",
                            "error": f"HTTP {response.status}"
                        }
        except Exception as e:
            return {
                "name": model_name,
                "status": "error",
                "error": str(e)
            }
    
    async def test_llm_model(self, model_name: str) -> Dict[str, Any]:
        """æ¸¬è©¦ LLM æ¨¡å‹"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.ollama_host}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": "ä½ å¥½",
                        "stream": False
                    },
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        response_text = data.get("response", "")
                        return {
                            "name": model_name,
                            "status": "working",
                            "response_length": len(response_text)
                        }
                    else:
                        return {
                            "name": model_name,
                            "status": "failed",
                            "error": f"HTTP {response.status}"
                        }
        except Exception as e:
            return {
                "name": model_name,
                "status": "error",
                "error": str(e)
            }
    
    async def run_full_check(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´æª¢æŸ¥"""
        logger.info("ğŸ” é–‹å§‹æª¢æŸ¥ Ollama æ¨¡å‹ç‹€æ…‹...")
        
        # æª¢æŸ¥æœå‹™ç‹€æ…‹
        service_ok = await self.check_ollama_service()
        if not service_ok:
            return {
                "service_status": "failed",
                "message": "Ollama æœå‹™ä¸å¯ç”¨"
            }
        
        # ç²å–å¯ç”¨æ¨¡å‹
        available_models = await self.get_available_models()
        available_model_names = [m.get("name", "") for m in available_models]
        
        logger.info(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {available_model_names}")
        
        # æª¢æŸ¥ LLM æ¨¡å‹
        llm_results = []
        for model in self.required_models["llm"]:
            if model in available_model_names:
                result = await self.test_llm_model(model)
            else:
                result = {"name": model, "status": "not_installed"}
            llm_results.append(result)
        
        # æª¢æŸ¥åµŒå…¥æ¨¡å‹
        embedding_results = []
        for model in self.required_models["embedding"]:
            if model in available_model_names:
                result = await self.test_embedding_model(model)
            else:
                result = {"name": model, "status": "not_installed"}
            embedding_results.append(result)
        
        return {
            "service_status": "ok",
            "available_models": available_model_names,
            "llm_models": llm_results,
            "embedding_models": embedding_results
        }
    
    def generate_fix_commands(self, check_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¿®å¾©å‘½ä»¤"""
        commands = []
        
        # æª¢æŸ¥ LLM æ¨¡å‹
        for model_result in check_results.get("llm_models", []):
            if model_result.get("status") == "not_installed":
                model_name = model_result.get("name")
                if model_name == "qwen2.5-taiwan-7b-instruct":
                    commands.append(f"# ä¸‹è¼‰å°ç£ç‰ˆæœ¬æ¨¡å‹")
                    commands.append(f"ollama pull weiren119/Qwen2.5-Taiwan-8B-Instruct")
                elif model_name == "qwen2.5:8b":
                    commands.append(f"# ä¸‹è¼‰ Qwen2.5 æ¨¡å‹")
                    commands.append(f"ollama pull Qwen/Qwen2.5-8B-Instruct")
                elif model_name == "qwen3:8b":
                    commands.append(f"# ä¸‹è¼‰ Qwen3 æ¨¡å‹")
                    commands.append(f"ollama pull qwen2.5:8b")
                elif model_name == "gpt-3.5-turbo":
                    commands.append(f"# ä¸‹è¼‰ GPT-3.5 æ¨¡å‹")
                    commands.append(f"ollama pull gpt-3.5-turbo")
        
        # æª¢æŸ¥åµŒå…¥æ¨¡å‹
        for model_result in check_results.get("embedding_models", []):
            if model_result.get("status") == "not_installed":
                model_name = model_result.get("name")
                if model_name == "bge-m3":
                    commands.append(f"# ä¸‹è¼‰ BGE-M3 åµŒå…¥æ¨¡å‹")
                    commands.append(f"ollama pull BAAI/bge-m3")
                elif model_name == "nomic-embed-text":
                    commands.append(f"# ä¸‹è¼‰ Nomic åµŒå…¥æ¨¡å‹")
                    commands.append(f"ollama pull nomic-embed-text")
                elif model_name == "all-minilm":
                    commands.append(f"# ä¸‹è¼‰ All-MiniLM åµŒå…¥æ¨¡å‹")
                    commands.append(f"ollama pull all-minilm")
        
        return commands


async def main():
    """ä¸»å‡½æ•¸"""
    # å¾ç’°å¢ƒè®Šæ•¸ç²å– Ollama ä¸»æ©Ÿ
    ollama_host = os.getenv("OLLAMA_HOST", "http://localhost:11434")
    
    checker = OllamaModelChecker(ollama_host)
    
    # åŸ·è¡Œæª¢æŸ¥
    results = await checker.run_full_check()
    
    # è¼¸å‡ºçµæœ
    print("\n" + "="*60)
    print("ğŸ” OLLAMA æ¨¡å‹æª¢æŸ¥çµæœ")
    print("="*60)
    
    if results["service_status"] == "failed":
        print("âŒ Ollama æœå‹™ä¸å¯ç”¨")
        print("è«‹æª¢æŸ¥:")
        print("1. Ollama æœå‹™æ˜¯å¦å•Ÿå‹•")
        print("2. ç¶²è·¯é€£æ¥æ˜¯å¦æ­£å¸¸")
        print("3. ç«¯å£æ˜¯å¦æ­£ç¢º")
        return
    
    print(f"âœ… Ollama æœå‹™æ­£å¸¸")
    print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹æ•¸é‡: {len(results['available_models'])}")
    
    # LLM æ¨¡å‹ç‹€æ…‹
    print("\nğŸ¤– LLM æ¨¡å‹ç‹€æ…‹:")
    for model in results["llm_models"]:
        status_icon = "âœ…" if model["status"] == "working" else "âŒ"
        print(f"  {status_icon} {model['name']}: {model['status']}")
        if model.get("error"):
            print(f"     éŒ¯èª¤: {model['error']}")
    
    # åµŒå…¥æ¨¡å‹ç‹€æ…‹
    print("\nğŸ”¢ åµŒå…¥æ¨¡å‹ç‹€æ…‹:")
    for model in results["embedding_models"]:
        status_icon = "âœ…" if model["status"] == "working" else "âŒ"
        dimension_info = f" ({model['dimension']}ç¶­)" if model.get("dimension") else ""
        print(f"  {status_icon} {model['name']}: {model['status']}{dimension_info}")
        if model.get("error"):
            print(f"     éŒ¯èª¤: {model['error']}")
    
    # ç”Ÿæˆä¿®å¾©å‘½ä»¤
    fix_commands = checker.generate_fix_commands(results)
    if fix_commands:
        print("\nğŸ”§ ä¿®å¾©å‘½ä»¤:")
        print("åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¾†å®‰è£ç¼ºå¤±çš„æ¨¡å‹:")
        for cmd in fix_commands:
            print(f"  {cmd}")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    asyncio.run(main()) 