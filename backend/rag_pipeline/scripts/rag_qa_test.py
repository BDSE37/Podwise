#!/usr/bin/env python3
"""
RAG å•ç­”æ¸¬è©¦ç³»çµ±
æ•´åˆéŸ³æª”è½‰éŒ„ç®¡ç·šå’Œå‰ç«¯å•ç­”ç³»çµ±
ä½¿ç”¨ PodWise é è¨­å•é¡Œåˆ—è¡¨é€²è¡Œæ¸¬è©¦
"""

import os
import sys
import json
import logging
import asyncio
import requests
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from pymongo import MongoClient
    from pymongo.errors import ConnectionFailure
except ImportError:
    print("è«‹å®‰è£ pymongo: pip install pymongo")
    sys.exit(1)

try:
    from pymilvus import connections, Collection, utility
except ImportError:
    print("è«‹å®‰è£ pymilvus: pip install pymilvus")
    sys.exit(1)

try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("è«‹å®‰è£ sentence-transformers: pip install sentence-transformers")
    sys.exit(1)

from audio_transcription_pipeline import AudioTranscriptionPipeline

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RAGQATestSystem:
    """RAG å•ç­”æ¸¬è©¦ç³»çµ±"""
    
    def __init__(self, 
                 mongodb_uri: str = "mongodb://localhost:27017/",
                 mongodb_db: str = "podwise",
                 mongodb_collection: str = "podcast",
                 milvus_host: str = "localhost",
                 milvus_port: str = "19530",
                 embedding_model: str = "BAAI/bge-m3",
                 rag_service_url: str = "http://localhost:8004"):
        """
        åˆå§‹åŒ– RAG å•ç­”æ¸¬è©¦ç³»çµ±
        
        Args:
            mongodb_uri: MongoDB é€£æ¥ URI
            mongodb_db: MongoDB è³‡æ–™åº«åç¨±
            mongodb_collection: MongoDB é›†åˆåç¨±
            milvus_host: Milvus ä¸»æ©Ÿ
            milvus_port: Milvus ç«¯å£
            embedding_model: åµŒå…¥æ¨¡å‹åç¨±
            rag_service_url: RAG æœå‹™ URL
        """
        self.mongodb_uri = mongodb_uri
        self.mongodb_db = mongodb_db
        self.mongodb_collection = mongodb_collection
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self.embedding_model = embedding_model
        self.rag_service_url = rag_service_url
        
        # åˆå§‹åŒ–é€£æ¥
        self.mongo_client = None
        self.mongo_db = None
        self.mongo_collection = None
        self.milvus_connected = False
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedding_model_instance = None
        
        # é€£æ¥è³‡æ–™åº«
        self.connect_databases()
        self.load_embedding_model()
        
        # é è¨­æ¸¬è©¦å•é¡Œ
        self.default_test_questions = [
            "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ",
            "å¦‚ä½•æå‡å·¥ä½œæ•ˆç‡ï¼Ÿ",
            "å‰µæ¥­éœ€è¦æ³¨æ„ä»€éº¼ï¼Ÿ",
            "æ™‚é–“ç®¡ç†çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ",
            "é ˜å°åŠ›çš„é‡è¦æ€§æ˜¯ä»€éº¼ï¼Ÿ",
            "å°ç£åŠå°é«”ç”¢æ¥­çš„ç™¼å±•å¦‚ä½•ï¼Ÿ",
            "é›²ç«¯é‹ç®—æœ‰ä»€éº¼å„ªå‹¢ï¼Ÿ",
            "å€å¡ŠéˆæŠ€è¡“æœ‰å“ªäº›æ‡‰ç”¨ï¼Ÿ",
            "æ•™è‚²å‰µæ–°èˆ‡å­¸ç¿’æœ‰ä»€éº¼é—œä¿‚ï¼Ÿ",
            "å•†æ¥­æ¨¡å¼å‰µæ–°æœ‰å“ªäº›æ–¹å¼ï¼Ÿ",
            "å¦‚ä½•é€²è¡ŒæŠ•è³‡ç†è²¡ï¼Ÿ",
            "å¥åº·ç”Ÿæ´»çš„é‡è¦æ€§æ˜¯ä»€éº¼ï¼Ÿ",
            "ç§‘æŠ€å°ç”Ÿæ´»çš„å½±éŸ¿æœ‰å“ªäº›ï¼Ÿ",
            "å¦‚ä½•åŸ¹é¤Šè‰¯å¥½çš„å­¸ç¿’ç¿’æ…£ï¼Ÿ",
            "ä¼æ¥­ç®¡ç†çš„æœ€ä½³å¯¦è¸æ˜¯ä»€éº¼ï¼Ÿ"
        ]
    
    def connect_databases(self):
        """é€£æ¥è³‡æ–™åº«"""
        # é€£æ¥ MongoDB
        try:
            self.mongo_client = MongoClient(self.mongodb_uri)
            self.mongo_db = self.mongo_client[self.mongodb_db]
            self.mongo_collection = self.mongo_db[self.mongodb_collection]
            
            # æ¸¬è©¦é€£æ¥
            self.mongo_client.admin.command('ping')
            logger.info("âœ… MongoDB é€£æ¥æˆåŠŸ")
            
        except ConnectionFailure as e:
            logger.error(f"âŒ MongoDB é€£æ¥å¤±æ•—: {e}")
            raise
        
        # é€£æ¥ Milvus
        try:
            connections.connect(
                alias="default",
                host=self.milvus_host,
                port=self.milvus_port
            )
            self.milvus_connected = True
            logger.info("âœ… Milvus é€£æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Milvus é€£æ¥å¤±æ•—: {e}")
            raise
    
    def load_embedding_model(self):
        """è¼‰å…¥åµŒå…¥æ¨¡å‹"""
        try:
            logger.info(f"ğŸ”§ è¼‰å…¥åµŒå…¥æ¨¡å‹: {self.embedding_model}")
            self.embedding_model_instance = SentenceTransformer(self.embedding_model)
            logger.info("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥åµŒå…¥æ¨¡å‹å¤±æ•—: {e}")
            raise
    
    def load_test_questions(self, questions_file: str = "PodWise é è¨­å•é¡Œåˆ—è¡¨.xlsx") -> List[str]:
        """
        è¼‰å…¥æ¸¬è©¦å•é¡Œ
        
        Args:
            questions_file: å•é¡Œæª”æ¡ˆè·¯å¾‘
            
        Returns:
            å•é¡Œåˆ—è¡¨
        """
        questions = []
        
        try:
            # å˜—è©¦è®€å– Excel æª”æ¡ˆ
            import pandas as pd
            
            file_path = Path(questions_file)
            if file_path.exists():
                df = pd.read_excel(file_path)
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥å•é¡Œæª”æ¡ˆ: {questions_file}")
                
                # æ ¹æ“šå¯¦éš›æ¬„ä½åç¨±èª¿æ•´
                if 'question' in df.columns:
                    questions = df['question'].dropna().tolist()
                elif 'å•é¡Œ' in df.columns:
                    questions = df['å•é¡Œ'].dropna().tolist()
                elif 'content' in df.columns:
                    questions = df['content'].dropna().tolist()
                else:
                    # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ¨™æº–æ¬„ä½ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
                    questions = df.iloc[:, 0].dropna().tolist()
                
                logger.info(f"ğŸ“ è¼‰å…¥ {len(questions)} å€‹æ¸¬è©¦å•é¡Œ")
            else:
                logger.warning(f"âš ï¸ å•é¡Œæª”æ¡ˆä¸å­˜åœ¨: {questions_file}ï¼Œä½¿ç”¨é è¨­å•é¡Œ")
                questions = self.default_test_questions
                
        except Exception as e:
            logger.warning(f"âš ï¸ è¼‰å…¥å•é¡Œæª”æ¡ˆå¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­å•é¡Œ")
            questions = self.default_test_questions
        
        return questions
    
    def search_similar_chunks(self, 
                            query_text: str, 
                            top_k: int = 5,
                            collection_name: str = "podcast_chunks") -> List[Dict]:
        """
        æœå°‹ç›¸ä¼¼ chunks
        
        Args:
            query_text: æŸ¥è©¢æ–‡æœ¬
            top_k: è¿”å›çµæœæ•¸é‡
            collection_name: é›†åˆåç¨±
            
        Returns:
            ç›¸ä¼¼ chunks åˆ—è¡¨
        """
        if not self.milvus_connected:
            raise Exception("Milvus æœªé€£æ¥")
        
        collection = Collection(collection_name)
        collection.load()
        
        try:
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            query_embedding = self.embedding_model_instance.encode(
                query_text, 
                normalize_embeddings=True
            )
            
            # æœå°‹åƒæ•¸
            search_params = {
                "metric_type": "COSINE",
                "params": {"nprobe": 10}
            }
            
            # åŸ·è¡Œæœå°‹
            results = collection.search(
                data=[query_embedding.tolist()],
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                output_fields=["chunk_id", "chunk_text", "tags", "source_document_id", "chunk_index", "audio_file"]
            )
            
            # æ ¼å¼åŒ–çµæœ
            similar_chunks = []
            for hits in results:
                for hit in hits:
                    chunk_data = {
                        'chunk_id': hit.entity.get('chunk_id'),
                        'chunk_text': hit.entity.get('chunk_text'),
                        'tags': json.loads(hit.entity.get('tags', '[]')),
                        'source_document_id': hit.entity.get('source_document_id'),
                        'chunk_index': hit.entity.get('chunk_index'),
                        'audio_file': hit.entity.get('audio_file'),
                        'score': hit.score
                    }
                    similar_chunks.append(chunk_data)
            
            return similar_chunks
            
        finally:
            collection.release()
    
    async def send_rag_query(self, query: str, user_id: str = "test_user") -> Dict[str, Any]:
        """
        ç™¼é€ RAG æŸ¥è©¢
        
        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            user_id: ç”¨æˆ¶ ID
            
        Returns:
            RAG å›æ‡‰
        """
        try:
            # æº–å‚™è«‹æ±‚è³‡æ–™
            payload = {
                "query": query,
                "user_id": user_id,
                "session_id": f"test_session_{int(datetime.now().timestamp())}",
                "category_filter": None,
                "use_advanced_features": True
            }
            
            # ç™¼é€è«‹æ±‚
            response = requests.post(
                f"{self.rag_service_url}/api/v1/query",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "response": data.get("response", ""),
                    "recommendations": data.get("recommendations", []),
                    "confidence": data.get("confidence", 0.0),
                    "source": "rag_service"
                }
            else:
                return {
                    "success": False,
                    "error": f"RAG æœå‹™éŒ¯èª¤: {response.status_code}",
                    "details": response.text
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": f"RAG æŸ¥è©¢å¤±æ•—: {str(e)}"
            }
    
    def generate_qa_prompt(self, question: str, context_chunks: List[Dict]) -> str:
        """
        ç”Ÿæˆå•ç­”æç¤ºè©
        
        Args:
            question: å•é¡Œ
            context_chunks: ä¸Šä¸‹æ–‡ chunks
            
        Returns:
            æç¤ºè©
        """
        # çµ„åˆä¸Šä¸‹æ–‡
        context_text = ""
        for i, chunk in enumerate(context_chunks[:3]):  # åªä½¿ç”¨å‰3å€‹æœ€ç›¸é—œçš„chunks
            context_text += f"ä¸Šä¸‹æ–‡ {i+1}:\n{chunk['chunk_text']}\n\n"
        
        # ç”Ÿæˆæç¤ºè©
        prompt = f"""
ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Podcast å…§å®¹åˆ†æåŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚

{context_text}

ç”¨æˆ¶å•é¡Œ: {question}

è«‹æä¾›ï¼š
1. ç›´æ¥å›ç­”å•é¡Œ
2. ç›¸é—œçš„ Podcast æ¨è–¦
3. é€²ä¸€æ­¥çš„å­¸ç¿’å»ºè­°

å›ç­”è¦æ±‚ï¼š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡
- å›ç­”è¦å…·é«”ä¸”æœ‰å¯¦ç”¨æ€§
- å¦‚æœæœ‰ç›¸é—œçš„ Podcast å…§å®¹ï¼Œè«‹æåŠ
- ä¿æŒå°ˆæ¥­ä¸”å‹å–„çš„èªæ°£
"""
        
        return prompt
    
    def evaluate_response_quality(self, question: str, response: str, context_chunks: List[Dict]) -> Dict[str, Any]:
        """
        è©•ä¼°å›æ‡‰å“è³ª
        
        Args:
            question: å•é¡Œ
            response: å›æ‡‰
            context_chunks: ä¸Šä¸‹æ–‡ chunks
            
        Returns:
            è©•ä¼°çµæœ
        """
        evaluation = {
            "relevance_score": 0.0,
            "completeness_score": 0.0,
            "clarity_score": 0.0,
            "context_usage_score": 0.0,
            "overall_score": 0.0,
            "feedback": []
        }
        
        try:
            # 1. ç›¸é—œæ€§è©•åˆ† (åŸºæ–¼å•é¡Œé—œéµå­—åŒ¹é…)
            question_keywords = set(question.lower().split())
            response_keywords = set(response.lower().split())
            keyword_overlap = len(question_keywords.intersection(response_keywords))
            evaluation["relevance_score"] = min(keyword_overlap / max(len(question_keywords), 1), 1.0)
            
            # 2. å®Œæ•´æ€§è©•åˆ† (åŸºæ–¼å›æ‡‰é•·åº¦å’Œçµæ§‹)
            response_length = len(response)
            if response_length > 100:
                evaluation["completeness_score"] = 0.8
            elif response_length > 50:
                evaluation["completeness_score"] = 0.6
            else:
                evaluation["completeness_score"] = 0.3
            
            # 3. æ¸…æ™°åº¦è©•åˆ† (åŸºæ–¼å¥å­çµæ§‹)
            sentences = response.split('ã€‚')
            if len(sentences) > 2:
                evaluation["clarity_score"] = 0.8
            elif len(sentences) > 1:
                evaluation["clarity_score"] = 0.6
            else:
                evaluation["clarity_score"] = 0.4
            
            # 4. ä¸Šä¸‹æ–‡ä½¿ç”¨è©•åˆ†
            if context_chunks:
                context_usage = any(
                    any(keyword in response.lower() for keyword in chunk.get('tags', []))
                    for chunk in context_chunks
                )
                evaluation["context_usage_score"] = 0.8 if context_usage else 0.3
            else:
                evaluation["context_usage_score"] = 0.0
            
            # 5. æ•´é«”è©•åˆ†
            evaluation["overall_score"] = (
                evaluation["relevance_score"] * 0.3 +
                evaluation["completeness_score"] * 0.25 +
                evaluation["clarity_score"] * 0.25 +
                evaluation["context_usage_score"] * 0.2
            )
            
            # 6. åé¥‹å»ºè­°
            if evaluation["relevance_score"] < 0.5:
                evaluation["feedback"].append("å›æ‡‰èˆ‡å•é¡Œç›¸é—œæ€§è¼ƒä½")
            if evaluation["completeness_score"] < 0.5:
                evaluation["feedback"].append("å›æ‡‰å…§å®¹ä¸å¤ å®Œæ•´")
            if evaluation["clarity_score"] < 0.5:
                evaluation["feedback"].append("å›æ‡‰è¡¨é”ä¸å¤ æ¸…æ™°")
            if evaluation["context_usage_score"] < 0.5:
                evaluation["feedback"].append("æœªå……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡è³‡è¨Š")
            
            if not evaluation["feedback"]:
                evaluation["feedback"].append("å›æ‡‰å“è³ªè‰¯å¥½")
                
        except Exception as e:
            logger.error(f"è©•ä¼°å›æ‡‰å“è³ªå¤±æ•—: {e}")
            evaluation["feedback"].append("è©•ä¼°éç¨‹ç™¼ç”ŸéŒ¯èª¤")
        
        return evaluation
    
    def run_qa_test(self, questions: List[str] = None, max_questions: int = 10) -> Dict[str, Any]:
        """
        åŸ·è¡Œå•ç­”æ¸¬è©¦
        
        Args:
            questions: æ¸¬è©¦å•é¡Œåˆ—è¡¨
            max_questions: æœ€å¤§æ¸¬è©¦å•é¡Œæ•¸é‡
            
        Returns:
            æ¸¬è©¦çµæœ
        """
        if not questions:
            questions = self.load_test_questions()
        
        # é™åˆ¶å•é¡Œæ•¸é‡
        if max_questions:
            questions = questions[:max_questions]
        
        logger.info(f"ğŸ§ª é–‹å§‹åŸ·è¡Œ RAG å•ç­”æ¸¬è©¦ï¼Œå…± {len(questions)} å€‹å•é¡Œ")
        
        test_results = {
            "total_questions": len(questions),
            "successful_queries": 0,
            "failed_queries": 0,
            "average_confidence": 0.0,
            "average_quality_score": 0.0,
            "question_results": [],
            "performance_metrics": {
                "total_time": 0.0,
                "average_response_time": 0.0,
                "rag_service_availability": 0.0
            }
        }
        
        start_time = datetime.now()
        
        for i, question in enumerate(questions, 1):
            logger.info(f"ğŸ” æ¸¬è©¦å•é¡Œ {i}/{len(questions)}: {question}")
            
            question_result = {
                "question": question,
                "question_index": i,
                "timestamp": datetime.now().isoformat(),
                "success": False,
                "response": "",
                "confidence": 0.0,
                "quality_evaluation": {},
                "context_chunks": [],
                "error": None,
                "response_time": 0.0
            }
            
            try:
                question_start_time = datetime.now()
                
                # 1. å‘é‡æœå°‹ç›¸é—œ chunks
                context_chunks = self.search_similar_chunks(question, top_k=3)
                question_result["context_chunks"] = context_chunks
                
                # 2. ç”Ÿæˆæç¤ºè©
                prompt = self.generate_qa_prompt(question, context_chunks)
                
                # 3. ç™¼é€ RAG æŸ¥è©¢
                rag_response = asyncio.run(self.send_rag_query(question))
                
                if rag_response["success"]:
                    question_result["success"] = True
                    question_result["response"] = rag_response["response"]
                    question_result["confidence"] = rag_response.get("confidence", 0.0)
                    
                    # 4. è©•ä¼°å›æ‡‰å“è³ª
                    quality_evaluation = self.evaluate_response_quality(
                        question, rag_response["response"], context_chunks
                    )
                    question_result["quality_evaluation"] = quality_evaluation
                    
                    test_results["successful_queries"] += 1
                    test_results["average_confidence"] += rag_response.get("confidence", 0.0)
                    test_results["average_quality_score"] += quality_evaluation["overall_score"]
                    
                    logger.info(f"âœ… å•é¡Œ {i} æ¸¬è©¦æˆåŠŸï¼Œä¿¡å¿ƒåº¦: {rag_response.get('confidence', 0.0):.2f}")
                else:
                    question_result["error"] = rag_response.get("error", "æœªçŸ¥éŒ¯èª¤")
                    test_results["failed_queries"] += 1
                    logger.error(f"âŒ å•é¡Œ {i} æ¸¬è©¦å¤±æ•—: {rag_response.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                
                # è¨ˆç®—å›æ‡‰æ™‚é–“
                question_result["response_time"] = (datetime.now() - question_start_time).total_seconds()
                
            except Exception as e:
                question_result["error"] = str(e)
                test_results["failed_queries"] += 1
                logger.error(f"âŒ å•é¡Œ {i} æ¸¬è©¦ç•°å¸¸: {e}")
            
            test_results["question_results"].append(question_result)
            
            # é¿å…éåº¦è² è¼‰
            time.sleep(1)
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        end_time = datetime.now()
        test_results["performance_metrics"]["total_time"] = (end_time - start_time).total_seconds()
        
        if test_results["successful_queries"] > 0:
            test_results["average_confidence"] /= test_results["successful_queries"]
            test_results["average_quality_score"] /= test_results["successful_queries"]
            test_results["performance_metrics"]["average_response_time"] = (
                test_results["performance_metrics"]["total_time"] / test_results["successful_queries"]
            )
        
        test_results["performance_metrics"]["rag_service_availability"] = (
            test_results["successful_queries"] / test_results["total_questions"]
        )
        
        return test_results
    
    def save_test_results(self, results: Dict[str, Any], output_file: str = "rag_qa_test_results.json"):
        """
        å„²å­˜æ¸¬è©¦çµæœ
        
        Args:
            results: æ¸¬è©¦çµæœ
            output_file: è¼¸å‡ºæª”æ¡ˆåç¨±
        """
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ æ¸¬è©¦çµæœå·²å„²å­˜åˆ°: {output_file}")
        except Exception as e:
            logger.error(f"âŒ å„²å­˜æ¸¬è©¦çµæœå¤±æ•—: {e}")
    
    def print_test_summary(self, results: Dict[str, Any]):
        """
        åˆ—å°æ¸¬è©¦æ‘˜è¦
        
        Args:
            results: æ¸¬è©¦çµæœ
        """
        print("\n" + "="*60)
        print("ğŸ§ª RAG å•ç­”æ¸¬è©¦æ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ“Š æ¸¬è©¦çµ±è¨ˆ:")
        print(f"  ç¸½å•é¡Œæ•¸: {results['total_questions']}")
        print(f"  æˆåŠŸæŸ¥è©¢: {results['successful_queries']}")
        print(f"  å¤±æ•—æŸ¥è©¢: {results['failed_queries']}")
        print(f"  æˆåŠŸç‡: {results['successful_queries']/results['total_questions']*100:.1f}%")
        
        print(f"\nğŸ“ˆ å“è³ªæŒ‡æ¨™:")
        print(f"  å¹³å‡ä¿¡å¿ƒåº¦: {results['average_confidence']:.3f}")
        print(f"  å¹³å‡å“è³ªåˆ†æ•¸: {results['average_quality_score']:.3f}")
        
        print(f"\nâ±ï¸ æ•ˆèƒ½æŒ‡æ¨™:")
        print(f"  ç¸½æ¸¬è©¦æ™‚é–“: {results['performance_metrics']['total_time']:.1f} ç§’")
        print(f"  å¹³å‡å›æ‡‰æ™‚é–“: {results['performance_metrics']['average_response_time']:.1f} ç§’")
        print(f"  RAG æœå‹™å¯ç”¨æ€§: {results['performance_metrics']['rag_service_availability']*100:.1f}%")
        
        print(f"\nğŸ” è©³ç´°çµæœ:")
        for i, result in enumerate(results['question_results'][:5]):  # åªé¡¯ç¤ºå‰5å€‹
            status = "âœ…" if result['success'] else "âŒ"
            confidence = result.get('confidence', 0.0)
            quality_score = result.get('quality_evaluation', {}).get('overall_score', 0.0)
            print(f"  {status} å•é¡Œ {i+1}: ä¿¡å¿ƒåº¦={confidence:.3f}, å“è³ªåˆ†æ•¸={quality_score:.3f}")
        
        if len(results['question_results']) > 5:
            print(f"  ... é‚„æœ‰ {len(results['question_results']) - 5} å€‹å•é¡Œçµæœ")
        
        print("="*60)
    
    def close_connections(self):
        """é—œé–‰é€£æ¥"""
        if self.mongo_client:
            self.mongo_client.close()
            logger.info("MongoDB é€£æ¥å·²é—œé–‰")
        
        if self.milvus_connected:
            connections.disconnect("default")
            logger.info("Milvus é€£æ¥å·²é—œé–‰")

def main():
    """ä¸»å‡½æ•¸"""
    # åˆå§‹åŒ–æ¸¬è©¦ç³»çµ±
    test_system = RAGQATestSystem(
        mongodb_uri="mongodb://localhost:27017/",
        mongodb_db="podwise",
        mongodb_collection="podcast",
        milvus_host="localhost",
        milvus_port="19530",
        embedding_model="BAAI/bge-m3",
        rag_service_url="http://localhost:8004"
    )
    
    try:
        # åŸ·è¡Œå•ç­”æ¸¬è©¦
        print("ğŸ§ª é–‹å§‹ RAG å•ç­”æ¸¬è©¦...")
        results = test_system.run_qa_test(max_questions=5)  # å…ˆæ¸¬è©¦5å€‹å•é¡Œ
        
        # åˆ—å°æ¸¬è©¦æ‘˜è¦
        test_system.print_test_summary(results)
        
        # å„²å­˜æ¸¬è©¦çµæœ
        test_system.save_test_results(results)
        
        # é¡¯ç¤ºè©³ç´°çµæœ
        print("\nğŸ“‹ è©³ç´°æ¸¬è©¦çµæœ:")
        for i, result in enumerate(results['question_results'], 1):
            print(f"\nå•é¡Œ {i}: {result['question']}")
            if result['success']:
                print(f"å›æ‡‰: {result['response'][:200]}...")
                print(f"ä¿¡å¿ƒåº¦: {result['confidence']:.3f}")
                print(f"å“è³ªåˆ†æ•¸: {result['quality_evaluation']['overall_score']:.3f}")
                if result['quality_evaluation']['feedback']:
                    print(f"åé¥‹: {', '.join(result['quality_evaluation']['feedback'])}")
            else:
                print(f"éŒ¯èª¤: {result['error']}")
        
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
    
    finally:
        test_system.close_connections()

if __name__ == "__main__":
    main() 