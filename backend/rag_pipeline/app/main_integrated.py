#!/usr/bin/env python3
"""
Podwise RAG Pipeline ä¸»æ‡‰ç”¨ç¨‹å¼ - CrewAI + LangChain å¤šä»£ç†äººæ©Ÿåˆ¶
å°ˆæ³¨æ–¼å•†æ¥­å’Œæ•™è‚²é¡åˆ¥ï¼Œå„ªå…ˆä½¿ç”¨ OpenAI æœå°‹å†ä½¿ç”¨ LLM
"""

import os
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# å°å…¥çµ±ä¸€çš„ API æ¨¡å‹
from ..core.api_models import QueryRequest, QueryResponse

# å°å…¥æ ¸å¿ƒçµ„ä»¶
from core.hierarchical_rag_pipeline import HierarchicalRAGPipeline
from core.unified_processor import UnifiedContentProcessor
from core.confidence_controller import ConfidenceController
from core.qwen3_llm_manager import Qwen3LLMManager
from core.chat_history_service import ChatHistoryService
from core.crew_agents import AgentManager

# å°å…¥å·¥å…·
from tools.enhanced_vector_search import EnhancedVectorSearchTool
from tools.deepseek_tool import DeepseekTool
from tools.qwen3_tool import Qwen3Tool

# å°å…¥é…ç½®
from config.integrated_config import get_config

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(
    title="Podwise RAG Pipeline - CrewAI + LangChain å¤šä»£ç†äººæ©Ÿåˆ¶",
    description="å°ˆæ³¨æ–¼å•†æ¥­å’Œæ•™è‚²é¡åˆ¥çš„æ™ºèƒ½å•ç­”ç³»çµ±ï¼Œæ•´åˆ OpenAI æœå°‹å’Œ LLM ç”Ÿæˆ",
    version="2.1.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¼‰å…¥é…ç½®
config = get_config()

# åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶
rag_pipeline = None
hierarchical_rag = None
content_processor = None
confidence_controller = None
qwen3_manager = None
chat_history_service = None
vector_search_tool = None
deepseek_tool = None
qwen3_tool = None
agent_manager = None

# æª¢æŸ¥ OpenAI é…ç½®
OPENAI_AVAILABLE = config.is_openai_configured()

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•äº‹ä»¶"""
    global rag_pipeline, hierarchical_rag, content_processor, confidence_controller
    global qwen3_manager, chat_history_service, vector_search_tool, deepseek_tool, qwen3_tool, agent_manager
    
    try:
        logger.info("ğŸš€ åˆå§‹åŒ– Podwise RAG Pipeline...")
        
        # åˆå§‹åŒ–åˆ†å±¤ RAG Pipeline
        hierarchical_rag = HierarchicalRAGPipeline("config/hierarchical_rag_config.yaml")
        logger.info("âœ… åˆ†å±¤ RAG Pipeline åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–çµ±ä¸€å…§å®¹è™•ç†å™¨
        content_processor = UnifiedContentProcessor()
        logger.info("âœ… çµ±ä¸€å…§å®¹è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–ä¿¡å¿ƒåº¦æ§åˆ¶å™¨
        confidence_controller = ConfidenceController()
        logger.info("âœ… ä¿¡å¿ƒåº¦æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ– Qwen3 LLM ç®¡ç†å™¨
        qwen3_manager = Qwen3LLMManager()
        logger.info("âœ… Qwen3 LLM ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–èŠå¤©æ­·å²æœå‹™
        chat_history_service = ChatHistoryService()
        logger.info("âœ… èŠå¤©æ­·å²æœå‹™åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–å·¥å…·
        vector_search_tool = EnhancedVectorSearchTool()
        deepseek_tool = DeepseekTool()
        qwen3_tool = Qwen3Tool()
        logger.info("âœ… å·¥å…·åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–ä¸‰å±¤ä»£ç†äººæ¶æ§‹
        agent_config = config.get_agent_config()
        agent_manager = AgentManager(agent_config)
        logger.info("âœ… ä¸‰å±¤ä»£ç†äººæ¶æ§‹åˆå§‹åŒ–å®Œæˆ")
        
        # è¨˜éŒ„ç³»çµ±ç‹€æ…‹
        logger.info(f"âœ… OpenAI æœå°‹: {'å¯ç”¨' if OPENAI_AVAILABLE else 'ä¸å¯ç”¨'}")
        logger.info("âœ… æ‰€æœ‰æ ¸å¿ƒçµ„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        raise

@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    services = [
        "CrewAI + LangChain å¤šä»£ç†äººæ©Ÿåˆ¶",
        "åˆ†å±¤ RAG Pipeline",
        "çµ±ä¸€å…§å®¹è™•ç†å™¨",
        "ä¿¡å¿ƒåº¦æ§åˆ¶å™¨",
        "Qwen3 LLM ç®¡ç†å™¨",
        "èŠå¤©æ­·å²æœå‹™",
        "å¢å¼·å‘é‡æœå°‹",
        "DeepSeek å·¥å…·",
        "Qwen3 å·¥å…·"
    ]
    
    if OPENAI_AVAILABLE:
        services.append("OpenAI æœå°‹")
    
    return {
        "message": "Podwise RAG Pipeline - CrewAI + LangChain å¤šä»£ç†äººæ©Ÿåˆ¶é‹è¡Œä¸­",
        "version": "2.1.0",
        "timestamp": datetime.now().isoformat(),
        "services": services,
        "openai_available": OPENAI_AVAILABLE,
        "supported_categories": ["å•†æ¥­", "æ•™è‚²"],
        "api_endpoints": {
            "rag_query": "/api/v1/query",
            "vector_search": "/api/v1/vector-search",
            "content_process": "/api/v1/process-content",
            "chat_history": "/api/v1/chat-history",
            "system_status": "/health",
            "docs": "/docs"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "hierarchical_rag": hierarchical_rag is not None,
            "content_processor": content_processor is not None,
            "confidence_controller": confidence_controller is not None,
            "qwen3_manager": qwen3_manager is not None,
            "chat_history_service": chat_history_service is not None,
            "vector_search_tool": vector_search_tool is not None,
            "deepseek_tool": deepseek_tool is not None,
            "qwen3_tool": qwen3_tool is not None,
            "agent_manager": agent_manager is not None,
            "openai_search": OPENAI_AVAILABLE
        }
    }

@app.post("/api/v1/query", response_model=QueryResponse)
async def query(request: QueryRequest, background_tasks: BackgroundTasks):
    """æ™ºèƒ½å•ç­”ç«¯é» - CrewAI + LangChain å¤šä»£ç†äººæ©Ÿåˆ¶"""
    try:
        if not hierarchical_rag:
            raise HTTPException(status_code=503, detail="RAG Pipeline æœªåˆå§‹åŒ–")
        
        start_time = datetime.now()
        
        # é©—è­‰é¡åˆ¥éæ¿¾å™¨
        if request.category_filter and request.category_filter not in ["å•†æ¥­", "æ•™è‚²"]:
            raise HTTPException(status_code=400, detail="é¡åˆ¥éæ¿¾å™¨åªèƒ½æ˜¯ 'å•†æ¥­' æˆ– 'æ•™è‚²'")
        
        # åŸ·è¡Œåˆ†å±¤ RAG æŸ¥è©¢
        result = await hierarchical_rag.process_query(request.query)
        
        # è¨ˆç®—è™•ç†æ™‚é–“
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # è¨˜éŒ„æŸ¥è©¢æ­·å²
        if request.user_id:
            background_tasks.add_task(
                chat_history_service.add_query,
                user_id=request.user_id,
                session_id=request.session_id,
                query=request.query,
                response=result["response"],
                confidence=result["confidence"],
                sources=result["sources"]
            )
        
        return QueryResponse(
            query=request.query,
            response=result["response"],
            confidence=result["confidence"],
            sources=result["sources"],
            processing_time=processing_time,
            level_used=result["level_used"],
            category=result["category"],
            metadata=result["metadata"],
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        logger.error(f"æŸ¥è©¢è™•ç†å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è©¢è™•ç†å¤±æ•—: {str(e)}")

@app.post("/api/v1/vector-search")
async def vector_search(query: str, top_k: int = 5, category_filter: Optional[str] = None):
    """å‘é‡æœå°‹ç«¯é»"""
    try:
        if not vector_search_tool:
            raise HTTPException(status_code=503, detail="å‘é‡æœå°‹å·¥å…·æœªåˆå§‹åŒ–")
        
        results = await vector_search_tool.search(query, top_k, category_filter)
        return {
            "query": query,
            "results": results,
            "total": len(results)
        }
        
    except Exception as e:
        logger.error(f"å‘é‡æœå°‹å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å‘é‡æœå°‹å¤±æ•—: {str(e)}")

@app.post("/api/v1/process-content")
async def process_content(content: str, metadata: Dict[str, Any] = None):
    """å…§å®¹è™•ç†ç«¯é»"""
    try:
        if not content_processor:
            raise HTTPException(status_code=503, detail="å…§å®¹è™•ç†å™¨æœªåˆå§‹åŒ–")
        
        result = await content_processor.process(content, metadata)
        return {
            "content": content,
            "processed_result": result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"å…§å®¹è™•ç†å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å…§å®¹è™•ç†å¤±æ•—: {str(e)}")

@app.get("/api/v1/chat-history/{user_id}")
async def get_chat_history(user_id: str, limit: int = 50):
    """å–å¾—èŠå¤©æ­·å²ç«¯é»"""
    try:
        if not chat_history_service:
            raise HTTPException(status_code=503, detail="èŠå¤©æ­·å²æœå‹™æœªåˆå§‹åŒ–")
        
        history = await chat_history_service.get_history(user_id, limit)
        return {
            "user_id": user_id,
            "history": history,
            "total": len(history)
        }
        
    except Exception as e:
        logger.error(f"å–å¾—èŠå¤©æ­·å²å¤±æ•—: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å–å¾—èŠå¤©æ­·å²å¤±æ•—: {str(e)}")

@app.get("/api/v1/system-info")
async def get_system_info():
    """ç³»çµ±è³‡è¨Šç«¯é»"""
    return {
        "version": "2.1.0",
        "timestamp": datetime.now().isoformat(),
        "components": {
            "hierarchical_rag": hierarchical_rag is not None,
            "content_processor": content_processor is not None,
            "confidence_controller": confidence_controller is not None,
            "qwen3_manager": qwen3_manager is not None,
            "chat_history_service": chat_history_service is not None,
            "vector_search_tool": vector_search_tool is not None,
            "deepseek_tool": deepseek_tool is not None,
            "qwen3_tool": qwen3_tool is not None,
            "agent_manager": agent_manager is not None,
            "openai_search": OPENAI_AVAILABLE
        },
        "config": {
            "environment": config.environment,
            "debug": config.debug,
            "log_level": config.log_level
        }
    }

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨åŸŸç•°å¸¸è™•ç†å™¨"""
    logger.error(f"æœªè™•ç†çš„ç•°å¸¸: {str(exc)}")
    return {
        "error": "å…§éƒ¨ä¼ºæœå™¨éŒ¯èª¤",
        "detail": str(exc),
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8004) 