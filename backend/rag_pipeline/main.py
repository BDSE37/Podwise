#!/usr/bin/env python3
"""
Podwise RAG Pipeline ä¸»æ¨¡çµ„

æä¾›çµ±ä¸€çš„ OOP ä»‹é¢ï¼Œæ•´åˆæ‰€æœ‰ RAG Pipeline åŠŸèƒ½ï¼š
- å±¤ç´šåŒ– CrewAI æ¶æ§‹
- èªæ„æª¢ç´¢ï¼ˆtext2vec-base-chinese + TAG_info.csvï¼‰
- æç¤ºè©æ¨¡æ¿ç³»çµ±
- Langfuse ç›£æ§
- èŠå¤©æ­·å²è¨˜éŒ„
- æ•ˆèƒ½å„ªåŒ–

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 1.0.0
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
import json
import os

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å°å…¥æ ¸å¿ƒæ¨¡çµ„
from core.crew_agents import LeaderAgent, BusinessExpertAgent, EducationExpertAgent, UserManagerAgent, UserQuery, AgentResponse
from core.hierarchical_rag_pipeline import HierarchicalRAGPipeline, RAGResponse
from core.content_categorizer import ContentCategorizer
from core.confidence_controller import get_confidence_controller
from core.qwen_llm_manager import Qwen3LLMManager
from core.chat_history_service import get_chat_history_service
from config.prompt_templates import PodwisePromptTemplates
from config.integrated_config import get_config
# Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™


class PodwiseRAGPipeline:
    """
    Podwise RAG Pipeline ä¸»é¡åˆ¥
    
    æä¾›çµ±ä¸€çš„ä»‹é¢ä¾†ä½¿ç”¨æ‰€æœ‰ RAG Pipeline åŠŸèƒ½
    å°ˆæ³¨æ–¼æ ¸å¿ƒ RAG è™•ç†é‚è¼¯ï¼Œä¸åŒ…å« Web API åŠŸèƒ½
    """
    
    def __init__(self, 
                 enable_monitoring: bool = True,
                 enable_semantic_retrieval: bool = True,
                 enable_chat_history: bool = True,
                 confidence_threshold: float = 0.7):
        """
        åˆå§‹åŒ– RAG Pipeline
        
        Args:
            enable_monitoring: æ˜¯å¦å•Ÿç”¨ Langfuse ç›£æ§
            enable_semantic_retrieval: æ˜¯å¦å•Ÿç”¨èªæ„æª¢ç´¢
            enable_chat_history: æ˜¯å¦å•Ÿç”¨èŠå¤©æ­·å²è¨˜éŒ„
            confidence_threshold: ä¿¡å¿ƒåº¦é–¾å€¼
        """
        self.enable_monitoring = enable_monitoring
        self.enable_semantic_retrieval = enable_semantic_retrieval
        self.enable_chat_history = enable_chat_history
        self.confidence_threshold = confidence_threshold
        
        # åˆå§‹åŒ–ç›£æ§å™¨ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
        self.monitor = None
        self.langfuse = None
        if os.getenv("LANGFUSE_PUBLIC_KEY") and os.getenv("LANGFUSE_SECRET_KEY"):
            try:
                from langfuse import Langfuse
                self.langfuse = Langfuse(
                    public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
                    secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
                    host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
                )
            except ImportError:
                self.langfuse = None
        
        # åˆå§‹åŒ–æ•´åˆé…ç½®
        self.config = get_config()
        
        # åˆå§‹åŒ–æç¤ºè©æ¨¡æ¿
        self.prompt_templates = PodwisePromptTemplates()
        
        # åˆå§‹åŒ– LLM ç®¡ç†å™¨
        self.llm_manager = Qwen3LLMManager()
        
        # åˆå§‹åŒ–å…§å®¹è™•ç†å™¨
        self.categorizer = ContentCategorizer()
        
        # åˆå§‹åŒ–ä¿¡å¿ƒåº¦æ§åˆ¶å™¨
        self.confidence_controller = get_confidence_controller()
        self.confidence_controller.update_confidence_threshold(confidence_threshold)
        
        # åˆå§‹åŒ–èŠå¤©æ­·å²æœå‹™
        self.chat_history = get_chat_history_service() if enable_chat_history else None
        
        # åˆå§‹åŒ– CrewAI ä»£ç†
        self._initialize_agents()
        
        # åˆå§‹åŒ–å±¤ç´šåŒ– RAG Pipeline
        self.rag_pipeline = HierarchicalRAGPipeline()
        
        logger.info("âœ… Podwise RAG Pipeline åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_agents(self):
        """åˆå§‹åŒ– CrewAI ä»£ç†"""
        # é…ç½®å­—å…¸
        config = {
            'confidence_threshold': self.confidence_threshold,
            'max_processing_time': 30.0
        }
        
        # ç”¨æˆ¶ç®¡ç†å±¤
        self.user_manager = UserManagerAgent(config)
        
        # å•†æ¥­å°ˆå®¶
        self.business_expert = BusinessExpertAgent(config)
        
        # æ•™è‚²å°ˆå®¶
        self.education_expert = EducationExpertAgent(config)
        
        # é ˜å°è€…ä»£ç†
        self.leader_agent = LeaderAgent(config)
        
        logger.info("âœ… CrewAI ä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def process_query(self, 
                           query: str, 
                           user_id: str = "default_user",
                           session_id: Optional[str] = None,
                           metadata: Optional[Dict[str, Any]] = None) -> RAGResponse:
        """
        è™•ç†ç”¨æˆ¶æŸ¥è©¢ï¼ˆæ ¸å¿ƒ RAG åŠŸèƒ½ï¼‰
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            user_id: ç”¨æˆ¶ ID
            session_id: æœƒè©± ID
            metadata: é¡å¤–å…ƒæ•¸æ“š
            
        Returns:
            RAGResponse: è™•ç†çµæœ
        """
        start_time = datetime.now()
        
        # è¨˜éŒ„ç”¨æˆ¶æŸ¥è©¢åˆ°èŠå¤©æ­·å²
        if self.enable_chat_history and self.chat_history:
            try:
                self.chat_history.save_chat_message(
                    user_id=user_id,
                    session_id=session_id or f"session_{user_id}_{int(start_time.timestamp())}",
                    role="user",
                    content=query,
                    chat_mode="rag",
                    metadata=metadata
                )
            except Exception as e:
                logger.warning(f"è¨˜éŒ„ç”¨æˆ¶æŸ¥è©¢å¤±æ•—: {e}")
        
        # å‰µå»ºè¿½è¹¤ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
        trace = None
        if self.langfuse:
            trace = self.langfuse.trace(name="RAG Pipeline Query", user_id=user_id, input=query)
        
        try:
            # ä½¿ç”¨å±¤ç´šåŒ– RAG Pipeline è™•ç†
            response = await self.rag_pipeline.process_query(query)
            
            # è¨˜éŒ„åŠ©æ‰‹å›æ‡‰åˆ°èŠå¤©æ­·å²
            if self.enable_chat_history and self.chat_history:
                try:
                    self.chat_history.save_chat_message(
                        user_id=user_id,
                        session_id=session_id or f"session_{user_id}_{int(start_time.timestamp())}",
                        role="assistant",
                        content=response.content,
                        chat_mode="rag",
                        metadata={
                            "confidence": response.confidence,
                            "level_used": response.level_used,
                            "sources_count": len(response.sources),
                            **(metadata or {})
                        }
                    )
                except Exception as e:
                    logger.warning(f"è¨˜éŒ„åŠ©æ‰‹å›æ‡‰å¤±æ•—: {e}")
            
            # è¿½è¹¤å®Œæ•´æµç¨‹ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
            if trace:
                trace.end(output=response.content, metadata={"confidence": response.confidence})
            
            return response
            
        except Exception as e:
            logger.error(f"è™•ç†æŸ¥è©¢å¤±æ•—: {e}")
            
            # è¨˜éŒ„éŒ¯èª¤å›æ‡‰
            if self.enable_chat_history and self.chat_history:
                try:
                    self.chat_history.save_chat_message(
                        user_id=user_id,
                        session_id=session_id or f"session_{user_id}_{int(start_time.timestamp())}",
                        role="assistant",
                        content=f"è™•ç†æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                        chat_mode="rag",
                        metadata={"error": str(e), **(metadata or {})}
                    )
                except Exception as chat_error:
                    logger.warning(f"è¨˜éŒ„éŒ¯èª¤å›æ‡‰å¤±æ•—: {chat_error}")
            
            # è¿½è¹¤éŒ¯èª¤ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
            if trace:
                trace.end(output=str(e), metadata={"error": True})
            
            # è¿”å›éŒ¯èª¤å›æ‡‰
            return RAGResponse(
                content=f"è™•ç†æŸ¥è©¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                confidence=0.0,
                sources=[],
                processing_time=(datetime.now() - start_time).total_seconds(),
                level_used="error",
                metadata={"error": str(e)}
            )
    
    async def process_with_agents(self, 
                                 query: str, 
                                 user_id: str = "default_user") -> AgentResponse:
        """
        ä½¿ç”¨ CrewAI ä»£ç†è™•ç†æŸ¥è©¢
        
        Args:
            query: ç”¨æˆ¶æŸ¥è©¢
            user_id: ç”¨æˆ¶ ID
            
        Returns:
            AgentResponse: ä»£ç†å›æ‡‰
        """
        start_time = datetime.now()
        
        # å‰µå»ºè¿½è¹¤ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
        trace = None
        if self.langfuse:
            trace = self.langfuse.trace(name="CrewAI Agent Query", user_id=user_id, input=query)
        
        try:
            # å‰µå»ºç”¨æˆ¶æŸ¥è©¢ç‰©ä»¶
            user_query = UserQuery(
                query=query,
                user_id=user_id,
                category=None,
                context=None
            )
            
            # ä½¿ç”¨é ˜å°è€…ä»£ç†è™•ç†
            response = await self.leader_agent.process(user_query)
            
            # è¿½è¹¤ä»£ç†äº’å‹• (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
            if trace:
                trace.end(output=response.content, metadata={"confidence": response.confidence})
            
            return response
            
        except Exception as e:
            logger.error(f"ä»£ç†è™•ç†å¤±æ•—: {e}")
            
            # è¿½è¹¤éŒ¯èª¤ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)
            if trace:
                trace.end(output=str(e), metadata={"error": True})
            
            return AgentResponse(
                content=f"ä»£ç†è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                confidence=0.0,
                reasoning="è™•ç†å¤±æ•—",
                processing_time=(datetime.now() - start_time).total_seconds()
            )
    
    def get_semantic_config(self) -> Optional[Dict[str, Any]]:
        """ç²å–èªæ„æª¢ç´¢é…ç½®"""
        if not hasattr(self.config, 'semantic_retrieval') or not self.config.semantic_retrieval:
            return None
        
        return {
            "model_config": getattr(self.config.semantic_retrieval, 'model_config', None),
            "retrieval_config": getattr(self.config.semantic_retrieval, 'retrieval_config', None),
            "tag_statistics": getattr(self.config.semantic_retrieval, 'tag_statistics', None)
        }
    
    def get_prompt_templates(self) -> Dict[str, str]:
        """ç²å–æç¤ºè©æ¨¡æ¿"""
        return {
            "category_classifier": "åˆ†é¡æç¤ºè©æ¨¡æ¿",
            "semantic_retrieval": "èªæ„æª¢ç´¢æç¤ºè©æ¨¡æ¿",
            "business_expert": "å•†æ¥­å°ˆå®¶æç¤ºè©æ¨¡æ¿",
            "education_expert": "æ•™è‚²å°ˆå®¶æç¤ºè©æ¨¡æ¿",
            "leader_decision": "é ˜å°è€…æ±ºç­–æç¤ºè©æ¨¡æ¿",
            "answer_generation": "å›ç­”ç”Ÿæˆæç¤ºè©æ¨¡æ¿"
        }
    
    def is_monitoring_enabled(self) -> bool:
        """æª¢æŸ¥ç›£æ§æ˜¯å¦å•Ÿç”¨ (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)"""
        return False
    
    def get_monitor_url(self, trace_id: str) -> Optional[str]:
        """ç²å–ç›£æ§ URL (Langfuse æ•´åˆå·²ç§»é™¤ï¼Œä½¿ç”¨ Langfuse Cloud æœå‹™)"""
        return None
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "components": {}
        }
        
        # æª¢æŸ¥ LLM ç®¡ç†å™¨
        try:
            health_status["components"]["llm_manager"] = {"status": "healthy"}
        except Exception as e:
            health_status["components"]["llm_manager"] = {"status": "error", "error": str(e)}
            health_status["status"] = "degraded"
        
        # æª¢æŸ¥èªæ„æª¢ç´¢
        if hasattr(self.config, 'semantic_retrieval') and self.config.semantic_retrieval:
            try:
                semantic_status = getattr(self.config.semantic_retrieval, 'model_config', None)
                health_status["components"]["semantic_retrieval"] = {
                    "status": "healthy",
                    "model": getattr(semantic_status, 'model_name', 'unknown') if semantic_status else "unknown"
                }
            except Exception as e:
                health_status["components"]["semantic_retrieval"] = {"status": "error", "error": str(e)}
                health_status["status"] = "degraded"
        
        # æª¢æŸ¥ç›£æ§
        health_status["components"]["monitoring"] = {
            "status": "enabled" if self.is_monitoring_enabled() else "disabled"
        }
        
        return health_status


# å…¨åŸŸ RAG Pipeline å¯¦ä¾‹
_rag_pipeline = None

def get_rag_pipeline() -> PodwiseRAGPipeline:
    """ç²å–å…¨åŸŸ RAG Pipeline å¯¦ä¾‹"""
    global _rag_pipeline
    if _rag_pipeline is None:
        _rag_pipeline = PodwiseRAGPipeline()
    return _rag_pipeline


async def main():
    """ä¸»å‡½æ•¸ - ç”¨æ–¼æ¸¬è©¦"""
    print("ğŸš€ Podwise RAG Pipeline æ¸¬è©¦")
    
    # å‰µå»º RAG Pipeline å¯¦ä¾‹
    pipeline = PodwiseRAGPipeline()
    
    # å¥åº·æª¢æŸ¥
    health = await pipeline.health_check()
    print(f"ğŸ“Š å¥åº·ç‹€æ…‹: {json.dumps(health, ensure_ascii=False, indent=2)}")
    
    # æ¸¬è©¦æŸ¥è©¢
    test_query = "æˆ‘æƒ³å­¸ç¿’æŠ•è³‡ç†è²¡ï¼Œæœ‰ä»€éº¼æ¨è–¦çš„ Podcast å—ï¼Ÿ"
    print(f"\nğŸ” æ¸¬è©¦æŸ¥è©¢: {test_query}")
    
    # ä½¿ç”¨å±¤ç´šåŒ– RAG Pipeline
    rag_response = await pipeline.process_query(test_query, "test_user")
    print(f"ğŸ“ RAG å›æ‡‰: {rag_response.content}")
    print(f"ğŸ¯ ä¿¡å¿ƒåº¦: {rag_response.confidence}")
    print(f"ğŸ“‚ ä¾†æº: {len(rag_response.sources)} å€‹")
    
    # ä½¿ç”¨ CrewAI ä»£ç†
    agent_response = await pipeline.process_with_agents(test_query, "test_user")
    print(f"ğŸ¤– ä»£ç†å›æ‡‰: {agent_response.content}")
    print(f"ğŸ¯ ä¿¡å¿ƒåº¦: {agent_response.confidence}")
    
    print("\nâœ… æ¸¬è©¦å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main()) 