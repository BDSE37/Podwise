"""
Qwen3:8b LLM ç®¡ç†å™¨
æ”¯æ´å¤šæ¨¡å‹åˆ‡æ›ã€å°ç£å„ªåŒ–ç‰ˆæœ¬ã€Langfuse è¿½è¹¤
"""

import os
import json
import logging
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from datetime import datetime

import requests
from langchain.llms.base import LLM
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field

from config.integrated_config import get_config

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class Qwen3ModelConfig:
    """Qwen3 æ¨¡å‹é…ç½®"""
    name: str
    endpoint: str
    model_type: str  # qwen3:8b, qwen3:taiwan, qwen3:7b
    max_tokens: int = 4096
    temperature: float = 0.7
    top_p: float = 0.9
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stop_sequences: Optional[List[str]] = None
    system_prompt: str = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œèƒ½å¤ æä¾›æº–ç¢ºã€æœ‰ç”¨çš„å›ç­”ã€‚"
    
    def __post_init__(self):
        if self.stop_sequences is None:
            self.stop_sequences = ["<|endoftext|>", "<|im_end|>"]


class Qwen3LLM:
    """Qwen3 LLM å¯¦ä½œ"""
    
    def __init__(self, model_config: Qwen3ModelConfig, config: Optional[Dict[str, Any]] = None):
        self.model_config = model_config
        self.config = config or {}
    
    @property
    def _llm_type(self) -> str:
        return f"qwen3_{self.model_config.model_type}"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """åŸ·è¡Œ LLM å‘¼å«"""
        try:
            # æª¢æŸ¥æ˜¯å¦ç‚º OpenAI æ¨¡å‹
            if self.model_config.model_type.startswith("openai:"):
                return self._call_openai(prompt, stop, **kwargs)
            else:
                return self._call_qwen3(prompt, stop, **kwargs)
                
        except Exception as e:
            logger.error(f"LLM å‘¼å«ç•°å¸¸: {str(e)}")
            return f"éŒ¯èª¤: {str(e)}"
    
    def _call_openai(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:
        """å‘¼å« OpenAI API"""
        try:
            config = get_config()
            if not config.api.openai_api_key:
                return "éŒ¯èª¤: OpenAI API Key æœªé…ç½®"
            
            # å‰µå»º OpenAI å®¢æˆ¶ç«¯
            openai_llm = ChatOpenAI(
                model=self.model_config.name,
                api_key=config.api.openai_api_key,
                temperature=self.model_config.temperature,
                max_tokens=self.model_config.max_tokens
            )
            
            # æº–å‚™æ¶ˆæ¯
            messages = [
                HumanMessage(content=f"{self.model_config.system_prompt}\n\n{prompt}")
            ]
            
            # ç™¼é€è«‹æ±‚
            response = openai_llm.invoke(messages)
            return str(response.content)
            
        except Exception as e:
            logger.error(f"OpenAI API å‘¼å«ç•°å¸¸: {str(e)}")
            return f"éŒ¯èª¤: OpenAI API å‘¼å«å¤±æ•— - {str(e)}"
    
    def _call_qwen3(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:
        """å‘¼å« Qwen3 API"""
        try:
            # æº–å‚™è«‹æ±‚åƒæ•¸
            request_data = {
                "model": self.model_config.name,
                "messages": [
                    {"role": "system", "content": self.model_config.system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": self.model_config.max_tokens,
                "temperature": self.model_config.temperature,
                "top_p": self.model_config.top_p,
                "frequency_penalty": self.model_config.frequency_penalty,
                "presence_penalty": self.model_config.presence_penalty,
                "stream": False
            }
            
            # æ·»åŠ åœæ­¢åºåˆ—
            if stop:
                request_data["stop"] = stop
            elif self.model_config.stop_sequences:
                request_data["stop"] = self.model_config.stop_sequences
            
            # æ·»åŠ é¡å¤–é…ç½®
            request_data.update(self.config)
            request_data.update(kwargs)
            
            # ç™¼é€è«‹æ±‚
            response = requests.post(
                self.model_config.endpoint,
                json=request_data,
                headers={"Content-Type": "application/json"},
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("choices", [{}])[0].get("message", {}).get("content", "")
            else:
                logger.error(f"Qwen3 API éŒ¯èª¤: {response.status_code} - {response.text}")
                return f"éŒ¯èª¤: API å‘¼å«å¤±æ•— (ç‹€æ…‹ç¢¼: {response.status_code})"
                
        except Exception as e:
            logger.error(f"Qwen3 API å‘¼å«ç•°å¸¸: {str(e)}")
            return f"éŒ¯èª¤: Qwen3 API å‘¼å«å¤±æ•— - {str(e)}"
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """ç²å–è­˜åˆ¥åƒæ•¸"""
        return {
            "model": self.model_config.name,
            "model_type": self.model_config.model_type,
            "endpoint": self.model_config.endpoint,
            "max_tokens": self.model_config.max_tokens,
            "temperature": self.model_config.temperature
        }


class Qwen3LLMManager:
    """Qwen3 LLM ç®¡ç†å™¨"""
    
    def __init__(self):
        self.config = get_config()
        self.models: Dict[str, Qwen3LLM] = {}
        self.current_model: Optional[str] = None
        self.model_health: Dict[str, bool] = {}
        self.model_metrics: Dict[str, Dict[str, Any]] = {}
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        # Qwen2.5-Taiwan å°ç£å„ªåŒ–ç‰ˆæœ¬ (ç¬¬ä¸€å„ªå…ˆ)
        qwen3_taiwan_config = Qwen3ModelConfig(
            name="weiren119/Qwen2.5-Taiwan-8B-Instruct",
            endpoint="http://worker1:11434/api/chat",
            model_type="qwen2.5:taiwan",
            system_prompt="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œç‰¹åˆ¥é‡å°å°ç£åœ°å€å„ªåŒ–ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæä¾›æº–ç¢ºã€æœ‰ç”¨çš„è³‡è¨Šã€‚"
        )
        
        # Qwen3:8b ä¸»è¦æ¨¡å‹ (ç¬¬äºŒå„ªå…ˆ)
        qwen3_8b_config = Qwen3ModelConfig(
            name="Qwen/Qwen2.5-8B-Instruct",
            endpoint="http://worker1:11434/api/chat",
            model_type="qwen3:8b",
            system_prompt="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œæ“…é•·ä¸­æ–‡å’Œè‹±æ–‡å°è©±ã€‚è«‹æä¾›æº–ç¢ºã€æœ‰ç”¨çš„å›ç­”ã€‚"
        )
        
        # OpenAI GPT-3.5 å‚™æ´æ¨¡å‹
        if self.config.api.openai_api_key:
            openai_gpt35_config = Qwen3ModelConfig(
                name="gpt-3.5-turbo",
                endpoint="https://api.openai.com/v1/chat/completions",
                model_type="openai:gpt-3.5",
                system_prompt="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œèƒ½å¤ æä¾›æº–ç¢ºã€æœ‰ç”¨çš„å›ç­”ã€‚"
            )
        
        # OpenAI GPT-4 æœ€å¾Œå‚™æ´æ¨¡å‹
        if self.config.api.openai_api_key:
            openai_gpt4_config = Qwen3ModelConfig(
                name="gpt-4",
                endpoint="https://api.openai.com/v1/chat/completions",
                model_type="openai:gpt-4",
                system_prompt="ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ AI åŠ©æ‰‹ï¼Œèƒ½å¤ æä¾›æº–ç¢ºã€æœ‰ç”¨çš„å›ç­”ã€‚"
            )
        
        # å‰µå»ºæ¨¡å‹å¯¦ä¾‹
        self.models["qwen2.5:taiwan"] = Qwen3LLM(model_config=qwen3_taiwan_config)
        self.models["qwen3:8b"] = Qwen3LLM(model_config=qwen3_8b_config)
        
        # åªæœ‰åœ¨æœ‰ OpenAI API Key æ™‚æ‰æ·»åŠ  OpenAI æ¨¡å‹
        if self.config.api.openai_api_key:
            self.models["openai:gpt-3.5"] = Qwen3LLM(model_config=openai_gpt35_config)
            self.models["openai:gpt-4"] = Qwen3LLM(model_config=openai_gpt4_config)
        
        # è¨­ç½®é è¨­æ¨¡å‹ç‚ºå°ç£å„ªåŒ–ç‰ˆæœ¬
        self.current_model = "qwen2.5:taiwan"
        
        # åˆå§‹åŒ–å¥åº·ç‹€æ…‹
        for model_name in self.models.keys():
            self.model_health[model_name] = True
            self.model_metrics[model_name] = {
                "total_calls": 0,
                "successful_calls": 0,
                "failed_calls": 0,
                "average_response_time": 0.0,
                "last_used": None
            }
    
    def get_model(self, model_name: Optional[str] = None) -> Qwen3LLM:
        """ç²å–æŒ‡å®šæ¨¡å‹"""
        if model_name is None:
            model_name = self.current_model or "qwen2.5:taiwan"
        
        if model_name not in self.models:
            logger.warning(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­æ¨¡å‹")
            model_name = self.current_model or "qwen2.5:taiwan"
        
        return self.models[model_name]
    
    def switch_model(self, model_name: str) -> bool:
        """åˆ‡æ›æ¨¡å‹"""
        if model_name in self.models:
            self.current_model = model_name
            logger.info(f"å·²åˆ‡æ›åˆ°æ¨¡å‹: {model_name}")
            return True
        else:
            logger.error(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return False
    
    def get_available_models(self) -> List[str]:
        """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return list(self.models.keys())
    
    def get_model_info(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹è³‡è¨Š"""
        if model_name is None:
            model_name = self.current_model
        
        if model_name not in self.models:
            return {"error": f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨"}
        
        model = self.models[model_name]
        metrics = self.model_metrics[model_name]
        
        return {
            "name": model_name,
            "model_type": model.model_config.model_type,
            "endpoint": model.model_config.endpoint,
            "max_tokens": model.model_config.max_tokens,
            "temperature": model.model_config.temperature,
            "health": self.model_health[model_name],
            "metrics": metrics,
            "is_current": model_name == self.current_model
        }
    
    def test_model_health(self, model_name: Optional[str] = None) -> bool:
        """æ¸¬è©¦æ¨¡å‹å¥åº·ç‹€æ…‹"""
        if model_name is None:
            model_name = self.current_model
        
        if model_name not in self.models:
            return False
        
        try:
            model = self.models[model_name]
            test_prompt = "è«‹å›ç­”ï¼šä½ å¥½"
            
            start_time = datetime.now()
            response = model._call(test_prompt)
            end_time = datetime.now()
            
            response_time = (end_time - start_time).total_seconds()
            
            # æ›´æ–°æŒ‡æ¨™
            self.model_metrics[model_name]["total_calls"] += 1
            self.model_metrics[model_name]["last_used"] = datetime.now().isoformat()
            
            if "éŒ¯èª¤" not in response:
                self.model_health[model_name] = True
                self.model_metrics[model_name]["successful_calls"] += 1
                
                # æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
                current_avg = self.model_metrics[model_name]["average_response_time"]
                total_calls = self.model_metrics[model_name]["total_calls"]
                self.model_metrics[model_name]["average_response_time"] = (
                    (current_avg * (total_calls - 1) + response_time) / total_calls
                )
                
                logger.info(f"æ¨¡å‹ {model_name} å¥åº·æª¢æŸ¥é€šéï¼ŒéŸ¿æ‡‰æ™‚é–“: {response_time:.2f}s")
                return True
            else:
                self.model_health[model_name] = False
                self.model_metrics[model_name]["failed_calls"] += 1
                logger.error(f"æ¨¡å‹ {model_name} å¥åº·æª¢æŸ¥å¤±æ•—: {response}")
                return False
                
        except Exception as e:
            self.model_health[model_name] = False
            self.model_metrics[model_name]["failed_calls"] += 1
            logger.error(f"æ¨¡å‹ {model_name} å¥åº·æª¢æŸ¥ç•°å¸¸: {str(e)}")
            return False
    
    def get_best_model(self) -> str:
        """ç²å–æœ€ä½³å¯ç”¨æ¨¡å‹"""
        # æª¢æŸ¥ç•¶å‰æ¨¡å‹æ˜¯å¦å¥åº·
        if self.current_model and self.test_model_health(self.current_model):
            return self.current_model
        
        # æŒ‰å„ªå…ˆç´šæª¢æŸ¥å…¶ä»–æ¨¡å‹
        priority_models = self.config.models.llm_priority or []
        for model_name in priority_models:
            if model_name in self.models and self.test_model_health(model_name):
                self.switch_model(model_name)
                return model_name
        
        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½ä¸å¥åº·ï¼Œè¿”å›é è¨­æ¨¡å‹
        logger.warning("æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¥åº·ï¼Œä½¿ç”¨é è¨­æ¨¡å‹")
        return self.current_model or "qwen2.5:taiwan"
    
    def call_with_fallback(self, prompt: str, **kwargs) -> str:
        """å¸¶å›é€€æ©Ÿåˆ¶çš„æ¨¡å‹å‘¼å«"""
        best_model_name = self.get_best_model()
        model = self.get_model(best_model_name)
        
        try:
            response = model._call(prompt, **kwargs)
            
            # æ›´æ–°æŒ‡æ¨™
            self.model_metrics[best_model_name]["total_calls"] += 1
            self.model_metrics[best_model_name]["last_used"] = datetime.now().isoformat()
            
            if "éŒ¯èª¤" not in response:
                self.model_metrics[best_model_name]["successful_calls"] += 1
            else:
                self.model_metrics[best_model_name]["failed_calls"] += 1
            
            return response
            
        except Exception as e:
            self.model_metrics[best_model_name]["failed_calls"] += 1
            logger.error(f"æ¨¡å‹å‘¼å«ç•°å¸¸: {str(e)}")
            return f"éŒ¯èª¤: {str(e)}"
    
    def get_metrics_summary(self) -> Dict[str, Any]:
        """ç²å–æŒ‡æ¨™æ‘˜è¦"""
        summary = {
            "current_model": self.current_model,
            "total_models": len(self.models),
            "healthy_models": sum(self.model_health.values()),
            "models": {}
        }
        
        for model_name in self.models.keys():
            summary["models"][model_name] = self.get_model_info(model_name)
        
        return summary
    
    def reset_metrics(self, model_name: Optional[str] = None):
        """é‡ç½®æ¨¡å‹æŒ‡æ¨™"""
        if model_name:
            if model_name in self.model_metrics:
                self.model_metrics[model_name] = {
                    "total_calls": 0,
                    "successful_calls": 0,
                    "failed_calls": 0,
                    "average_response_time": 0.0,
                    "last_used": None
                }
        else:
            for model_name in self.model_metrics.keys():
                self.model_metrics[model_name] = {
                    "total_calls": 0,
                    "successful_calls": 0,
                    "failed_calls": 0,
                    "average_response_time": 0.0,
                    "last_used": None
                }


# å…¨åŸŸ LLM ç®¡ç†å™¨å¯¦ä¾‹
qwen3_llm_manager = Qwen3LLMManager()


def get_qwen3_llm_manager() -> Qwen3LLMManager:
    """ç²å– Qwen3 LLM ç®¡ç†å™¨å¯¦ä¾‹"""
    return qwen3_llm_manager


if __name__ == "__main__":
    # æ¸¬è©¦ LLM ç®¡ç†å™¨
    manager = get_qwen3_llm_manager()
    
    print("ğŸ”§ Qwen3 LLM ç®¡ç†å™¨æ¸¬è©¦")
    print("=" * 50)
    
    # é¡¯ç¤ºå¯ç”¨æ¨¡å‹
    print(f"å¯ç”¨æ¨¡å‹: {manager.get_available_models()}")
    print(f"ç•¶å‰æ¨¡å‹: {manager.current_model}")
    
    # æ¸¬è©¦å¥åº·æª¢æŸ¥
    print("\nğŸ¥ æ¨¡å‹å¥åº·æª¢æŸ¥:")
    for model_name in manager.get_available_models():
        is_healthy = manager.test_model_health(model_name)
        print(f"  {model_name}: {'âœ…' if is_healthy else 'âŒ'}")
    
    # æ¸¬è©¦æ¨¡å‹å‘¼å«
    print("\nğŸ¤– æ¨¡å‹å‘¼å«æ¸¬è©¦:")
    test_prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡ä»‹ç´¹ä¸€ä¸‹ä½ è‡ªå·±"
    response = manager.call_with_fallback(test_prompt)
    print(f"å›æ‡‰: {response}")
    
    # é¡¯ç¤ºæŒ‡æ¨™æ‘˜è¦
    print("\nğŸ“Š æŒ‡æ¨™æ‘˜è¦:")
    metrics = manager.get_metrics_summary()
    print(json.dumps(metrics, indent=2, ensure_ascii=False)) 