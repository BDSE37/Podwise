#!/usr/bin/env python3
"""
å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG Pipeline
å¯¦ç¾æŸ¥è©¢é‡å¯«ã€æ··åˆæœå°‹ã€æª¢ç´¢å¢å¼·ã€é‡æ–°æ’åºã€ä¸Šä¸‹æ–‡å£“ç¸®ã€æ··åˆå¼RAGçš„å®Œæ•´æ¶æ§‹
"""

import os
import logging
import asyncio
import aiohttp
import json
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod
import yaml

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class QueryContext:
    """æŸ¥è©¢ä¸Šä¸‹æ–‡"""
    original_query: str
    rewritten_query: str
    intent: str
    entities: List[str]
    domain: str
    confidence: float
    metadata: Dict[str, Any]

@dataclass
class SearchResult:
    """æœå°‹çµæœ"""
    document_id: str
    content: str
    score: float
    source: str
    metadata: Dict[str, Any]

@dataclass
class RAGResponse:
    """RAG å›æ‡‰"""
    content: str
    confidence: float
    sources: List[str]
    processing_time: float
    level_used: str
    metadata: Dict[str, Any]

class RAGLevel(ABC):
    """RAG å±¤ç´šæŠ½è±¡åŸºé¡"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.name = config.get('name', 'Unknown')
        self.confidence_threshold = config.get('confidence_threshold', 0.7)
        self.fallback_strategy = config.get('fallback_strategy', None)
    
    @abstractmethod
    async def process(self, input_data: Any) -> Tuple[Any, float]:
        """è™•ç†è¼¸å…¥æ•¸æ“š"""
        pass
    
    async def should_fallback(self, confidence: float) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦é™ç´š"""
        return confidence < self.confidence_threshold

class Level1QueryProcessing(RAGLevel):
    """ç¬¬ä¸€å±¤ï¼šæŸ¥è©¢é‡å¯«è½‰æ›æ‹“å±•"""
    
    async def process(self, input_data: str) -> Tuple[QueryContext, float]:
        """è™•ç†æŸ¥è©¢é‡å¯«ã€è½‰æ›å’Œæ‹“å±•"""
        logger.info(f"ğŸ” {self.name}: è™•ç†æŸ¥è©¢é‡å¯«è½‰æ›æ‹“å±•")
        
        start_time = time.time()
        
        try:
            # æŸ¥è©¢é‡å¯«
            rewritten_query = await self._rewrite_query(input_data)
            
            # æ„åœ–è­˜åˆ¥
            intent = await self._recognize_intent(input_data)
            
            # å¯¦é«”è­˜åˆ¥
            entities = await self._extract_entities(input_data)
            
            # é ˜åŸŸåˆ†é¡
            domain = await self._classify_domain(input_data)
            
            # è¨ˆç®—ä¿¡å¿ƒå€¼
            confidence = await self._calculate_confidence(
                rewritten_query, intent, entities, domain
            )
            
            processing_time = time.time() - start_time
            
            context = QueryContext(
                original_query=input_data,
                rewritten_query=rewritten_query,
                intent=intent,
                entities=entities,
                domain=domain,
                confidence=confidence,
                metadata={
                    'processing_time': processing_time,
                    'level': 'query_processing'
                }
            )
            
            logger.info(f"âœ… {self.name}: ä¿¡å¿ƒå€¼ {confidence:.3f}, è™•ç†æ™‚é–“ {processing_time:.3f}s")
            return context, confidence
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: è™•ç†å¤±æ•— - {e}")
            return None, 0.0
    
    async def _rewrite_query(self, query: str) -> str:
        """æŸ¥è©¢é‡å¯«"""
        # é€™è£¡å¯ä»¥æ•´åˆ T5 æˆ– BART æ¨¡å‹é€²è¡ŒæŸ¥è©¢é‡å¯«
        # ç›®å‰ä½¿ç”¨ç°¡åŒ–çš„åŒç¾©è©æ“´å±•
        synonyms = {
            'æ¨è–¦': ['å»ºè­°', 'ä»‹ç´¹', 'åˆ†äº«'],
            'æ’­å®¢': ['podcast', 'éŸ³é »ç¯€ç›®', 'å»£æ’­'],
            'ç§‘æŠ€': ['æŠ€è¡“', 'ç§‘æŠ€', 'å‰µæ–°']
        }
        
        rewritten = query
        for word, syns in synonyms.items():
            if word in query:
                # éš¨æ©Ÿé¸æ“‡ä¸€å€‹åŒç¾©è©æ›¿æ›
                import random
                rewritten = rewritten.replace(word, random.choice(syns))
        
        return rewritten
    
    async def _recognize_intent(self, query: str) -> str:
        """æ„åœ–è­˜åˆ¥"""
        intents = {
            'recommendation': ['æ¨è–¦', 'å»ºè­°', 'ä»‹ç´¹'],
            'analysis': ['åˆ†æ', 'ç ”ç©¶', 'æ¢è¨'],
            'comparison': ['æ¯”è¼ƒ', 'å°æ¯”', 'å·®ç•°'],
            'explanation': ['è§£é‡‹', 'èªªæ˜', 'ä»‹ç´¹']
        }
        
        for intent, keywords in intents.items():
            if any(keyword in query for keyword in keywords):
                return intent
        
        return 'general'
    
    async def _extract_entities(self, query: str) -> List[str]:
        """å¯¦é«”è­˜åˆ¥"""
        # ç°¡åŒ–çš„å¯¦é«”è­˜åˆ¥
        entities = []
        if 'æ’­å®¢' in query:
            entities.append('podcast')
        if 'ç§‘æŠ€' in query:
            entities.append('technology')
        if 'æ¨è–¦' in query:
            entities.append('recommendation')
        
        return entities
    
    async def _classify_domain(self, query: str) -> str:
        """é ˜åŸŸåˆ†é¡"""
        domains = {
            'business': ['å•†æ¥­', 'å‰µæ¥­', 'æŠ•è³‡', 'å¸‚å ´'],
            'education': ['æ•™è‚²', 'å­¸ç¿’', 'èª²ç¨‹', 'çŸ¥è­˜']
        }
        
        for domain, keywords in domains.items():
            if any(keyword in query for keyword in keywords):
                return domain
        
        return 'general'
    
    async def _calculate_confidence(self, rewritten: str, intent: str, entities: List[str], domain: str) -> float:
        """è¨ˆç®—ä¿¡å¿ƒå€¼"""
        confidence = 0.0
        
        # é‡å¯«æŸ¥è©¢çš„ç›¸ä¼¼åº¦
        if rewritten != self.config.get('original_query', ''):
            confidence += 0.3
        
        # æ„åœ–è­˜åˆ¥çš„æº–ç¢ºæ€§
        if intent != 'general':
            confidence += 0.2
        
        # å¯¦é«”è­˜åˆ¥çš„æ•¸é‡
        confidence += min(len(entities) * 0.1, 0.2)
        
        # é ˜åŸŸåˆ†é¡çš„æº–ç¢ºæ€§
        if domain != 'general':
            confidence += 0.3
        
        return min(confidence, 1.0)

class Level2HybridSearch(RAGLevel):
    """ç¬¬äºŒå±¤ï¼šæ··åˆæœå°‹"""
    
    async def process(self, input_data: QueryContext) -> Tuple[List[SearchResult], float]:
        """åŸ·è¡Œæ··åˆæœå°‹"""
        logger.info(f"ğŸ” {self.name}: åŸ·è¡Œæ··åˆæœå°‹")
        
        start_time = time.time()
        
        try:
            # å¯†é›†æª¢ç´¢
            dense_results = await self._dense_retrieval(input_data.rewritten_query)
            
            # ç¨€ç–æª¢ç´¢
            sparse_results = await self._sparse_retrieval(input_data.rewritten_query)
            
            # èªç¾©æœå°‹
            semantic_results = await self._semantic_search(input_data.rewritten_query)
            
            # æ··åˆèåˆ
            fused_results = await self._hybrid_fusion(dense_results, sparse_results, semantic_results)
            
            processing_time = time.time() - start_time
            
            # è¨ˆç®—ä¿¡å¿ƒå€¼
            confidence = await self._calculate_search_confidence(fused_results)
            
            logger.info(f"âœ… {self.name}: æª¢ç´¢åˆ° {len(fused_results)} å€‹çµæœ, ä¿¡å¿ƒå€¼ {confidence:.3f}")
            return fused_results, confidence
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: æœå°‹å¤±æ•— - {e}")
            return [], 0.0
    
    async def _dense_retrieval(self, query: str) -> List[SearchResult]:
        """å¯†é›†æª¢ç´¢"""
        # æ¨¡æ“¬å¯†é›†æª¢ç´¢çµæœ
        return [
            SearchResult(
                document_id="doc_001",
                content="é€™æ˜¯ä¸€å€‹é—œæ–¼ç§‘æŠ€æ’­å®¢çš„è©³ç´°ä»‹ç´¹...",
                score=0.85,
                source="dense_retrieval",
                metadata={'method': 'dense', 'model': 'bge-large-zh'}
            ),
            SearchResult(
                document_id="doc_002", 
                content="æœ€æ–°çš„ç§‘æŠ€è¶¨å‹¢åˆ†æå ±å‘Š...",
                score=0.78,
                source="dense_retrieval",
                metadata={'method': 'dense', 'model': 'bge-large-zh'}
            )
        ]
    
    async def _sparse_retrieval(self, query: str) -> List[SearchResult]:
        """ç¨€ç–æª¢ç´¢"""
        # æ¨¡æ“¬ç¨€ç–æª¢ç´¢çµæœ
        return [
            SearchResult(
                document_id="doc_003",
                content="BM25 æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”...",
                score=0.72,
                source="sparse_retrieval",
                metadata={'method': 'sparse', 'algorithm': 'bm25'}
            )
        ]
    
    async def _semantic_search(self, query: str) -> List[SearchResult]:
        """èªç¾©æœå°‹"""
        # æ¨¡æ“¬èªç¾©æœå°‹çµæœ
        return [
            SearchResult(
                document_id="doc_004",
                content="èªç¾©ç›¸é—œçš„æ’­å®¢å…§å®¹...",
                score=0.80,
                source="semantic_search",
                metadata={'method': 'semantic', 'model': 'sentence-transformers'}
            )
        ]
    
    async def _hybrid_fusion(self, dense_results: List[SearchResult], 
                           sparse_results: List[SearchResult], 
                           semantic_results: List[SearchResult]) -> List[SearchResult]:
        """æ··åˆèåˆ"""
        all_results = dense_results + sparse_results + semantic_results
        
        # æŒ‰åˆ†æ•¸æ’åº
        all_results.sort(key=lambda x: x.score, reverse=True)
        
        # å»é‡ï¼ˆåŸºæ–¼ document_idï¼‰
        seen_ids = set()
        unique_results = []
        for result in all_results:
            if result.document_id not in seen_ids:
                seen_ids.add(result.document_id)
                unique_results.append(result)
        
        return unique_results[:10]  # è¿”å›å‰10å€‹çµæœ
    
    async def _calculate_search_confidence(self, results: List[SearchResult]) -> float:
        """è¨ˆç®—æœå°‹ä¿¡å¿ƒå€¼"""
        if not results:
            return 0.0
        
        # åŸºæ–¼çµæœæ•¸é‡å’Œå¹³å‡åˆ†æ•¸è¨ˆç®—ä¿¡å¿ƒå€¼
        avg_score = sum(r.score for r in results) / len(results)
        result_count_factor = min(len(results) / 5, 1.0)  # æœ€å¤š5å€‹çµæœç‚ºæ»¿åˆ†
        
        confidence = (avg_score * 0.7 + result_count_factor * 0.3)
        return min(confidence, 1.0)

class Level3RetrievalAugmentation(RAGLevel):
    """ç¬¬ä¸‰å±¤ï¼šæª¢ç´¢å¢å¼·"""
    
    async def process(self, input_data: List[SearchResult]) -> Tuple[List[SearchResult], float]:
        """åŸ·è¡Œæª¢ç´¢å¢å¼·"""
        logger.info(f"ğŸ” {self.name}: åŸ·è¡Œæª¢ç´¢å¢å¼·")
        
        start_time = time.time()
        
        try:
            augmented_results = []
            
            for result in input_data:
                # ä¸Šä¸‹æ–‡å¢å¼·
                augmented_content = await self._augment_context(result)
                
                # çŸ¥è­˜åœ–è­œæ•´åˆ
                knowledge_enhanced = await self._integrate_knowledge_graph(augmented_content)
                
                # å¤–éƒ¨æ•¸æ“šèåˆ
                external_enhanced = await self._fuse_external_data(knowledge_enhanced)
                
                # å‰µå»ºå¢å¼·å¾Œçš„çµæœ
                augmented_result = SearchResult(
                    document_id=result.document_id,
                    content=external_enhanced,
                    score=result.score * 1.1,  # å¢å¼·å¾Œåˆ†æ•¸ç•¥é«˜
                    source=result.source,
                    metadata={
                        **result.metadata,
                        'augmented': True,
                        'context_enhanced': True,
                        'knowledge_integrated': True,
                        'external_fused': True
                    }
                )
                
                augmented_results.append(augmented_result)
            
            processing_time = time.time() - start_time
            
            # è¨ˆç®—ä¿¡å¿ƒå€¼
            confidence = await self._calculate_augmentation_confidence(augmented_results)
            
            logger.info(f"âœ… {self.name}: å¢å¼·äº† {len(augmented_results)} å€‹çµæœ, ä¿¡å¿ƒå€¼ {confidence:.3f}")
            return augmented_results, confidence
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: å¢å¼·å¤±æ•— - {e}")
            return input_data, 0.0
    
    async def _augment_context(self, result: SearchResult) -> str:
        """ä¸Šä¸‹æ–‡å¢å¼·"""
        # æ¨¡æ“¬ä¸Šä¸‹æ–‡å¢å¼·
        enhanced_content = f"[ä¸Šä¸‹æ–‡å¢å¼·] {result.content} [åŒ…å«ç›¸é—œèƒŒæ™¯ä¿¡æ¯]"
        return enhanced_content
    
    async def _integrate_knowledge_graph(self, content: str) -> str:
        """çŸ¥è­˜åœ–è­œæ•´åˆ"""
        # æ¨¡æ“¬çŸ¥è­˜åœ–è­œæ•´åˆ
        enhanced_content = f"[çŸ¥è­˜åœ–è­œ] {content} [æ•´åˆå¯¦é«”é—œä¿‚]"
        return enhanced_content
    
    async def _fuse_external_data(self, content: str) -> str:
        """å¤–éƒ¨æ•¸æ“šèåˆ"""
        # æ¨¡æ“¬å¤–éƒ¨æ•¸æ“šèåˆ
        enhanced_content = f"[å¤–éƒ¨æ•¸æ“š] {content} [èåˆæœ€æ–°ä¿¡æ¯]"
        return enhanced_content
    
    async def _calculate_augmentation_confidence(self, results: List[SearchResult]) -> float:
        """è¨ˆç®—å¢å¼·ä¿¡å¿ƒå€¼"""
        if not results:
            return 0.0
        
        # åŸºæ–¼å¢å¼·æ¨™è¨˜è¨ˆç®—ä¿¡å¿ƒå€¼
        augmented_count = sum(1 for r in results if r.metadata.get('augmented', False))
        confidence = augmented_count / len(results)
        
        return confidence

class Level4Reranking(RAGLevel):
    """ç¬¬å››å±¤ï¼šé‡æ–°æ’åº"""
    
    async def process(self, input_data: List[SearchResult]) -> Tuple[List[SearchResult], float]:
        """åŸ·è¡Œé‡æ–°æ’åº"""
        logger.info(f"ğŸ” {self.name}: åŸ·è¡Œé‡æ–°æ’åº")
        
        start_time = time.time()
        
        try:
            # å¤šæº–å‰‡æ’åº
            multi_criteria_ranked = await self._multi_criteria_ranking(input_data)
            
            # å€‹äººåŒ–æ’åº
            personalized_ranked = await self._personalization_ranking(multi_criteria_ranked)
            
            # å¤šæ¨£æ€§æ’åº
            diversity_ranked = await self._diversity_ranking(personalized_ranked)
            
            processing_time = time.time() - start_time
            
            # è¨ˆç®—ä¿¡å¿ƒå€¼
            confidence = await self._calculate_ranking_confidence(diversity_ranked)
            
            logger.info(f"âœ… {self.name}: é‡æ–°æ’åºå®Œæˆ, ä¿¡å¿ƒå€¼ {confidence:.3f}")
            return diversity_ranked, confidence
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: é‡æ–°æ’åºå¤±æ•— - {e}")
            return input_data, 0.0
    
    async def _multi_criteria_ranking(self, results: List[SearchResult]) -> List[SearchResult]:
        """å¤šæº–å‰‡æ’åº"""
        # æ¨¡æ“¬å¤šæº–å‰‡æ’åº
        for result in results:
            # èª¿æ•´åˆ†æ•¸åŸºæ–¼å¤šå€‹æº–å‰‡
            relevance_score = result.score * 0.4
            freshness_score = 0.8 * 0.2  # å‡è¨­æ–°é®®åº¦ç‚º0.8
            authority_score = 0.7 * 0.2  # å‡è¨­æ¬Šå¨æ€§ç‚º0.7
            diversity_score = 0.6 * 0.1  # å‡è¨­å¤šæ¨£æ€§ç‚º0.6
            novelty_score = 0.9 * 0.1    # å‡è¨­æ–°ç©æ€§ç‚º0.9
            
            result.score = relevance_score + freshness_score + authority_score + diversity_score + novelty_score
        
        # é‡æ–°æ’åº
        results.sort(key=lambda x: x.score, reverse=True)
        return results
    
    async def _personalization_ranking(self, results: List[SearchResult]) -> List[SearchResult]:
        """å€‹äººåŒ–æ’åº"""
        # æ¨¡æ“¬å€‹äººåŒ–æ’åºï¼ˆåŸºæ–¼ç”¨æˆ¶åå¥½ï¼‰
        user_preferences = {
            'technology': 1.2,
            'business': 1.1,
            'entertainment': 0.9
        }
        
        for result in results:
            # æ ¹æ“šå…§å®¹èª¿æ•´åˆ†æ•¸
            if 'ç§‘æŠ€' in result.content:
                result.score *= user_preferences.get('technology', 1.0)
            elif 'å•†æ¥­' in result.content:
                result.score *= user_preferences.get('business', 1.0)
        
        results.sort(key=lambda x: x.score, reverse=True)
        return results
    
    async def _diversity_ranking(self, results: List[SearchResult]) -> List[SearchResult]:
        """å¤šæ¨£æ€§æ’åº"""
        # æ¨¡æ“¬å¤šæ¨£æ€§æ’åº
        diverse_results = []
        seen_topics = set()
        
        for result in results:
            # ç°¡å–®çš„å¤šæ¨£æ€§æª¢æŸ¥
            topic = self._extract_topic(result.content)
            if topic not in seen_topics or len(diverse_results) < 3:
                diverse_results.append(result)
                seen_topics.add(topic)
        
        return diverse_results[:5]  # è¿”å›å‰5å€‹å¤šæ¨£åŒ–çµæœ
    
    def _extract_topic(self, content: str) -> str:
        """æå–ä¸»é¡Œ"""
        topics = ['ç§‘æŠ€', 'å•†æ¥­', 'å¨›æ¨‚', 'æ•™è‚²']
        for topic in topics:
            if topic in content:
                return topic
        return 'general'
    
    async def _calculate_ranking_confidence(self, results: List[SearchResult]) -> float:
        """è¨ˆç®—æ’åºä¿¡å¿ƒå€¼"""
        if not results:
            return 0.0
        
        # åŸºæ–¼æ’åºä¸€è‡´æ€§å’Œåˆ†æ•¸åˆ†å¸ƒè¨ˆç®—ä¿¡å¿ƒå€¼
        scores = [r.score for r in results]
        avg_score = sum(scores) / len(scores)
        score_variance = sum((s - avg_score) ** 2 for s in scores) / len(scores)
        
        # åˆ†æ•¸è¶Šé«˜ï¼Œæ–¹å·®è¶Šå°ï¼Œä¿¡å¿ƒå€¼è¶Šé«˜
        confidence = avg_score * (1 - score_variance)
        return min(max(confidence, 0.0), 1.0)

class Level5ContextCompression(RAGLevel):
    """ç¬¬äº”å±¤ï¼šä¸Šä¸‹æ–‡å£“ç¸®éæ¿¾"""
    
    async def process(self, input_data: List[SearchResult]) -> Tuple[List[SearchResult], float]:
        """åŸ·è¡Œä¸Šä¸‹æ–‡å£“ç¸®éæ¿¾"""
        logger.info(f"ğŸ” {self.name}: åŸ·è¡Œä¸Šä¸‹æ–‡å£“ç¸®éæ¿¾")
        
        start_time = time.time()
        
        try:
            compressed_results = []
            
            for result in input_data:
                # ä¸Šä¸‹æ–‡å£“ç¸®
                compressed_content = await self._compress_context(result.content)
                
                # ä¿¡æ¯éæ¿¾
                filtered_content = await self._filter_information(compressed_content)
                
                # å‰µå»ºå£“ç¸®å¾Œçš„çµæœ
                compressed_result = SearchResult(
                    document_id=result.document_id,
                    content=filtered_content,
                    score=result.score,
                    source=result.source,
                    metadata={
                        **result.metadata,
                        'compressed': True,
                        'filtered': True,
                        'compression_ratio': len(filtered_content) / len(result.content)
                    }
                )
                
                compressed_results.append(compressed_result)
            
            processing_time = time.time() - start_time
            
            # è¨ˆç®—ä¿¡å¿ƒå€¼
            confidence = await self._calculate_compression_confidence(compressed_results)
            
            logger.info(f"âœ… {self.name}: å£“ç¸®äº† {len(compressed_results)} å€‹çµæœ, ä¿¡å¿ƒå€¼ {confidence:.3f}")
            return compressed_results, confidence
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: å£“ç¸®å¤±æ•— - {e}")
            return input_data, 0.0
    
    async def _compress_context(self, content: str) -> str:
        """ä¸Šä¸‹æ–‡å£“ç¸®"""
        # æ¨¡æ“¬ä¸Šä¸‹æ–‡å£“ç¸®ï¼ˆæå–é—œéµä¿¡æ¯ï¼‰
        import re
        
        # ç§»é™¤å†—é¤˜ä¿¡æ¯
        compressed = re.sub(r'\[.*?\]', '', content)  # ç§»é™¤æ–¹æ‹¬è™Ÿå…§å®¹
        compressed = re.sub(r'\s+', ' ', compressed)  # åˆä½µå¤šå€‹ç©ºæ ¼
        
        # é™åˆ¶é•·åº¦
        if len(compressed) > 200:
            compressed = compressed[:200] + "..."
        
        return compressed
    
    async def _filter_information(self, content: str) -> str:
        """ä¿¡æ¯éæ¿¾"""
        # æ¨¡æ“¬ä¿¡æ¯éæ¿¾
        # ç§»é™¤ä½è³ªé‡å…§å®¹
        filtered = content
        
        # ç§»é™¤å¸¸è¦‹çš„ç„¡ç”¨è©
        stop_words = ['é€™å€‹', 'é‚£å€‹', 'ä¸€äº›', 'å¾ˆå¤š', 'éå¸¸']
        for word in stop_words:
            filtered = filtered.replace(word, '')
        
        return filtered
    
    async def _calculate_compression_confidence(self, results: List[SearchResult]) -> float:
        """è¨ˆç®—å£“ç¸®ä¿¡å¿ƒå€¼"""
        if not results:
            return 0.0
        
        # åŸºæ–¼å£“ç¸®æ¯”ä¾‹å’Œä¿¡æ¯ä¿ç•™åº¦è¨ˆç®—ä¿¡å¿ƒå€¼
        total_compression_ratio = 0.0
        compressed_count = 0
        
        for result in results:
            if result.metadata.get('compressed', False):
                compression_ratio = result.metadata.get('compression_ratio', 1.0)
                total_compression_ratio += compression_ratio
                compressed_count += 1
        
        if compressed_count == 0:
            return 0.0
        
        avg_compression_ratio = total_compression_ratio / compressed_count
        
        # ç†æƒ³çš„å£“ç¸®æ¯”ä¾‹åœ¨ 0.3-0.7 ä¹‹é–“
        if 0.3 <= avg_compression_ratio <= 0.7:
            confidence = 0.9
        elif avg_compression_ratio < 0.3:
            confidence = avg_compression_ratio / 0.3 * 0.9
        else:
            confidence = (1.0 - avg_compression_ratio) / 0.3 * 0.9
        
        return min(confidence, 1.0)

class Level6HybridRAG(RAGLevel):
    """ç¬¬å…­å±¤ï¼šæ··åˆå¼RAG"""
    
    async def process(self, input_data: List[SearchResult]) -> Tuple[RAGResponse, float]:
        """åŸ·è¡Œæ··åˆå¼RAGç”Ÿæˆ"""
        logger.info(f"ğŸ” {self.name}: åŸ·è¡Œæ··åˆå¼RAGç”Ÿæˆ")
        
        start_time = time.time()
        
        try:
            # å¤šæ¨¡å‹ç”Ÿæˆ
            multi_model_response = await self._multi_model_generation(input_data)
            
            # è‡ªé©æ‡‰ç”Ÿæˆ
            adaptive_response = await self._adaptive_generation(multi_model_response, input_data)
            
            # è³ªé‡æ§åˆ¶
            quality_controlled_response = await self._quality_control(adaptive_response)
            
            processing_time = time.time() - start_time
            
            # å‰µå»ºæœ€çµ‚å›æ‡‰
            rag_response = RAGResponse(
                content=quality_controlled_response,
                confidence=0.9,  # æœ€çµ‚å±¤ç´šé€šå¸¸æœ‰è¼ƒé«˜ä¿¡å¿ƒå€¼
                sources=[r.document_id for r in input_data[:3]],  # å‰3å€‹ä¾†æº
                processing_time=processing_time,
                level_used='hybrid_rag',
                metadata={
                    'models_used': ['qwen2.5-7b', 'deepseek-coder-6.7b'],
                    'quality_controlled': True,
                    'adaptive_generation': True
                }
            )
            
            logger.info(f"âœ… {self.name}: ç”Ÿæˆå®Œæˆ, è™•ç†æ™‚é–“ {processing_time:.3f}s")
            return rag_response, 0.9
            
        except Exception as e:
            logger.error(f"âŒ {self.name}: ç”Ÿæˆå¤±æ•— - {e}")
            return None, 0.0
    
    async def _multi_model_generation(self, results: List[SearchResult]) -> str:
        """å¤šæ¨¡å‹ç”Ÿæˆ"""
        # æ¨¡æ“¬å¤šæ¨¡å‹ç”Ÿæˆ
        context = "\n".join([r.content for r in results[:3]])
        
        # ä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆ
        qwen_response = f"[Qwen2.5-7B] åŸºæ–¼æª¢ç´¢çµæœï¼Œæˆ‘æ¨è–¦ä»¥ä¸‹æ’­å®¢ç¯€ç›®ï¼š{context[:100]}..."
        deepseek_response = f"[DeepSeek-Coder] æ ¹æ“šåˆ†æï¼Œé€™äº›æ˜¯ç›¸é—œçš„ç§‘æŠ€æ’­å®¢ï¼š{context[:100]}..."
        
        # èåˆå›æ‡‰
        combined_response = f"{qwen_response}\n\n{deepseek_response}"
        return combined_response
    
    async def _adaptive_generation(self, base_response: str, results: List[SearchResult]) -> str:
        """è‡ªé©æ‡‰ç”Ÿæˆ"""
        # æ ¹æ“šæª¢ç´¢çµæœèª¿æ•´ç”Ÿæˆç­–ç•¥
        if len(results) > 5:
            # è±å¯Œçš„æª¢ç´¢çµæœï¼Œç”Ÿæˆè©³ç´°å›æ‡‰
            adaptive_response = f"[è©³ç´°æ¨¡å¼] {base_response} [åŒ…å«æ·±åº¦åˆ†æ]"
        else:
            # æœ‰é™çš„æª¢ç´¢çµæœï¼Œç”Ÿæˆç°¡æ½”å›æ‡‰
            adaptive_response = f"[ç°¡æ½”æ¨¡å¼] {base_response} [é‡é»æ‘˜è¦]"
        
        return adaptive_response
    
    async def _quality_control(self, response: str) -> str:
        """è³ªé‡æ§åˆ¶"""
        # æ¨¡æ“¬è³ªé‡æ§åˆ¶
        # æª¢æŸ¥äº‹å¯¦æº–ç¢ºæ€§
        if 'æ’­å®¢' in response and 'æ¨è–¦' in response:
            quality_response = f"[è³ªé‡é©—è­‰é€šé] {response}"
        else:
            quality_response = f"[éœ€è¦æ”¹é€²] {response}"
        
        return quality_response

class HierarchicalRAGPipeline:
    """å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG Pipeline"""
    
    def __init__(self, config_path: str = "config/hierarchical_rag_config.yaml"):
        """
        åˆå§‹åŒ–å±¤ç´šåŒ– RAG Pipeline
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾‘
        """
        self.config = self._load_config(config_path)
        self.levels = self._initialize_levels()
        self.fallback_service = None  # AnythingLLM å‚™æ´æœå‹™
        
        logger.info("ğŸŒ³ å±¤ç´šåŒ–æ¨¹ç‹€çµæ§‹ RAG Pipeline åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… é…ç½®è¼‰å…¥æˆåŠŸ: {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ é…ç½®è¼‰å…¥å¤±æ•—: {e}")
            return {}
    
    def _initialize_levels(self) -> Dict[str, RAGLevel]:
        """åˆå§‹åŒ–å„å±¤ç´š"""
        levels = {}
        
        # åˆå§‹åŒ–å„å±¤ç´š
        hierarchical_config = self.config.get('hierarchical_structure', {})
        
        levels['level_1'] = Level1QueryProcessing(
            hierarchical_config.get('level_1_query_processing', {})
        )
        
        levels['level_2'] = Level2HybridSearch(
            hierarchical_config.get('level_2_hybrid_search', {})
        )
        
        levels['level_3'] = Level3RetrievalAugmentation(
            hierarchical_config.get('level_3_retrieval_augmentation', {})
        )
        
        levels['level_4'] = Level4Reranking(
            hierarchical_config.get('level_4_reranking', {})
        )
        
        levels['level_5'] = Level5ContextCompression(
            hierarchical_config.get('level_5_context_compression', {})
        )
        
        levels['level_6'] = Level6HybridRAG(
            hierarchical_config.get('level_6_hybrid_rag', {})
        )
        
        logger.info(f"âœ… åˆå§‹åŒ–äº† {len(levels)} å€‹å±¤ç´š")
        return levels
    
    async def process_query(self, query: str) -> RAGResponse:
        """
        è™•ç†æŸ¥è©¢ï¼ˆå±¤ç´šåŒ–è™•ç†ï¼‰
        
        Args:
            query: ä½¿ç”¨è€…æŸ¥è©¢
            
        Returns:
            RAGResponse: æœ€çµ‚å›æ‡‰
        """
        logger.info(f"ğŸš€ é–‹å§‹å±¤ç´šåŒ–è™•ç†æŸ¥è©¢: {query[:50]}...")
        
        start_time = time.time()
        current_input = query
        level_used = "fallback"
        
        try:
            # ç¬¬ä¸€å±¤ï¼šæŸ¥è©¢è™•ç†
            level_1_result, confidence_1 = await self.levels['level_1'].process(current_input)
            if confidence_1 >= self.levels['level_1'].confidence_threshold:
                current_input = level_1_result
                level_used = "level_1"
                logger.info(f"âœ… ç¬¬ä¸€å±¤è™•ç†æˆåŠŸï¼Œä¿¡å¿ƒå€¼: {confidence_1:.3f}")
            else:
                logger.warning(f"âš ï¸ ç¬¬ä¸€å±¤ä¿¡å¿ƒå€¼ä¸è¶³ ({confidence_1:.3f})ï¼Œå˜—è©¦ç¬¬äºŒå±¤")
            
            # ç¬¬äºŒå±¤ï¼šæ··åˆæœå°‹
            level_2_result, confidence_2 = await self.levels['level_2'].process(current_input)
            if confidence_2 >= self.levels['level_2'].confidence_threshold:
                current_input = level_2_result
                level_used = "level_2"
                logger.info(f"âœ… ç¬¬äºŒå±¤è™•ç†æˆåŠŸï¼Œä¿¡å¿ƒå€¼: {confidence_2:.3f}")
            else:
                logger.warning(f"âš ï¸ ç¬¬äºŒå±¤ä¿¡å¿ƒå€¼ä¸è¶³ ({confidence_2:.3f})ï¼Œå˜—è©¦ç¬¬ä¸‰å±¤")
            
            # ç¬¬ä¸‰å±¤ï¼šæª¢ç´¢å¢å¼·
            level_3_result, confidence_3 = await self.levels['level_3'].process(current_input)
            if confidence_3 >= self.levels['level_3'].confidence_threshold:
                current_input = level_3_result
                level_used = "level_3"
                logger.info(f"âœ… ç¬¬ä¸‰å±¤è™•ç†æˆåŠŸï¼Œä¿¡å¿ƒå€¼: {confidence_3:.3f}")
            else:
                logger.warning(f"âš ï¸ ç¬¬ä¸‰å±¤ä¿¡å¿ƒå€¼ä¸è¶³ ({confidence_3:.3f})ï¼Œå˜—è©¦ç¬¬å››å±¤")
            
            # ç¬¬å››å±¤ï¼šé‡æ–°æ’åº
            level_4_result, confidence_4 = await self.levels['level_4'].process(current_input)
            if confidence_4 >= self.levels['level_4'].confidence_threshold:
                current_input = level_4_result
                level_used = "level_4"
                logger.info(f"âœ… ç¬¬å››å±¤è™•ç†æˆåŠŸï¼Œä¿¡å¿ƒå€¼: {confidence_4:.3f}")
            else:
                logger.warning(f"âš ï¸ ç¬¬å››å±¤ä¿¡å¿ƒå€¼ä¸è¶³ ({confidence_4:.3f})ï¼Œå˜—è©¦ç¬¬äº”å±¤")
            
            # ç¬¬äº”å±¤ï¼šä¸Šä¸‹æ–‡å£“ç¸®
            level_5_result, confidence_5 = await self.levels['level_5'].process(current_input)
            if confidence_5 >= self.levels['level_5'].confidence_threshold:
                current_input = level_5_result
                level_used = "level_5"
                logger.info(f"âœ… ç¬¬äº”å±¤è™•ç†æˆåŠŸï¼Œä¿¡å¿ƒå€¼: {confidence_5:.3f}")
            else:
                logger.warning(f"âš ï¸ ç¬¬äº”å±¤ä¿¡å¿ƒå€¼ä¸è¶³ ({confidence_5:.3f})ï¼Œå˜—è©¦ç¬¬å…­å±¤")
            
            # ç¬¬å…­å±¤ï¼šæ··åˆå¼RAG
            level_6_result, confidence_6 = await self.levels['level_6'].process(current_input)
            if confidence_6 >= self.levels['level_6'].confidence_threshold:
                level_used = "level_6"
                logger.info(f"âœ… ç¬¬å…­å±¤è™•ç†æˆåŠŸï¼Œä¿¡å¿ƒå€¼: {confidence_6:.3f}")
                
                # æ›´æ–°æœ€çµ‚å›æ‡‰çš„å±¤ç´šä¿¡æ¯
                level_6_result.level_used = level_used
                return level_6_result
            else:
                logger.warning(f"âš ï¸ ç¬¬å…­å±¤ä¿¡å¿ƒå€¼ä¸è¶³ ({confidence_6:.3f})ï¼Œä½¿ç”¨å‚™æ´æœå‹™")
        
        except Exception as e:
            logger.error(f"âŒ å±¤ç´šåŒ–è™•ç†å¤±æ•—: {e}")
        
        # å‚™æ´æœå‹™
        logger.info("ğŸ”„ ä½¿ç”¨å‚™æ´æœå‹™ (AnythingLLM)")
        fallback_response = await self._fallback_service(query)
        
        total_time = time.time() - start_time
        
        return RAGResponse(
            content=fallback_response,
            confidence=0.8,  # å‚™æ´æœå‹™çš„é è¨­ä¿¡å¿ƒå€¼
            sources=[],
            processing_time=total_time,
            level_used="fallback",
            metadata={'fallback_used': True, 'error': 'All levels failed'}
        )
    
    async def _fallback_service(self, query: str) -> str:
        """å‚™æ´æœå‹™"""
        # æ¨¡æ“¬ AnythingLLM å‚™æ´æœå‹™
        return f"[å‚™æ´æœå‹™] åŸºæ–¼æ‚¨çš„æŸ¥è©¢ã€Œ{query}ã€ï¼Œæˆ‘å»ºè­°æ‚¨æŸ¥çœ‹ç›¸é—œçš„æ’­å®¢ç¯€ç›®ã€‚ç”±æ–¼ç³»çµ±æ­£åœ¨å„ªåŒ–ä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

async def main():
    """ä¸»å‡½æ•¸"""
    # å»ºç«‹å±¤ç´šåŒ– RAG Pipeline
    pipeline = HierarchicalRAGPipeline()
    
    # æ¸¬è©¦æŸ¥è©¢
    test_queries = [
        "è«‹æ¨è–¦ä¸€äº›ç§‘æŠ€é¡çš„æ’­å®¢ç¯€ç›®",
        "åˆ†ææœ€è¿‘æ’­å®¢ç”¢æ¥­çš„ç™¼å±•è¶¨å‹¢",
        "æ¯”è¼ƒä¸åŒæ’­å®¢å¹³å°çš„ç‰¹è‰²"
    ]
    
    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"æ¸¬è©¦æŸ¥è©¢: {query}")
        print(f"{'='*60}")
        
        response = await pipeline.process_query(query)
        
        print(f"å›æ‡‰å…§å®¹: {response.content}")
        print(f"ä¿¡å¿ƒå€¼: {response.confidence}")
        print(f"ä½¿ç”¨å±¤ç´š: {response.level_used}")
        print(f"è™•ç†æ™‚é–“: {response.processing_time:.3f}s")
        print(f"ä¾†æº: {response.sources}")

if __name__ == "__main__":
    asyncio.run(main()) 