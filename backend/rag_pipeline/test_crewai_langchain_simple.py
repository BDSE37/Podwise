#!/usr/bin/env python3
"""
CrewAI + LangChain + LLM ç°¡åŒ–æ•´åˆæ¸¬è©¦è…³æœ¬

æ­¤è…³æœ¬æä¾›ç°¡åŒ–çš„æ•´åˆæ¸¬è©¦ï¼Œå°ˆæ³¨æ–¼æ ¸å¿ƒçµ„ä»¶çš„åŠŸèƒ½é©—è­‰ï¼Œ
é¿å…è¤‡é›œçš„ä¾è³´é—œä¿‚å’Œé¡å‹æª¢æŸ¥å•é¡Œã€‚

æ¸¬è©¦å…§å®¹ï¼š
- åŸºæœ¬çµ„ä»¶åˆå§‹åŒ–
- æ ¸å¿ƒåŠŸèƒ½é©—è­‰
- å·¥ä½œæµç¨‹æ¸¬è©¦

ä½œè€…: Podwise Team
ç‰ˆæœ¬: 1.0.0
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, Any, List, Optional

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SimpleIntegrationTest:
    """
    ç°¡åŒ–æ•´åˆæ¸¬è©¦é¡åˆ¥
    
    æ­¤é¡åˆ¥æä¾›åŸºæœ¬çš„æ•´åˆæ¸¬è©¦åŠŸèƒ½ï¼Œé¿å…è¤‡é›œçš„é¡å‹æª¢æŸ¥ã€‚
    """
    
    def __init__(self) -> None:
        """åˆå§‹åŒ–æ¸¬è©¦é¡åˆ¥"""
        self.test_results: Dict[str, Any] = {}
        self.start_time = datetime.now()
    
    async def test_component_imports(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦çµ„ä»¶å°å…¥
        
        Returns:
            Dict[str, Any]: æ¸¬è©¦çµæœ
        """
        logger.info("ğŸ§ª æ¸¬è©¦çµ„ä»¶å°å…¥...")
        
        results = {}
        
        try:
            # æ¸¬è©¦æ ¸å¿ƒçµ„ä»¶å°å…¥
            from core.crew_agents import AgentManager
            results["crew_agents"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… Crew Agents å°å…¥æˆåŠŸ")
        except Exception as e:
            results["crew_agents"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ Crew Agents å°å…¥å¤±æ•—: {str(e)}")
        
        try:
            from core.chat_history_service import ChatHistoryService
            results["chat_history"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… Chat History Service å°å…¥æˆåŠŸ")
        except Exception as e:
            results["chat_history"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ Chat History Service å°å…¥å¤±æ•—: {str(e)}")
        
        try:
            from core.qwen3_llm_manager import Qwen3LLMManager
            results["qwen3_manager"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… Qwen3 LLM Manager å°å…¥æˆåŠŸ")
        except Exception as e:
            results["qwen3_manager"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ Qwen3 LLM Manager å°å…¥å¤±æ•—: {str(e)}")
        
        try:
            from tools.keyword_mapper import KeywordMapper
            results["keyword_mapper"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… Keyword Mapper å°å…¥æˆåŠŸ")
        except Exception as e:
            results["keyword_mapper"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ Keyword Mapper å°å…¥å¤±æ•—: {str(e)}")
        
        try:
            from tools.knn_recommender import KNNRecommender
            results["knn_recommender"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… KNN Recommender å°å…¥æˆåŠŸ")
        except Exception as e:
            results["knn_recommender"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ KNN Recommender å°å…¥å¤±æ•—: {str(e)}")
        
        try:
            from tools.unified_llm_tool import UnifiedLLMTool
            results["unified_llm_tool"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… Unified LLM Tool å°å…¥æˆåŠŸ")
        except Exception as e:
            results["unified_llm_tool"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ Unified LLM Tool å°å…¥å¤±æ•—: {str(e)}")
        
        try:
            from config.integrated_config import get_config
            results["config"] = "âœ… å°å…¥æˆåŠŸ"
            logger.info("  âœ… Config å°å…¥æˆåŠŸ")
        except Exception as e:
            results["config"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            logger.error(f"  âŒ Config å°å…¥å¤±æ•—: {str(e)}")
        
        return {
            "component": "Component Imports",
            "status": "completed",
            "results": results,
            "success_count": len([r for r in results.values() if "âœ…" in r])
        }
    
    async def test_basic_functionality(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦åŸºæœ¬åŠŸèƒ½
        
        Returns:
            Dict[str, Any]: æ¸¬è©¦çµæœ
        """
        logger.info("ğŸ§ª æ¸¬è©¦åŸºæœ¬åŠŸèƒ½...")
        
        results = {}
        
        # æ¸¬è©¦ Keyword Mapper åŸºæœ¬åŠŸèƒ½
        try:
            from tools.keyword_mapper import KeywordMapper
            mapper = KeywordMapper()
            
            # æ¸¬è©¦åˆ†é¡åŠŸèƒ½
            test_queries = [
                "æˆ‘æƒ³äº†è§£è‚¡ç¥¨æŠ•è³‡",
                "æ¨è–¦å­¸ç¿’è³‡æº",
                "å•†æ¥­ç­–ç•¥åˆ†æ"
            ]
            
            classification_results = []
            for query in test_queries:
                try:
                    result = mapper.categorize_query(query)
                    classification_results.append({
                        "query": query,
                        "category": result.category,
                        "confidence": result.confidence
                    })
                except Exception as e:
                    classification_results.append({
                        "query": query,
                        "error": str(e)
                    })
            
            results["keyword_mapper"] = {
                "status": "âœ… åŠŸèƒ½æ­£å¸¸",
                "classifications": classification_results
            }
            logger.info("  âœ… Keyword Mapper åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé")
            
        except Exception as e:
            results["keyword_mapper"] = {
                "status": f"âŒ åŠŸèƒ½ç•°å¸¸: {str(e)}"
            }
            logger.error(f"  âŒ Keyword Mapper åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {str(e)}")
        
        # æ¸¬è©¦ KNN Recommender åŸºæœ¬åŠŸèƒ½
        try:
            from tools.knn_recommender import KNNRecommender
            recommender = KNNRecommender(k=5, metric="cosine")
            
            results["knn_recommender"] = {
                "status": "âœ… åˆå§‹åŒ–æˆåŠŸ",
                "k": 5,
                "metric": "cosine"
            }
            logger.info("  âœ… KNN Recommender åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé")
            
        except Exception as e:
            results["knn_recommender"] = {
                "status": f"âŒ åŠŸèƒ½ç•°å¸¸: {str(e)}"
            }
            logger.error(f"  âŒ KNN Recommender åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {str(e)}")
        
        # æ¸¬è©¦ LLM å·¥å…·åŸºæœ¬åŠŸèƒ½
        try:
            from tools.unified_llm_tool import get_unified_llm_tool
            llm_tool = get_unified_llm_tool()
            
            results["unified_llm_tool"] = {
                "status": "âœ… åˆå§‹åŒ–æˆåŠŸ",
                "available_models": llm_tool.get_available_models()
            }
            logger.info("  âœ… Unified LLM Tool åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé")
            
        except Exception as e:
            results["unified_llm_tool"] = {
                "status": f"âŒ åŠŸèƒ½ç•°å¸¸: {str(e)}"
            }
            logger.error(f"  âŒ Unified LLM Tool åŸºæœ¬åŠŸèƒ½æ¸¬è©¦å¤±æ•—: {str(e)}")
        
        return {
            "component": "Basic Functionality",
            "status": "completed",
            "results": results,
            "success_count": len([r for r in results.values() if "âœ…" in r.get("status", "")])
        }
    
    async def test_configuration(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦é…ç½®ç³»çµ±
        
        Returns:
            Dict[str, Any]: æ¸¬è©¦çµæœ
        """
        logger.info("ğŸ§ª æ¸¬è©¦é…ç½®ç³»çµ±...")
        
        try:
            from config.integrated_config import get_config
            config = get_config()
            
            # æª¢æŸ¥åŸºæœ¬é…ç½®
            config_check = {
                "database": hasattr(config, 'database'),
                "models": hasattr(config, 'models'),
                "api": hasattr(config, 'api'),
                "agent": hasattr(config, 'agent')
            }
            
            results = {
                "config_loaded": "âœ… é…ç½®è¼‰å…¥æˆåŠŸ",
                "config_sections": config_check,
                "total_sections": len(config_check),
                "valid_sections": sum(config_check.values())
            }
            
            logger.info("  âœ… é…ç½®ç³»çµ±æ¸¬è©¦é€šé")
            
        except Exception as e:
            results = {
                "config_loaded": f"âŒ é…ç½®è¼‰å…¥å¤±æ•—: {str(e)}"
            }
            logger.error(f"  âŒ é…ç½®ç³»çµ±æ¸¬è©¦å¤±æ•—: {str(e)}")
        
        return {
            "component": "Configuration System",
            "status": "completed",
            "results": results,
            "success_count": 1 if "âœ…" in results.get("config_loaded", "") else 0
        }
    
    async def test_crewai_integration(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦ CrewAI æ•´åˆ
        
        Returns:
            Dict[str, Any]: æ¸¬è©¦çµæœ
        """
        logger.info("ğŸ§ª æ¸¬è©¦ CrewAI æ•´åˆ...")
        
        try:
            from core.crew_agents import AgentManager
            from config.integrated_config import get_config
            
            config = get_config()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ä»£ç†äººé…ç½®
            if hasattr(config, 'get_agent_config'):
                agent_config = config.get_agent_config()
                agent_manager = AgentManager(agent_config)
                
                results = {
                    "agent_manager": "âœ… åˆå§‹åŒ–æˆåŠŸ",
                    "config_method": "âœ… é…ç½®æ–¹æ³•å­˜åœ¨",
                    "agent_count": len(agent_config.get('agents', {})) if isinstance(agent_config, dict) else 0
                }
                logger.info("  âœ… CrewAI æ•´åˆæ¸¬è©¦é€šé")
                
            else:
                results = {
                    "agent_manager": "âš ï¸ ç„¡æ³•åˆå§‹åŒ–",
                    "config_method": "âŒ é…ç½®æ–¹æ³•ä¸å­˜åœ¨"
                }
                logger.warning("  âš ï¸ CrewAI æ•´åˆæ¸¬è©¦éƒ¨åˆ†é€šé")
                
        except Exception as e:
            results = {
                "error": f"âŒ CrewAI æ•´åˆå¤±æ•—: {str(e)}"
            }
            logger.error(f"  âŒ CrewAI æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")
        
        return {
            "component": "CrewAI Integration",
            "status": "completed",
            "results": results,
            "success_count": len([r for r in results.values() if "âœ…" in str(r)])
        }
    
        async def test_langchain_integration(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦ LangChain æ•´åˆ
        
        Returns:
            Dict[str, Any]: æ¸¬è©¦çµæœ
        """
        logger.info("ğŸ§ª æ¸¬è©¦ LangChain æ•´åˆ...")
        
        try:
            # æª¢æŸ¥ LangChain ç›¸é—œå°å…¥
            langchain_imports = {}
            
            try:
                from langchain.tools import BaseTool
                langchain_imports["BaseTool"] = "âœ… å°å…¥æˆåŠŸ"
            except Exception as e:
                langchain_imports["BaseTool"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            
            try:
                from langchain_core.pydantic_v1 import BaseModel, Field
                langchain_imports["Pydantic"] = "âœ… å°å…¥æˆåŠŸ"
            except Exception as e:
                langchain_imports["Pydantic"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            
            try:
                from langchain.schema import BaseMessage
                langchain_imports["Schema"] = "âœ… å°å…¥æˆåŠŸ"
            except Exception as e:
                langchain_imports["Schema"] = f"âŒ å°å…¥å¤±æ•—: {str(e)}"
            
            # æ¸¬è©¦çµ±ä¸€ LLM å·¥å…·
            try:
                from tools.unified_llm_tool import UnifiedLLMTool
                tool = UnifiedLLMTool()
                
                results = {
                    "langchain_imports": langchain_imports,
                    "unified_llm_tool": "âœ… å·¥å…·å‰µå»ºæˆåŠŸ",
                    "tool_name": tool.name,
                    "tool_description": tool.description
                }
                logger.info("  âœ… LangChain æ•´åˆæ¸¬è©¦é€šé")
                
            except Exception as e:
                results = {
                    "langchain_imports": langchain_imports,
                    "unified_llm_tool": f"âŒ å·¥å…·å‰µå»ºå¤±æ•—: {str(e)}"
                }
                logger.error(f"  âŒ LangChain æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")
            
        except Exception as e:
            results = {
                "error": f"âŒ LangChain æ•´åˆå¤±æ•—: {str(e)}"
            }
            logger.error(f"  âŒ LangChain æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")
        
        return {
            "component": "LangChain Integration",
            "status": "completed",
            "results": results,
            "success_count": len([r for r in results.values() if "âœ…" in str(r)])
        }
    
    async def test_llm_integration(self) -> Dict[str, Any]:
        """
        æ¸¬è©¦ LLM æ•´åˆå’Œå‚™æ´æ©Ÿåˆ¶
        
        Returns:
            Dict[str, Any]: æ¸¬è©¦çµæœ
        """
        logger.info("ğŸ¤– æ¸¬è©¦ LLM æ•´åˆå’Œå‚™æ´æ©Ÿåˆ¶...")
        
        try:
            from core.qwen3_llm_manager import get_qwen3_llm_manager
            from config.integrated_config import get_config
            
            # æ¸¬è©¦ LLM ç®¡ç†å™¨
            llm_manager = get_qwen3_llm_manager()
            config = get_config()
            
            # æª¢æŸ¥å¯ç”¨æ¨¡å‹
            available_models = llm_manager.get_available_models()
            logger.info(f" å¯ç”¨æ¨¡å‹: {available_models}")
            
            # æª¢æŸ¥å„ªå…ˆç´šé †åº
            priority_models = config.models.llm_priority or []
            logger.info(f" å„ªå…ˆç´šé †åº: {priority_models}")
            
            # æª¢æŸ¥å°ç£æ¨¡å‹æ˜¯å¦ç‚ºç¬¬ä¸€å„ªå…ˆ
            taiwan_first = priority_models and priority_models[0] == "qwen2.5:taiwan"
            logger.info(f" å°ç£æ¨¡å‹ç¬¬ä¸€å„ªå…ˆ: {'âœ…' if taiwan_first else 'âŒ'}")
            
            # æ¸¬è©¦æ¨¡å‹å¥åº·æª¢æŸ¥
            current_model = llm_manager.current_model
            is_healthy = llm_manager.test_model_health(current_model)
            logger.info(f" ç•¶å‰æ¨¡å‹ {current_model} å¥åº·ç‹€æ…‹: {'âœ…' if is_healthy else 'âŒ'}")
            
            # æ¸¬è©¦å‚™æ´æ©Ÿåˆ¶
            best_model = llm_manager.get_best_model()
            logger.info(f" æœ€ä½³æ¨¡å‹: {best_model}")
            
            # æ¸¬è©¦æ¨¡å‹å‘¼å«
            test_prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šä½ å¥½"
            response = llm_manager.call_with_fallback(test_prompt)
            
            success = "éŒ¯èª¤" not in response and len(response) > 5
            logger.info(f" LLM å‘¼å«: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'}")
            
            # æª¢æŸ¥ OpenAI å‚™æ´é…ç½®
            openai_configured = bool(config.api.openai_api_key)
            logger.info(f" OpenAI å‚™æ´é…ç½®: {'âœ… å·²é…ç½®' if openai_configured else 'âŒ æœªé…ç½®'}")
            
            results = {
                "llm_manager": "âœ… åˆå§‹åŒ–æˆåŠŸ",
                "available_models": len(available_models),
                "current_model": current_model,
                "best_model": best_model,
                "is_healthy": is_healthy,
                "taiwan_first_priority": taiwan_first,
                "openai_configured": openai_configured,
                "llm_call_success": success,
                "response_length": len(response)
            }
            
            logger.info("  âœ… LLM æ•´åˆæ¸¬è©¦é€šé")
            
        except Exception as e:
            results = {
                "error": f"âŒ LLM æ•´åˆå¤±æ•—: {str(e)}"
            }
            logger.error(f"  âŒ LLM æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}")
        
        return {
            "component": "LLM Integration",
            "status": "completed",
            "results": results,
            "success_count": len([r for r in results.values() if "âœ…" in str(r)])
        }
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        
        Returns:
            Dict[str, Any]: å®Œæ•´æ¸¬è©¦çµæœ
        """
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œç°¡åŒ–æ•´åˆæ¸¬è©¦")
        logger.info("=" * 50)
        
        # åŸ·è¡Œå„é …æ¸¬è©¦
        tests = [
            ("component_imports", self.test_component_imports),
            ("basic_functionality", self.test_basic_functionality),
            ("configuration", self.test_configuration),
            ("crewai_integration", self.test_crewai_integration),
            ("langchain_integration", self.test_langchain_integration),
            ("llm_integration", self.test_llm_integration)
        ]
        
        for test_name, test_func in tests:
            try:
                logger.info(f"\nğŸ“‹ åŸ·è¡Œæ¸¬è©¦: {test_name}")
                result = await test_func()
                self.test_results[test_name] = result
                
                success_rate = result.get("success_count", 0) / max(1, len(result.get("results", {})))
                logger.info(f"  ğŸ“Š æˆåŠŸç‡: {success_rate:.2%}")
                
            except Exception as e:
                logger.error(f"âŒ æ¸¬è©¦ {test_name} åŸ·è¡Œå¤±æ•—: {str(e)}")
                self.test_results[test_name] = {
                    "component": test_name,
                    "status": "failed",
                    "error": str(e)
                }
        
        # ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
        total_tests = len(tests)
        successful_tests = len([r for r in self.test_results.values() if r.get("status") == "completed"])
        overall_success_rate = successful_tests / total_tests
        
        # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        test_summary = {
            "test_suite": "CrewAI + LangChain + LLM Simple Integration",
            "timestamp": datetime.now().isoformat(),
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "overall_success_rate": overall_success_rate,
            "total_processing_time": total_time,
            "test_results": self.test_results
        }
        
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š æ¸¬è©¦æ‘˜è¦")
        logger.info(f"  ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        logger.info(f"  æˆåŠŸæ¸¬è©¦æ•¸: {successful_tests}")
        logger.info(f"  æ•´é«”æˆåŠŸç‡: {overall_success_rate:.2%}")
        logger.info(f"  ç¸½è™•ç†æ™‚é–“: {total_time:.2f} ç§’")
        logger.info("=" * 50)
        
        return test_summary
    
    def save_test_results(self, filename: str = "simple_integration_test_results.json") -> None:
        """
        ä¿å­˜æ¸¬è©¦çµæœåˆ°æª”æ¡ˆ
        
        Args:
            filename: æª”æ¡ˆåç¨±
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"âœ… æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¸¬è©¦çµæœå¤±æ•—: {str(e)}")


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ§ Podwise RAG Pipeline - CrewAI + LangChain + LLM ç°¡åŒ–æ•´åˆæ¸¬è©¦")
    print("=" * 60)
    
    # å‰µå»ºæ¸¬è©¦å¥—ä»¶
    test_suite = SimpleIntegrationTest()
    
    try:
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        results = await test_suite.run_all_tests()
        
        # ä¿å­˜çµæœ
        test_suite.save_test_results()
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        if "error" not in results:
            print(f"\nğŸ‰ æ¸¬è©¦å®Œæˆï¼æ•´é«”æˆåŠŸç‡: {results['overall_success_rate']:.2%}")
            
            if results['overall_success_rate'] >= 0.8:
                print("âœ… ç°¡åŒ–æ•´åˆæ¸¬è©¦é€šéï¼æ ¸å¿ƒçµ„ä»¶é‹ä½œæ­£å¸¸ã€‚")
            elif results['overall_success_rate'] >= 0.6:
                print("âš ï¸  ç°¡åŒ–æ•´åˆæ¸¬è©¦éƒ¨åˆ†é€šéï¼Œéƒ¨åˆ†çµ„ä»¶éœ€è¦æª¢æŸ¥ã€‚")
            else:
                print("âŒ ç°¡åŒ–æ•´åˆæ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥æ ¸å¿ƒçµ„ä»¶ã€‚")
        else:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {results['error']}")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 