#!/usr/bin/env python3
"""
æª¢æŸ¥ PostgreSQL æ¬„ä½ mapping å’Œé‡è¤‡æ’å…¥å•é¡Œ
ç¢ºä¿ JSON è³‡æ–™æ­£ç¢ºå°æ‡‰åˆ°è³‡æ–™åº«æ¬„ä½ï¼Œä¸¦é¿å…é‡è¤‡æ’å…¥
"""

import os
import sys
import json
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
from typing import List, Dict, Any, Optional, Set
from pathlib import Path
import traceback

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MappingAndDuplicateChecker:
    """æª¢æŸ¥æ¬„ä½ mapping å’Œé‡è¤‡æ’å…¥çš„é¡åˆ¥"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        
        Args:
            config: è³‡æ–™åº«é…ç½®
        """
        self.config = config
        self.conn = None
        self._connect()
    
    def _connect(self):
        """å»ºç«‹è³‡æ–™åº«é€£æ¥"""
        try:
            self.conn = psycopg2.connect(
                host=self.config['host'],
                port=self.config['port'],
                database=self.config['database'],
                user=self.config['user'],
                password=self.config['password']
            )
            logger.info("PostgreSQL é€£æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            raise
    
    def get_table_columns(self, table_name: str) -> Dict[str, dict]:
        """ç²å–è¡¨æ ¼æ¬„ä½è³‡è¨Š
        
        Args:
            table_name: è¡¨æ ¼åç¨±
            
        Returns:
            æ¬„ä½è³‡è¨Šå­—å…¸
        """
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    SELECT column_name, data_type, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_name = %s 
                    ORDER BY ordinal_position
                """, (table_name,))
                
                columns = {}
                for row in cursor.fetchall():
                    columns[row[0]] = {
                        'type': row[1],
                        'nullable': row[2] == 'YES',
                        'default': row[3]
                    }
                return columns
                
        except Exception as e:
            logger.error(f"ç²å–è¡¨æ ¼æ¬„ä½å¤±æ•—: {e}")
            raise
    
    def get_primary_keys(self, table_name: str) -> List[str]:
        """ç²å–è¡¨æ ¼ä¸»éµ
        
        Args:
            table_name: è¡¨æ ¼åç¨±
            
        Returns:
            ä¸»éµæ¬„ä½åˆ—è¡¨
        """
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    SELECT column_name
                    FROM information_schema.key_column_usage
                    WHERE table_name = %s AND constraint_name LIKE '%%_pkey'
                    ORDER BY ordinal_position
                """, (table_name,))
                
                return [row[0] for row in cursor.fetchall()]
                
        except Exception as e:
            logger.error(f"ç²å–ä¸»éµå¤±æ•—: {e}")
            raise
    
    def check_existing_records(self, table_name: str, key_field: str, key_values: List[Any]) -> Set[Any]:
        """æª¢æŸ¥å·²å­˜åœ¨çš„è¨˜éŒ„
        
        Args:
            table_name: è¡¨æ ¼åç¨±
            key_field: ä¸»éµæ¬„ä½
            key_values: è¦æª¢æŸ¥çš„éµå€¼åˆ—è¡¨
            
        Returns:
            å·²å­˜åœ¨çš„éµå€¼é›†åˆ
        """
        try:
            with self.conn.cursor() as cursor:
                placeholders = ','.join(['%s'] * len(key_values))
                cursor.execute(f"""
                    SELECT {key_field}
                    FROM {table_name}
                    WHERE {key_field} IN ({placeholders})
                """, key_values)
                
                return {row[0] for row in cursor.fetchall()}
                
        except Exception as e:
            logger.error(f"æª¢æŸ¥ç¾æœ‰è¨˜éŒ„å¤±æ•—: {e}")
            raise
    
    def analyze_podcast_mapping(self, json_file_path: str) -> Dict[str, Any]:
        """åˆ†æ Podcast JSON æª”æ¡ˆçš„æ¬„ä½ mapping
        
        Args:
            json_file_path: JSON æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åˆ†æçµæœ
        """
        try:
            logger.info(f"åˆ†æ Podcast æª”æ¡ˆ: {json_file_path}")
            
            # è®€å– JSON æª”æ¡ˆ
            with open(json_file_path, 'r', encoding='utf-8') as f:
                podcast_data = json.load(f)
            
            # ç²å–è³‡æ–™åº«æ¬„ä½è³‡è¨Š
            db_columns = self.get_table_columns('podcasts')
            primary_keys = self.get_primary_keys('podcasts')
            
            # åˆ†ææ¬„ä½ mapping
            json_fields = set(podcast_data.keys())
            db_fields = set(db_columns.keys())
            
            # è¨ˆç®—å°æ‡‰é—œä¿‚
            mapped_fields = json_fields.intersection(db_fields)
            unmapped_json_fields = json_fields - db_fields
            missing_db_fields = db_fields - json_fields
            
            # æª¢æŸ¥ä¸»éµ
            podcast_id = podcast_data.get('id')
            if podcast_id:
                existing_ids = self.check_existing_records('podcasts', 'podcast_id', [int(podcast_id)])
                is_duplicate = int(podcast_id) in existing_ids
            else:
                is_duplicate = False
                existing_ids = set()
            
            return {
                'file_path': json_file_path,
                'podcast_id': podcast_id,
                'is_duplicate': is_duplicate,
                'existing_ids': list(existing_ids),
                'mapping_analysis': {
                    'total_json_fields': len(json_fields),
                    'total_db_fields': len(db_fields),
                    'mapped_fields': len(mapped_fields),
                    'unmapped_json_fields': list(unmapped_json_fields),
                    'missing_db_fields': list(missing_db_fields),
                    'field_mapping': {
                        'mapped': list(mapped_fields),
                        'unmapped': list(unmapped_json_fields)
                    }
                },
                'primary_keys': primary_keys,
                'sample_data': {
                    'title': podcast_data.get('title'),
                    'provider': podcast_data.get('provider'),
                    'rating': podcast_data.get('rating'),
                    'category': podcast_data.get('category')
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†æ Podcast æª”æ¡ˆå¤±æ•—: {json_file_path} - {e}")
            return {
                'file_path': json_file_path,
                'error': str(e),
                'trace': traceback.format_exc()
            }
    
    def analyze_rss_mapping(self, json_file_path: str) -> Dict[str, Any]:
        """åˆ†æ RSS JSON æª”æ¡ˆçš„æ¬„ä½ mapping
        
        Args:
            json_file_path: JSON æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åˆ†æçµæœ
        """
        try:
            logger.info(f"åˆ†æ RSS æª”æ¡ˆ: {json_file_path}")
            
            # è®€å– JSON æª”æ¡ˆ
            with open(json_file_path, 'r', encoding='utf-8') as f:
                episodes_data = json.load(f)
            
            if not isinstance(episodes_data, list):
                return {
                    'file_path': json_file_path,
                    'error': 'RSS æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼Œæ‡‰è©²æ˜¯é™£åˆ—æ ¼å¼'
                }
            
            # ç²å–è³‡æ–™åº«æ¬„ä½è³‡è¨Š
            db_columns = self.get_table_columns('episodes')
            primary_keys = self.get_primary_keys('episodes')
            
            # åˆ†æç¬¬ä¸€å€‹ episode çš„æ¬„ä½ mapping
            if episodes_data:
                first_episode = episodes_data[0]
                json_fields = set(first_episode.keys())
                db_fields = set(db_columns.keys())
                
                mapped_fields = json_fields.intersection(db_fields)
                unmapped_json_fields = json_fields - db_fields
                missing_db_fields = db_fields - json_fields
            else:
                mapped_fields = set()
                unmapped_json_fields = set()
                missing_db_fields = set()
                json_fields = set()
                db_fields = set(db_columns.keys())
            
            # æª¢æŸ¥é‡è¤‡çš„ episodes
            episode_titles = [ep.get('title', '') for ep in episodes_data if ep.get('title')]
            existing_titles = self.check_existing_records('episodes', 'episode_title', episode_titles)
            
            duplicate_titles = set(episode_titles).intersection(existing_titles)
            
            return {
                'file_path': json_file_path,
                'total_episodes': len(episodes_data),
                'duplicate_analysis': {
                    'total_titles': len(episode_titles),
                    'existing_titles': len(existing_titles),
                    'duplicate_titles': len(duplicate_titles),
                    'duplicate_list': list(duplicate_titles)[:10]  # åªé¡¯ç¤ºå‰10å€‹
                },
                'mapping_analysis': {
                    'total_json_fields': len(json_fields),
                    'total_db_fields': len(db_fields),
                    'mapped_fields': len(mapped_fields),
                    'unmapped_json_fields': list(unmapped_json_fields),
                    'missing_db_fields': list(missing_db_fields),
                    'field_mapping': {
                        'mapped': list(mapped_fields),
                        'unmapped': list(unmapped_json_fields)
                    }
                },
                'primary_keys': primary_keys,
                'sample_episode': episodes_data[0] if episodes_data else None
            }
            
        except Exception as e:
            logger.error(f"åˆ†æ RSS æª”æ¡ˆå¤±æ•—: {json_file_path} - {e}")
            return {
                'file_path': json_file_path,
                'error': str(e),
                'trace': traceback.format_exc()
            }
    
    def check_batch_input_folder(self, input_folder: str) -> Dict[str, Any]:
        """æª¢æŸ¥æ•´å€‹ batch_input è³‡æ–™å¤¾
        
        Args:
            input_folder: è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘
            
        Returns:
            æª¢æŸ¥çµæœæ‘˜è¦
        """
        logger.info(f"é–‹å§‹æª¢æŸ¥è³‡æ–™å¤¾: {input_folder}")
        
        if not os.path.exists(input_folder):
            return {"error": "è³‡æ–™å¤¾ä¸å­˜åœ¨"}
        
        json_files = [f for f in os.listdir(input_folder) if f.endswith('.json')]
        logger.info(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
        
        results = {
            'podcasts': [],
            'rss_files': [],
            'summary': {
                'total_files': len(json_files),
                'podcast_files': 0,
                'rss_files': 0,
                'duplicates_found': 0,
                'mapping_issues': 0
            }
        }
        
        for filename in json_files:
            file_path = os.path.join(input_folder, filename)
            
            if filename.startswith('podcast_'):
                result = self.analyze_podcast_mapping(file_path)
                results['podcasts'].append(result)
                results['summary']['podcast_files'] += 1
                
                if result.get('is_duplicate'):
                    results['summary']['duplicates_found'] += 1
                if result.get('mapping_analysis', {}).get('unmapped_json_fields'):
                    results['summary']['mapping_issues'] += 1
                    
            elif filename.startswith('RSS_') or filename.startswith('Spotify_RSS_'):
                result = self.analyze_rss_mapping(file_path)
                results['rss_files'].append(result)
                results['summary']['rss_files'] += 1
                
                if result.get('duplicate_analysis', {}).get('duplicate_titles'):
                    results['summary']['duplicates_found'] += 1
                if result.get('mapping_analysis', {}).get('unmapped_json_fields'):
                    results['summary']['mapping_issues'] += 1
            else:
                logger.warning(f"è·³éæœªçŸ¥æ ¼å¼æª”æ¡ˆ: {filename}")
        
        return results
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæª¢æŸ¥å ±å‘Š
        
        Args:
            results: æª¢æŸ¥çµæœ
            
        Returns:
            å ±å‘Šæ–‡å­—
        """
        report = []
        report.append("=" * 60)
        report.append("PostgreSQL æ¬„ä½ Mapping å’Œé‡è¤‡æª¢æŸ¥å ±å‘Š")
        report.append("=" * 60)
        
        # æ‘˜è¦
        summary = results.get('summary', {})
        report.append(f"\nğŸ“Š æ‘˜è¦çµ±è¨ˆ:")
        report.append(f"   - ç¸½æª”æ¡ˆæ•¸: {summary.get('total_files', 0)}")
        report.append(f"   - Podcast æª”æ¡ˆ: {summary.get('podcast_files', 0)}")
        report.append(f"   - RSS æª”æ¡ˆ: {summary.get('rss_files', 0)}")
        report.append(f"   - ç™¼ç¾é‡è¤‡: {summary.get('duplicates_found', 0)}")
        report.append(f"   - Mapping å•é¡Œ: {summary.get('mapping_issues', 0)}")
        
        # Podcast æª”æ¡ˆåˆ†æ
        if results.get('podcasts'):
            report.append(f"\nğŸ™ï¸ Podcast æª”æ¡ˆåˆ†æ:")
            for podcast in results['podcasts']:
                if 'error' in podcast:
                    report.append(f"   âŒ {Path(podcast['file_path']).name}: {podcast['error']}")
                    continue
                
                report.append(f"   ğŸ“ {Path(podcast['file_path']).name}:")
                report.append(f"      - Podcast ID: {podcast.get('podcast_id', 'N/A')}")
                report.append(f"      - é‡è¤‡æª¢æŸ¥: {'âŒ é‡è¤‡' if podcast.get('is_duplicate') else 'âœ… æ–°è¨˜éŒ„'}")
                
                mapping = podcast.get('mapping_analysis', {})
                report.append(f"      - æ¬„ä½å°æ‡‰: {mapping.get('mapped_fields', 0)}/{mapping.get('total_json_fields', 0)}")
                
                if mapping.get('unmapped_json_fields'):
                    report.append(f"      - æœªå°æ‡‰æ¬„ä½: {', '.join(mapping['unmapped_json_fields'][:5])}")
                    if len(mapping['unmapped_json_fields']) > 5:
                        report.append(f"        ... é‚„æœ‰ {len(mapping['unmapped_json_fields']) - 5} å€‹æ¬„ä½")
        
        # RSS æª”æ¡ˆåˆ†æ
        if results.get('rss_files'):
            report.append(f"\nğŸ“» RSS æª”æ¡ˆåˆ†æ:")
            for rss in results['rss_files']:
                if 'error' in rss:
                    report.append(f"   âŒ {Path(rss['file_path']).name}: {rss['error']}")
                    continue
                
                report.append(f"   ğŸ“ {Path(rss['file_path']).name}:")
                report.append(f"      - Episode æ•¸é‡: {rss.get('total_episodes', 0)}")
                
                duplicate_analysis = rss.get('duplicate_analysis', {})
                report.append(f"      - é‡è¤‡æ¨™é¡Œ: {duplicate_analysis.get('duplicate_titles', 0)}")
                
                mapping = rss.get('mapping_analysis', {})
                report.append(f"      - æ¬„ä½å°æ‡‰: {mapping.get('mapped_fields', 0)}/{mapping.get('total_json_fields', 0)}")
                
                if mapping.get('unmapped_json_fields'):
                    report.append(f"      - æœªå°æ‡‰æ¬„ä½: {', '.join(mapping['unmapped_json_fields'][:5])}")
                    if len(mapping['unmapped_json_fields']) > 5:
                        report.append(f"        ... é‚„æœ‰ {len(mapping['unmapped_json_fields']) - 5} å€‹æ¬„ä½")
        
        # å»ºè­°
        report.append(f"\nğŸ’¡ å»ºè­°:")
        if summary.get('duplicates_found', 0) > 0:
            report.append("   - ç™¼ç¾é‡è¤‡è¨˜éŒ„ï¼Œå»ºè­°ä½¿ç”¨ UPSERT æ“ä½œ")
        if summary.get('mapping_issues', 0) > 0:
            report.append("   - ç™¼ç¾æœªå°æ‡‰æ¬„ä½ï¼Œè«‹æª¢æŸ¥æ¬„ä½åç¨±æ˜¯å¦æ­£ç¢º")
        if summary.get('total_files', 0) == 0:
            report.append("   - æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆï¼Œè«‹æª¢æŸ¥è³‡æ–™å¤¾è·¯å¾‘")
        
        report.append("\n" + "=" * 60)
        return "\n".join(report)
    
    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

def main():
    """ä¸»ç¨‹å¼"""
    logger.info("=== PostgreSQL æ¬„ä½ Mapping å’Œé‡è¤‡æª¢æŸ¥ ===")
    
    # è³‡æ–™åº«é…ç½®
    config = {
        'host': os.getenv('POSTGRES_HOST', 'postgres.podwise.svc.cluster.local'),
        'port': int(os.getenv('POSTGRES_PORT', 5432)),
        'database': os.getenv('POSTGRES_DB', 'podcast'),
        'user': os.getenv('POSTGRES_USER', 'bdse37'),
        'password': os.getenv('POSTGRES_PASSWORD', '111111')
    }
    
    # è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘
    input_folder = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'batch_input'
    )
    
    try:
        # å»ºç«‹æª¢æŸ¥å™¨
        checker = MappingAndDuplicateChecker(config)
        
        # åŸ·è¡Œæª¢æŸ¥
        results = checker.check_batch_input_folder(input_folder)
        
        # ç”Ÿæˆå ±å‘Š
        report = checker.generate_report(results)
        print(report)
        
        # å„²å­˜å ±å‘Šåˆ°æª”æ¡ˆ
        report_file = os.path.join(os.path.dirname(input_folder), 'mapping_check_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æª¢æŸ¥å ±å‘Šå·²å„²å­˜åˆ°: {report_file}")
        
        # é—œé–‰é€£æ¥
        checker.close()
        
    except Exception as e:
        logger.error(f"æª¢æŸ¥å¤±æ•—: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 