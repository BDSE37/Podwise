#!/usr/bin/env python3
"""
æ‰¹æ¬¡ä¸Šå‚³ batch_input è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ JSON æª”æ¡ˆåˆ° PostgreSQL
æ”¯æ´ RSS å’Œ Podcast æ ¼å¼çš„ JSON æª”æ¡ˆ
"""

import os
import sys
import json
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import traceback

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BatchPostgreSQLUploader:
    """æ‰¹æ¬¡ PostgreSQL è³‡æ–™ä¸Šå‚³é¡åˆ¥"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        
        Args:
            config: è³‡æ–™åº«é…ç½®
        """
        self.config = config
        self.conn = None
        self._connect()
        self._ensure_tables_exist()
    
    def _connect(self):
        """å»ºç«‹è³‡æ–™åº«é€£æ¥"""
        try:
            self.conn = psycopg2.connect(
                host=self.config['host'],
                port=self.config['port'],
                database=self.config['database'],
                user=self.config['user'],
                password=self.config['password']
            )
            self.conn.autocommit = False
            logger.info("PostgreSQL é€£æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            raise
    
    def _ensure_tables_exist(self):
        """ç¢ºä¿å¿…è¦çš„è¡¨æ ¼å­˜åœ¨"""
        try:
            with self.conn.cursor() as cursor:
                # æª¢æŸ¥ episodes è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'episodes'
                    );
                """)
                
                if not cursor.fetchone()[0]:
                    logger.info("å»ºç«‹ episodes è¡¨æ ¼...")
                    cursor.execute("""
                        CREATE TABLE episodes (
                            episode_id SERIAL PRIMARY KEY,
                            podcast_id INTEGER NOT NULL,
                            episode_title VARCHAR(255) NOT NULL,
                            published_date DATE,
                            audio_url VARCHAR(512),
                            duration INTEGER,
                            description TEXT,
                            audio_preview_url VARCHAR(512),
                            languages VARCHAR(64),
                            explicit BOOLEAN,
                            created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            apple_episodes_ranking INTEGER
                        );
                    """)
                    
                    # å»ºç«‹ç´¢å¼•
                    cursor.execute("""
                        CREATE INDEX idx_episodes_podcast_id ON episodes(podcast_id);
                        CREATE INDEX idx_episodes_published_date ON episodes(published_date);
                    """)
                    
                    logger.info("episodes è¡¨æ ¼å»ºç«‹å®Œæˆ")
                
                # æª¢æŸ¥ podcasts è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'podcasts'
                    );
                """)
                
                if not cursor.fetchone()[0]:
                    logger.info("å»ºç«‹ podcasts è¡¨æ ¼...")
                    cursor.execute("""
                        CREATE TABLE podcasts (
                            podcast_id INTEGER PRIMARY KEY,
                            name VARCHAR(255) NOT NULL,
                            description TEXT,
                            author VARCHAR(255),
                            rss_link VARCHAR(512) UNIQUE,
                            images_640 VARCHAR(512),
                            images_300 VARCHAR(512),
                            images_64 VARCHAR(512),
                            languages VARCHAR(64),
                            created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            apple_podcasts_ranking INTEGER,
                            apple_rating DECIMAL(3,2),
                            category VARCHAR(64),
                            update_frequency VARCHAR(64)
                        );
                    """)
                    
                    logger.info("podcasts è¡¨æ ¼å»ºç«‹å®Œæˆ")
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼å»ºç«‹å¤±æ•—: {e}")
            self.conn.rollback()
            raise
    
    def insert_podcast(self, podcast_data: Dict[str, Any]) -> int:
        """æ’å…¥ Podcast è³‡æ–™
        
        Args:
            podcast_data: Podcast è³‡æ–™
            
        Returns:
            podcast_id: æ’å…¥çš„ Podcast ID
        """
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO podcasts (
                        podcast_id, name, description, author, rss_link, 
                        languages, category, created_at, updated_at,
                        apple_rating, update_frequency
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (podcast_id) DO UPDATE SET
                        name = EXCLUDED.name,
                        description = EXCLUDED.description,
                        author = EXCLUDED.author,
                        rss_link = EXCLUDED.rss_link,
                        languages = EXCLUDED.languages,
                        category = EXCLUDED.category,
                        apple_rating = EXCLUDED.apple_rating,
                        update_frequency = EXCLUDED.update_frequency,
                        updated_at = CURRENT_TIMESTAMP
                    RETURNING podcast_id
                """, (
                    podcast_data.get('podcast_id'),
                    podcast_data.get('title', ''),
                    podcast_data.get('description', ''),
                    podcast_data.get('provider', ''),
                    podcast_data.get('rss_link', ''),
                    podcast_data.get('languages', 'zh-TW'),
                    podcast_data.get('category', ''),
                    datetime.now(),
                    datetime.now(),
                    self._extract_rating(podcast_data.get('rating', '')),
                    podcast_data.get('update_frequency', '')
                ))
                
                podcast_id = cursor.fetchone()[0]
                self.conn.commit()
                logger.info(f"Podcast æ’å…¥æˆåŠŸ: {podcast_data.get('title', 'Unknown')} (ID: {podcast_id})")
                return podcast_id
                
        except Exception as e:
            logger.error(f"Podcast æ’å…¥å¤±æ•—: {e}")
            self.conn.rollback()
            raise
    
    def _extract_rating(self, rating_str: str) -> Optional[float]:
        """å¾è©•åˆ†å­—ä¸²ä¸­æå–æ•¸å€¼"""
        if not rating_str:
            return None
        try:
            # è™•ç† "4.8ï¼ˆ3.3è¬å‰‡è©•åˆ†ï¼‰" æ ¼å¼
            rating = rating_str.split('ï¼ˆ')[0]
            return float(rating)
        except:
            return None
    
    def insert_episode(self, episode_data: Dict[str, Any], podcast_id: int) -> int:
        """æ’å…¥ Episode è³‡æ–™
        
        Args:
            episode_data: Episode è³‡æ–™
            podcast_id: å°æ‡‰çš„ Podcast ID
            
        Returns:
            episode_id: æ’å…¥çš„ Episode ID
        """
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO episodes (
                        podcast_id, episode_title, published_date, audio_url, 
                        duration, description, audio_preview_url, languages, 
                        explicit, created_at, updated_at, apple_episodes_ranking
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING episode_id
                """, (
                    podcast_id,
                    episode_data.get('title', ''),
                    self._parse_date(episode_data.get('published', '')),
                    episode_data.get('audio_url', ''),
                    episode_data.get('duration'),
                    episode_data.get('description', ''),
                    episode_data.get('audio_preview_url', ''),
                    episode_data.get('languages', 'zh-TW'),
                    episode_data.get('explicit', False),
                    datetime.now(),
                    datetime.now(),
                    episode_data.get('apple_episodes_ranking')
                ))
                
                episode_id = cursor.fetchone()[0]
                self.conn.commit()
                return episode_id
                
        except Exception as e:
            logger.error(f"Episode æ’å…¥å¤±æ•—: {e}")
            self.conn.rollback()
            raise
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ä¸²"""
        if not date_str:
            return None
        try:
            # è™•ç† "Wed, 02 Jul 2025 08:59:01 GMT" æ ¼å¼
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str).date()
        except:
            try:
                # å˜—è©¦å…¶ä»–æ ¼å¼
                return datetime.strptime(date_str, '%Y-%m-%d').date()
            except:
                return None
    
    def get_table_columns(self, table_name: str) -> Dict[str, dict]:
        """å–å¾—è¡¨æ ¼æ‰€æœ‰æ¬„ä½è³‡è¨Šï¼ˆå‹åˆ¥ã€NOT NULLã€é è¨­å€¼ï¼‰"""
        columns = {}
        try:
            with self.conn.cursor(cursor_factory=RealDictCursor) as cursor:
                cursor.execute("""
                    SELECT column_name, data_type, is_nullable, column_default
                    FROM information_schema.columns
                    WHERE table_name = %s
                    ORDER BY ordinal_position
                """, (table_name,))
                for row in cursor.fetchall():
                    columns[row['column_name']] = {
                        'type': row['data_type'],
                        'not_null': row['is_nullable'] == 'NO',
                        'default': row['column_default']
                    }
        except Exception as e:
            logger.error(f"å–å¾— {table_name} æ¬„ä½è³‡è¨Šå¤±æ•—: {e}")
        return columns

    def get_primary_keys(self, table_name: str) -> List[str]:
        """å–å¾—ä¸»éµæ¬„ä½åç¨±"""
        keys = []
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    SELECT a.attname
                    FROM   pg_index i
                    JOIN   pg_attribute a ON a.attrelid = i.indrelid
                                         AND a.attnum = ANY(i.indkey)
                    WHERE  i.indrelid = %s::regclass
                    AND    i.indisprimary;
                """, (table_name,))
                keys = [row[0] for row in cursor.fetchall()]
        except Exception as e:
            logger.error(f"å–å¾— {table_name} ä¸»éµå¤±æ•—: {e}")
        return keys

    def convert_type(self, value, col_type):
        """æ ¹æ“šå‹åˆ¥è‡ªå‹•è½‰æ›"""
        if value is None:
            return None
        try:
            if col_type in ('integer', 'bigint', 'smallint'):
                return int(value)
            elif col_type in ('double precision', 'real', 'numeric', 'decimal'):
                return float(value)
            elif col_type in ('boolean',):
                if isinstance(value, bool):
                    return value
                if str(value).lower() in ['true', '1', 't', 'yes', 'y']:
                    return True
                return False
            elif col_type in ('date',):
                if isinstance(value, datetime):
                    return value.date()
                from dateutil.parser import parse
                return parse(value).date()
            elif col_type in ('timestamp without time zone', 'timestamp with time zone'):
                if isinstance(value, datetime):
                    return value
                from dateutil.parser import parse
                return parse(value)
            else:
                return str(value)
        except Exception:
            return None

    def prepare_row(self, record: dict, columns: dict) -> dict:
        """æ ¹æ“šæ¬„ä½è³‡è¨Šå‹•æ…‹ç”¢ç”Ÿæ’å…¥è³‡æ–™"""
        row = {}
        for col, info in columns.items():
            if col in record:
                row[col] = self.convert_type(record[col], info['type'])
            elif info['default'] is not None:
                # è™•ç† serial, now(), 'NULL' ç­‰é è¨­å€¼
                if info['default'].startswith('nextval') or info['default'] == 'NULL':
                    row[col] = None
                elif info['default'].startswith('now()'):
                    row[col] = datetime.now()
                else:
                    row[col] = info['default']
            else:
                row[col] = None
        return row

    def validate_row(self, row: dict, columns: dict, primary_keys: List[str]) -> Optional[str]:
        """æª¢æŸ¥ä¸»éµ/NOT NULL æ¬„ä½"""
        for pk in primary_keys:
            if row.get(pk) is None:
                return f"ç¼ºå°‘ä¸»éµæ¬„ä½: {pk}"
        for col, info in columns.items():
            if info['not_null'] and row.get(col) is None:
                return f"ç¼ºå°‘ NOT NULL æ¬„ä½: {col}"
        return None

    def upsert_row(self, table: str, row: dict, primary_keys: List[str]):
        """åŸ·è¡Œ upsert"""
        cols = list(row.keys())
        values = [row[c] for c in cols]
        update_cols = [c for c in cols if c not in primary_keys]
        set_clause = ', '.join([f"{c}=EXCLUDED.{c}" for c in update_cols])
        conflict_clause = ', '.join(primary_keys)
        placeholders = ', '.join(['%s'] * len(cols))
        sql = f"""
            INSERT INTO {table} ({', '.join(cols)})
            VALUES ({placeholders})
            ON CONFLICT ({conflict_clause}) DO UPDATE SET {set_clause}
        """
        with self.conn.cursor() as cursor:
            cursor.execute(sql, values)
        self.conn.commit()

    def process_json_file(self, file_path: str, table: str) -> Dict[str, Any]:
        """é€šç”¨è™•ç† JSON æª”æ¡ˆï¼Œå‹•æ…‹å°æ‡‰æ¬„ä½ä¸¦ upsert"""
        columns = self.get_table_columns(table)
        primary_keys = self.get_primary_keys(table)
        success, fail, errors = 0, 0, []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if table == 'episodes' and not isinstance(data, list):
                return {"success": False, "error": "episodes æª”æ¡ˆæ ¼å¼éŒ¯èª¤"}
            if table == 'podcasts' and not isinstance(data, dict):
                return {"success": False, "error": "podcasts æª”æ¡ˆæ ¼å¼éŒ¯èª¤"}
            records = data if isinstance(data, list) else [data]
            for idx, record in enumerate(records):
                try:
                    row = self.prepare_row(record, columns)
                    err = self.validate_row(row, columns, primary_keys)
                    if err:
                        fail += 1
                        errors.append({"index": idx, "reason": err, "data": record})
                        continue
                    self.upsert_row(table, row, primary_keys)
                    success += 1
                except Exception as e:
                    fail += 1
                    errors.append({"index": idx, "reason": str(e), "trace": traceback.format_exc(), "data": record})
            return {"success": True, "total": len(records), "success_count": success, "fail_count": fail, "errors": errors}
        except Exception as e:
            return {"success": False, "error": str(e), "trace": traceback.format_exc()}

    def process_rss_file(self, file_path: str) -> Dict[str, Any]:
        """è™•ç† RSS æ ¼å¼çš„ JSON æª”æ¡ˆ
        
        Args:
            file_path: JSON æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è™•ç†çµæœçµ±è¨ˆ
        """
        try:
            logger.info(f"è™•ç† RSS æª”æ¡ˆ: {file_path}")
            
            # å¾æª”æ¡ˆåæå– podcast_id
            filename = Path(file_path).stem
            if filename.startswith('RSS_'):
                podcast_id = int(filename.split('_')[1])
            elif filename.startswith('Spotify_RSS_'):
                podcast_id = int(filename.split('_')[2])
            else:
                logger.error(f"ç„¡æ³•å¾æª”æ¡ˆåæå– podcast_id: {filename}")
                return {"success": False, "error": "ç„¡æ³•æå– podcast_id"}
            
            # è®€å– JSON æª”æ¡ˆ
            with open(file_path, 'r', encoding='utf-8') as f:
                episodes_data = json.load(f)
            
            if not isinstance(episodes_data, list):
                logger.error(f"RSS æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {file_path}")
                return {"success": False, "error": "æª”æ¡ˆæ ¼å¼éŒ¯èª¤"}
            
            # æ’å…¥ episodes
            success_count = 0
            for episode in episodes_data:
                try:
                    self.insert_episode(episode, podcast_id)
                    success_count += 1
                except Exception as e:
                    logger.error(f"æ’å…¥ episode å¤±æ•—: {episode.get('title', 'Unknown')} - {e}")
            
            logger.info(f"RSS æª”æ¡ˆè™•ç†å®Œæˆ: {file_path}, æˆåŠŸæ’å…¥ {success_count}/{len(episodes_data)} ç­†")
            return {
                "success": True,
                "total_episodes": len(episodes_data),
                "successful_inserts": success_count,
                "failed_inserts": len(episodes_data) - success_count
            }
            
        except Exception as e:
            logger.error(f"è™•ç† RSS æª”æ¡ˆå¤±æ•—: {file_path} - {e}")
            return {"success": False, "error": str(e)}
    
    def process_podcast_file(self, file_path: str) -> Dict[str, Any]:
        """è™•ç† Podcast æ ¼å¼çš„ JSON æª”æ¡ˆ
        
        Args:
            file_path: JSON æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è™•ç†çµæœçµ±è¨ˆ
        """
        try:
            logger.info(f"è™•ç† Podcast æª”æ¡ˆ: {file_path}")
            
            # å¾æª”æ¡ˆåæå– podcast_id
            filename = Path(file_path).stem
            if filename.startswith('podcast_'):
                podcast_id = int(filename.split('_')[1])
            else:
                logger.error(f"ç„¡æ³•å¾æª”æ¡ˆåæå– podcast_id: {filename}")
                return {"success": False, "error": "ç„¡æ³•æå– podcast_id"}
            
            # è®€å– JSON æª”æ¡ˆ
            with open(file_path, 'r', encoding='utf-8') as f:
                podcast_data = json.load(f)
            
            if not isinstance(podcast_data, dict):
                logger.error(f"Podcast æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {file_path}")
                return {"success": False, "error": "æª”æ¡ˆæ ¼å¼éŒ¯èª¤"}
            
            # æ·»åŠ  podcast_id
            podcast_data['podcast_id'] = podcast_id
            
            # æ’å…¥ podcast
            try:
                self.insert_podcast(podcast_data)
                logger.info(f"Podcast æª”æ¡ˆè™•ç†å®Œæˆ: {file_path}")
                return {
                    "success": True,
                    "podcast_inserted": True
                }
            except Exception as e:
                logger.error(f"æ’å…¥ podcast å¤±æ•—: {e}")
                return {"success": False, "error": str(e)}
            
        except Exception as e:
            logger.error(f"è™•ç† Podcast æª”æ¡ˆå¤±æ•—: {file_path} - {e}")
            return {"success": False, "error": str(e)}
    
    def process_batch_input_folder(self, input_folder: str) -> Dict[str, Any]:
        logger.info(f"é–‹å§‹è™•ç†è³‡æ–™å¤¾: {input_folder}")
        if not os.path.exists(input_folder):
            logger.error(f"è³‡æ–™å¤¾ä¸å­˜åœ¨: {input_folder}")
            return {"success": False, "error": "è³‡æ–™å¤¾ä¸å­˜åœ¨"}
        json_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith('.json')]
        logger.info(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
        summary = []
        for file_path in json_files:
            filename = Path(file_path).name
            if filename.startswith('RSS_') or filename.startswith('Spotify_RSS_'):
                table = 'episodes'
            elif filename.startswith('podcast_'):
                table = 'podcasts'
            else:
                logger.warning(f"è·³éæœªçŸ¥æ ¼å¼æª”æ¡ˆ: {filename}")
                continue
            logger.info(f"è™•ç†æª”æ¡ˆ: {filename} -> {table}")
            result = self.process_json_file(file_path, table)
            summary.append({"file": filename, **result})
            if not result.get('success'):
                logger.error(f"è™•ç†å¤±æ•—: {filename} - {result.get('error')}\n{result.get('trace', '')}")
        return summary
    
    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

def main():
    """ä¸»ç¨‹å¼"""
    logger.info("=== æ‰¹æ¬¡ä¸Šå‚³ batch_input è³‡æ–™å¤¾åˆ° PostgreSQL ===")
    
    # è³‡æ–™åº«é…ç½® - ä½¿ç”¨æ­£ç¢ºçš„ PostgreSQL æœå‹™åœ°å€
    config = {
        'host': os.getenv('POSTGRES_HOST', 'postgres.podwise.svc.cluster.local'),
        'port': int(os.getenv('POSTGRES_PORT', 5432)),
        'database': os.getenv('POSTGRES_DB', 'podcast'),
        'user': os.getenv('POSTGRES_USER', 'bdse37'),
        'password': os.getenv('POSTGRES_PASSWORD', '111111')
    }
    
    # è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘
    input_folder = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'batch_input'
    )
    
    try:
        # å»ºç«‹ä¸Šå‚³å™¨
        uploader = BatchPostgreSQLUploader(config)
        
        # è™•ç†æ‰¹æ¬¡è³‡æ–™
        summary = uploader.process_batch_input_folder(input_folder)
        
        print("\n=== åŒ¯å…¥çµæœç¸½çµ ===")
        for item in summary:
            print(f"æª”æ¡ˆ: {item['file']} | æˆåŠŸ: {item.get('success_count', 0)} | å¤±æ•—: {item.get('fail_count', 0)}")
            if item.get('errors'):
                print(f"  å¤±æ•—æ˜ç´°: {item['errors']}")
        print("\nğŸ‰ æ‰¹æ¬¡ä¸Šå‚³çµæŸï¼")
        
        uploader.close()
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ä¸Šå‚³åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 