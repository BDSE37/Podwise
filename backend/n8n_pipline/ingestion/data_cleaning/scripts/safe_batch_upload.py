#!/usr/bin/env python3
"""
å®‰å…¨çš„æ‰¹æ¬¡ä¸Šå‚³è…³æœ¬
ç¢ºä¿ JSON è³‡æ–™æ­£ç¢ºå°æ‡‰åˆ° PostgreSQL æ¬„ä½ï¼Œä¸¦é¿å…é‡è¤‡æ’å…¥
ä½¿ç”¨ UPSERT æ“ä½œå’Œè©³ç´°çš„éŒ¯èª¤è™•ç†
"""

import os
import sys
import json
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
from typing import List, Dict, Any, Optional, Set
from pathlib import Path
import traceback

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SafeBatchUploader:
    """å®‰å…¨çš„æ‰¹æ¬¡ä¸Šå‚³é¡åˆ¥"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        
        Args:
            config: è³‡æ–™åº«é…ç½®
        """
        self.config = config
        self.conn = None
        self._connect()
        self._ensure_tables_exist()
    
    def _connect(self):
        """å»ºç«‹è³‡æ–™åº«é€£æ¥"""
        try:
            self.conn = psycopg2.connect(
                host=self.config['host'],
                port=self.config['port'],
                database=self.config['database'],
                user=self.config['user'],
                password=self.config['password']
            )
            self.conn.autocommit = False
            logger.info("PostgreSQL é€£æ¥æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            raise
    
    def _ensure_tables_exist(self):
        """ç¢ºä¿å¿…è¦çš„è¡¨æ ¼å­˜åœ¨"""
        try:
            with self.conn.cursor() as cursor:
                # æª¢æŸ¥ episodes è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'episodes'
                    );
                """)
                
                if not cursor.fetchone()[0]:
                    logger.info("å»ºç«‹ episodes è¡¨æ ¼...")
                    cursor.execute("""
                        CREATE TABLE episodes (
                            episode_id SERIAL PRIMARY KEY,
                            podcast_id INTEGER NOT NULL,
                            episode_title VARCHAR(255) NOT NULL,
                            published_date DATE,
                            audio_url VARCHAR(512),
                            duration INTEGER,
                            description TEXT,
                            audio_preview_url VARCHAR(512),
                            languages VARCHAR(64),
                            explicit BOOLEAN,
                            created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            apple_episodes_ranking INTEGER
                        );
                    """)
                    
                    # å»ºç«‹ç´¢å¼•
                    cursor.execute("""
                        CREATE INDEX idx_episodes_podcast_id ON episodes(podcast_id);
                        CREATE INDEX idx_episodes_published_date ON episodes(published_date);
                        CREATE UNIQUE INDEX idx_episodes_podcast_title ON episodes(podcast_id, episode_title);
                    """)
                    
                    logger.info("episodes è¡¨æ ¼å»ºç«‹å®Œæˆ")
                
                # æª¢æŸ¥ podcasts è¡¨æ ¼æ˜¯å¦å­˜åœ¨
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'podcasts'
                    );
                """)
                
                if not cursor.fetchone()[0]:
                    logger.info("å»ºç«‹ podcasts è¡¨æ ¼...")
                    cursor.execute("""
                        CREATE TABLE podcasts (
                            podcast_id INTEGER PRIMARY KEY,
                            name VARCHAR(255) NOT NULL,
                            description TEXT,
                            author VARCHAR(255),
                            rss_link VARCHAR(512) UNIQUE,
                            images_640 VARCHAR(512),
                            images_300 VARCHAR(512),
                            images_64 VARCHAR(512),
                            languages VARCHAR(64),
                            created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
                            apple_podcasts_ranking INTEGER,
                            apple_rating DECIMAL(3,2),
                            category VARCHAR(64),
                            update_frequency VARCHAR(64)
                        );
                    """)
                    
                    logger.info("podcasts è¡¨æ ¼å»ºç«‹å®Œæˆ")
            
            self.conn.commit()
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼å»ºç«‹å¤±æ•—: {e}")
            self.conn.rollback()
            raise
    
    def get_table_columns(self, table_name: str) -> Dict[str, dict]:
        """ç²å–è¡¨æ ¼æ¬„ä½è³‡è¨Š"""
        try:
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    SELECT column_name, data_type, is_nullable, column_default
                    FROM information_schema.columns 
                    WHERE table_name = %s 
                    ORDER BY ordinal_position
                """, (table_name,))
                
                columns = {}
                for row in cursor.fetchall():
                    columns[row[0]] = {
                        'type': row[1],
                        'nullable': row[2] == 'YES',
                        'default': row[3]
                    }
                return columns
                
        except Exception as e:
            logger.error(f"ç²å–è¡¨æ ¼æ¬„ä½å¤±æ•—: {e}")
            raise
    
    def safe_insert_podcast(self, podcast_data: Dict[str, Any]) -> Dict[str, Any]:
        """å®‰å…¨æ’å…¥ Podcast è³‡æ–™ï¼Œä½¿ç”¨ UPSERT
        
        Args:
            podcast_data: Podcast è³‡æ–™
            
        Returns:
            æ’å…¥çµæœ
        """
        try:
            # å¾æª”æ¡ˆåæå– podcast_id
            filename = Path(podcast_data.get('_file_path', '')).stem
            if filename.startswith('podcast_'):
                podcast_id = int(filename.split('_')[1])
            else:
                podcast_id = podcast_data.get('id')
            
            if not podcast_id:
                return {
                    'success': False,
                    'error': 'ç„¡æ³•æå– podcast_id',
                    'data': podcast_data
                }
            
            # æº–å‚™æ’å…¥è³‡æ–™
            insert_data = {
                'podcast_id': int(podcast_id),
                'name': podcast_data.get('title', ''),
                'description': podcast_data.get('description', ''),
                'author': podcast_data.get('provider', ''),
                'rss_link': podcast_data.get('url', ''),
                'languages': 'zh-TW',
                'category': podcast_data.get('category', ''),
                'apple_rating': self._extract_rating(podcast_data.get('rating', '')),
                'update_frequency': podcast_data.get('update_frequency', ''),
                'created_at': datetime.now(),
                'updated_at': datetime.now()
            }
            
            with self.conn.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO podcasts (
                        podcast_id, name, description, author, rss_link, 
                        languages, category, apple_rating, update_frequency,
                        created_at, updated_at
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (podcast_id) DO UPDATE SET
                        name = EXCLUDED.name,
                        description = EXCLUDED.description,
                        author = EXCLUDED.author,
                        rss_link = EXCLUDED.rss_link,
                        languages = EXCLUDED.languages,
                        category = EXCLUDED.category,
                        apple_rating = EXCLUDED.apple_rating,
                        update_frequency = EXCLUDED.update_frequency,
                        updated_at = CURRENT_TIMESTAMP
                    RETURNING podcast_id
                """, (
                    insert_data['podcast_id'],
                    insert_data['name'],
                    insert_data['description'],
                    insert_data['author'],
                    insert_data['rss_link'],
                    insert_data['languages'],
                    insert_data['category'],
                    insert_data['apple_rating'],
                    insert_data['update_frequency'],
                    insert_data['created_at'],
                    insert_data['updated_at']
                ))
                
                result_id = cursor.fetchone()[0]
                self.conn.commit()
                
                return {
                    'success': True,
                    'podcast_id': result_id,
                    'action': 'inserted' if result_id == insert_data['podcast_id'] else 'updated',
                    'data': insert_data
                }
                
        except Exception as e:
            logger.error(f"Podcast æ’å…¥å¤±æ•—: {e}")
            self.conn.rollback()
            return {
                'success': False,
                'error': str(e),
                'data': podcast_data
            }
    
    def safe_insert_episodes(self, episodes_data: List[Dict[str, Any]], podcast_id: int) -> Dict[str, Any]:
        """å®‰å…¨æ’å…¥ Episodes è³‡æ–™ï¼Œä½¿ç”¨ UPSERT
        
        Args:
            episodes_data: Episodes è³‡æ–™åˆ—è¡¨
            podcast_id: Podcast ID
            
        Returns:
            æ’å…¥çµæœçµ±è¨ˆ
        """
        try:
            results = {
                'total': len(episodes_data),
                'inserted': 0,
                'updated': 0,
                'skipped': 0,
                'errors': []
            }
            
            for idx, episode in enumerate(episodes_data):
                try:
                    # æº–å‚™æ’å…¥è³‡æ–™
                    insert_data = {
                        'podcast_id': podcast_id,
                        'episode_title': episode.get('title', ''),
                        'published_date': self._parse_date(episode.get('published_date')),
                        'audio_url': episode.get('audio_url', ''),
                        'duration': episode.get('duration'),
                        'description': episode.get('description', ''),
                        'audio_preview_url': episode.get('audio_preview_url', ''),
                        'languages': episode.get('languages', 'zh-TW'),
                        'explicit': episode.get('explicit', False),
                        'apple_episodes_ranking': episode.get('apple_episodes_ranking'),
                        'created_at': datetime.now(),
                        'updated_at': datetime.now()
                    }
                    
                    # æª¢æŸ¥å¿…è¦æ¬„ä½
                    if not insert_data['episode_title']:
                        results['skipped'] += 1
                        results['errors'].append({
                            'index': idx,
                            'error': 'ç¼ºå°‘ episode_title',
                            'data': episode
                        })
                        continue
                    
                    with self.conn.cursor() as cursor:
                        cursor.execute("""
                            INSERT INTO episodes (
                                podcast_id, episode_title, published_date, audio_url,
                                duration, description, audio_preview_url, languages,
                                explicit, apple_episodes_ranking, created_at, updated_at
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (podcast_id, episode_title) DO UPDATE SET
                                published_date = EXCLUDED.published_date,
                                audio_url = EXCLUDED.audio_url,
                                duration = EXCLUDED.duration,
                                description = EXCLUDED.description,
                                audio_preview_url = EXCLUDED.audio_preview_url,
                                languages = EXCLUDED.languages,
                                explicit = EXCLUDED.explicit,
                                apple_episodes_ranking = EXCLUDED.apple_episodes_ranking,
                                updated_at = CURRENT_TIMESTAMP
                            RETURNING episode_id, (xmax = 0) as is_insert
                        """, (
                            insert_data['podcast_id'],
                            insert_data['episode_title'],
                            insert_data['published_date'],
                            insert_data['audio_url'],
                            insert_data['duration'],
                            insert_data['description'],
                            insert_data['audio_preview_url'],
                            insert_data['languages'],
                            insert_data['explicit'],
                            insert_data['apple_episodes_ranking'],
                            insert_data['created_at'],
                            insert_data['updated_at']
                        ))
                        
                        row = cursor.fetchone()
                        if row[1]:  # is_insert
                            results['inserted'] += 1
                        else:
                            results['updated'] += 1
                    
                except Exception as e:
                    results['errors'].append({
                        'index': idx,
                        'error': str(e),
                        'data': episode
                    })
                    logger.error(f"Episode æ’å…¥å¤±æ•— (ç´¢å¼• {idx}): {e}")
            
            self.conn.commit()
            return results
            
        except Exception as e:
            logger.error(f"Episodes æ‰¹æ¬¡æ’å…¥å¤±æ•—: {e}")
            self.conn.rollback()
            return {
                'total': len(episodes_data),
                'inserted': 0,
                'updated': 0,
                'skipped': 0,
                'errors': [{'error': str(e)}]
            }
    
    def _extract_rating(self, rating_str: str) -> Optional[float]:
        """å¾è©•åˆ†å­—ä¸²ä¸­æå–æ•¸å€¼"""
        if not rating_str:
            return None
        try:
            # è™•ç† "4.8ï¼ˆ3.3è¬å‰‡è©•åˆ†ï¼‰" æ ¼å¼
            rating = rating_str.split('ï¼ˆ')[0]
            return float(rating)
        except:
            return None
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ä¸²"""
        if not date_str:
            return None
        try:
            # å˜—è©¦å¤šç¨®æ—¥æœŸæ ¼å¼
            formats = [
                '%Y-%m-%d',
                '%Y/%m/%d',
                '%d/%m/%Y',
                '%Y-%m-%dT%H:%M:%S',
                '%Y-%m-%dT%H:%M:%SZ'
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue
            
            return None
        except:
            return None
    
    def process_podcast_file(self, file_path: str) -> Dict[str, Any]:
        """è™•ç† Podcast æª”æ¡ˆ
        
        Args:
            file_path: JSON æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è™•ç†çµæœ
        """
        try:
            logger.info(f"è™•ç† Podcast æª”æ¡ˆ: {file_path}")
            
            # è®€å– JSON æª”æ¡ˆ
            with open(file_path, 'r', encoding='utf-8') as f:
                podcast_data = json.load(f)
            
            # æ·»åŠ æª”æ¡ˆè·¯å¾‘è³‡è¨Š
            podcast_data['_file_path'] = file_path
            
            # æ’å…¥ podcast
            result = self.safe_insert_podcast(podcast_data)
            
            if result['success']:
                logger.info(f"Podcast è™•ç†æˆåŠŸ: {Path(file_path).name} (ID: {result['podcast_id']})")
            else:
                logger.error(f"Podcast è™•ç†å¤±æ•—: {Path(file_path).name} - {result['error']}")
            
            return result
            
        except Exception as e:
            logger.error(f"è™•ç† Podcast æª”æ¡ˆå¤±æ•—: {file_path} - {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': file_path
            }
    
    def process_rss_file(self, file_path: str) -> Dict[str, Any]:
        """è™•ç† RSS æª”æ¡ˆ
        
        Args:
            file_path: JSON æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è™•ç†çµæœ
        """
        try:
            logger.info(f"è™•ç† RSS æª”æ¡ˆ: {file_path}")
            
            # å¾æª”æ¡ˆåæå– podcast_id
            filename = Path(file_path).stem
            if filename.startswith('RSS_'):
                podcast_id = int(filename.split('_')[1])
            elif filename.startswith('Spotify_RSS_'):
                podcast_id = int(filename.split('_')[2])
            else:
                return {
                    'success': False,
                    'error': f'ç„¡æ³•å¾æª”æ¡ˆåæå– podcast_id: {filename}',
                    'file_path': file_path
                }
            
            # è®€å– JSON æª”æ¡ˆ
            with open(file_path, 'r', encoding='utf-8') as f:
                episodes_data = json.load(f)
            
            if not isinstance(episodes_data, list):
                return {
                    'success': False,
                    'error': 'RSS æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼Œæ‡‰è©²æ˜¯é™£åˆ—æ ¼å¼',
                    'file_path': file_path
                }
            
            # æ’å…¥ episodes
            result = self.safe_insert_episodes(episodes_data, podcast_id)
            
            logger.info(f"RSS æª”æ¡ˆè™•ç†å®Œæˆ: {Path(file_path).name}")
            logger.info(f"  ç¸½è¨ˆ: {result['total']}, æ–°å¢: {result['inserted']}, æ›´æ–°: {result['updated']}, è·³é: {result['skipped']}")
            
            if result['errors']:
                logger.warning(f"  éŒ¯èª¤æ•¸é‡: {len(result['errors'])}")
            
            return {
                'success': True,
                'podcast_id': podcast_id,
                'episodes_result': result,
                'file_path': file_path
            }
            
        except Exception as e:
            logger.error(f"è™•ç† RSS æª”æ¡ˆå¤±æ•—: {file_path} - {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': file_path
            }
    
    def process_batch_input_folder(self, input_folder: str) -> Dict[str, Any]:
        """è™•ç†æ•´å€‹ batch_input è³‡æ–™å¤¾
        
        Args:
            input_folder: è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘
            
        Returns:
            è™•ç†çµæœæ‘˜è¦
        """
        logger.info(f"é–‹å§‹è™•ç†è³‡æ–™å¤¾: {input_folder}")
        
        if not os.path.exists(input_folder):
            return {"error": "è³‡æ–™å¤¾ä¸å­˜åœ¨"}
        
        json_files = [f for f in os.listdir(input_folder) if f.endswith('.json')]
        logger.info(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
        
        results = {
            'podcasts': [],
            'rss_files': [],
            'summary': {
                'total_files': len(json_files),
                'podcast_files': 0,
                'rss_files': 0,
                'successful_podcasts': 0,
                'successful_rss': 0,
                'failed_files': 0
            }
        }
        
        for filename in json_files:
            file_path = os.path.join(input_folder, filename)
            
            if filename.startswith('podcast_'):
                result = self.process_podcast_file(file_path)
                results['podcasts'].append(result)
                results['summary']['podcast_files'] += 1
                
                if result.get('success'):
                    results['summary']['successful_podcasts'] += 1
                else:
                    results['summary']['failed_files'] += 1
                    
            elif filename.startswith('RSS_') or filename.startswith('Spotify_RSS_'):
                result = self.process_rss_file(file_path)
                results['rss_files'].append(result)
                results['summary']['rss_files'] += 1
                
                if result.get('success'):
                    results['summary']['successful_rss'] += 1
                else:
                    results['summary']['failed_files'] += 1
            else:
                logger.warning(f"è·³éæœªçŸ¥æ ¼å¼æª”æ¡ˆ: {filename}")
        
        return results
    
    def generate_summary_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        summary = results.get('summary', {})
        
        report = []
        report.append("=" * 60)
        report.append("æ‰¹æ¬¡ä¸Šå‚³æ‘˜è¦å ±å‘Š")
        report.append("=" * 60)
        
        report.append(f"\nğŸ“Š æª”æ¡ˆè™•ç†çµ±è¨ˆ:")
        report.append(f"   - ç¸½æª”æ¡ˆæ•¸: {summary.get('total_files', 0)}")
        report.append(f"   - Podcast æª”æ¡ˆ: {summary.get('podcast_files', 0)}")
        report.append(f"   - RSS æª”æ¡ˆ: {summary.get('rss_files', 0)}")
        report.append(f"   - æˆåŠŸè™•ç†: {summary.get('successful_podcasts', 0) + summary.get('successful_rss', 0)}")
        report.append(f"   - è™•ç†å¤±æ•—: {summary.get('failed_files', 0)}")
        
        # Podcast è™•ç†çµæœ
        if results.get('podcasts'):
            report.append(f"\nğŸ™ï¸ Podcast è™•ç†çµæœ:")
            for podcast in results['podcasts']:
                filename = Path(podcast.get('file_path', '')).name
                if podcast.get('success'):
                    report.append(f"   âœ… {filename}: {podcast.get('action', 'processed')}")
                else:
                    report.append(f"   âŒ {filename}: {podcast.get('error', 'unknown error')}")
        
        # RSS è™•ç†çµæœ
        if results.get('rss_files'):
            report.append(f"\nğŸ“» RSS è™•ç†çµæœ:")
            for rss in results['rss_files']:
                filename = Path(rss.get('file_path', '')).name
                if rss.get('success'):
                    episodes_result = rss.get('episodes_result', {})
                    report.append(f"   âœ… {filename}: {episodes_result.get('inserted', 0)} æ–°å¢, {episodes_result.get('updated', 0)} æ›´æ–°")
                else:
                    report.append(f"   âŒ {filename}: {rss.get('error', 'unknown error')}")
        
        report.append("\n" + "=" * 60)
        return "\n".join(report)
    
    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£æ¥"""
        if self.conn:
            self.conn.close()
            logger.info("è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

def main():
    """ä¸»ç¨‹å¼"""
    logger.info("=== å®‰å…¨æ‰¹æ¬¡ä¸Šå‚³ batch_input è³‡æ–™å¤¾åˆ° PostgreSQL ===")
    
    # è³‡æ–™åº«é…ç½®
    config = {
        'host': os.getenv('POSTGRES_HOST', 'postgres.podwise.svc.cluster.local'),
        'port': int(os.getenv('POSTGRES_PORT', 5432)),
        'database': os.getenv('POSTGRES_DB', 'podcast'),
        'user': os.getenv('POSTGRES_USER', 'bdse37'),
        'password': os.getenv('POSTGRES_PASSWORD', '111111')
    }
    
    # è¼¸å…¥è³‡æ–™å¤¾è·¯å¾‘
    input_folder = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        'batch_input'
    )
    
    try:
        # å»ºç«‹ä¸Šå‚³å™¨
        uploader = SafeBatchUploader(config)
        
        # è™•ç†æ‰¹æ¬¡è³‡æ–™
        results = uploader.process_batch_input_folder(input_folder)
        
        # ç”Ÿæˆå ±å‘Š
        report = uploader.generate_summary_report(results)
        print(report)
        
        # å„²å­˜å ±å‘Šåˆ°æª”æ¡ˆ
        report_file = os.path.join(os.path.dirname(input_folder), 'safe_upload_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ä¸Šå‚³å ±å‘Šå·²å„²å­˜åˆ°: {report_file}")
        
        # é—œé–‰é€£æ¥
        uploader.close()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
        if results.get('summary', {}).get('failed_files', 0) > 0:
            logger.warning("éƒ¨åˆ†æª”æ¡ˆè™•ç†å¤±æ•—ï¼Œè«‹æª¢æŸ¥å ±å‘Š")
            sys.exit(1)
        else:
            logger.info("æ‰€æœ‰æª”æ¡ˆè™•ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ä¸Šå‚³å¤±æ•—: {e}")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 