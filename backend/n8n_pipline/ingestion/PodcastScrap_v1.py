

import os
import re
import requests
import threading
import time
import undetected_chromedriver as uc
import requests as req
import feedparser
import json
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.support import expected_conditions
from selenium.webdriver.support.ui import WebDriverWait
from time import sleep
from typing import List
from bs4 import BeautifulSoup as bs
from selenium.webdriver.support import expected_conditions as EC
from typing import List, Dict






def extract_apple_urls(
    podcast_url: str,
    tab_text: str = "ç†±é–€ç¯€ç›®",
    max_attempts: int = 10,
    output_file: str = "apple_urls.json"
) -> None:
    options = uc.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = uc.Chrome(options=options, version_main=136)
    driver.get(podcast_url)

    try:
        wait = WebDriverWait(driver, 15)
        tab = wait.until(EC.element_to_be_clickable((By.XPATH, f'//span[text()="{tab_text}"]')))
        tab.click()
        print(f"âœ… é»æ“Šæ¨™ç±¤ï¼š{tab_text}")
        time.sleep(3)

        body = driver.find_element(By.TAG_NAME, "body")
        prev_count = 0
        stable_attempts = 0

        while stable_attempts < max_attempts:
            for _ in range(5):
                body.send_keys(Keys.END)
                time.sleep(0.5)

            elements = driver.find_elements(By.XPATH, '//a[@data-testid="product-lockup-link"]')
            curr_count = len(elements)
            print(f"ğŸ¯ å·²æ“·å–ç¯€ç›®æ•¸ï¼š{curr_count}")

            if curr_count == prev_count:
                stable_attempts += 1
                print(f"âš ï¸ ç„¡æ–°å¢å…§å®¹ï¼Œç¬¬ {stable_attempts}/{max_attempts} æ¬¡ç¢ºèª")
            else:
                stable_attempts = 0
                prev_count = curr_count

        seen_links = set()
        result: List[Dict[str, str]] = []

        for el in elements:
            href = el.get_attribute("href")
            if href and href not in seen_links:
                seen_links.add(href)
                print(f"ğŸ”— Apple URL: {href}")
                result.append({"apple_url": href})

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å„²å­˜è‡³ {output_file}")

    finally:
        driver.quit()


def split_apple_urls(input_file: str, start_idx: int, end_idx: int) -> None:
    with open(input_file, "r", encoding="utf-8") as f:
        data: List[Dict[str, str]] = json.load(f)

    selected = data[start_idx:end_idx]
    out_file = f"{os.path.splitext(input_file)[0]}_part_{start_idx}_{end_idx}.json"

    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(selected, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²è¼¸å‡ºç¬¬ {start_idx} åˆ° {end_idx} ç­†è³‡æ–™ -> {out_file}")


if __name__ == "__main__":
    podcast_url = "https://podcasts.apple.com/tw/charts?genre=1304"
    extract_apple_urls(podcast_url)

    # ğŸ” å¾ URL è‡ªå‹•æ“·å– genre_id
    genre_match = re.search(r"genre=(\d+)", podcast_url)
    genre_id = genre_match.group(1) if genre_match else "unknown"
    print(f"ğŸ¯ æ“·å–åˆ° genre_id: {genre_id}")

    # âœ… å¯è‡ªå®šæ§åˆ¶åˆ†å‰²ç¯„åœ
    part_start = 0
    part_end = 100

    # åˆ†å‰² JSON
    split_apple_urls("apple_urls.json", start_idx=part_start, end_idx=part_end)

    # çµæœæª”æ¡ˆå‘½å
    part_file = f"apple_urls_part_{part_start}_{part_end}.json"
    print(f"ğŸ“„ ä½ å¯ä»¥æ¥è‘—è™•ç†é€™å€‹æª”æ¡ˆï¼š{part_file}")

from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed


class PodcastRSSExtractor:
    def __init__(
        self,
        input_file: str,
        output_file: str,
        threads: int = 5,
        delay: float = 1.5,
    ) -> None:
        self.input_file: str = input_file
        self.output_file: str = output_file
        self.threads: int = threads
        self.delay: float = delay

    def is_valid_rss(self, url: str) -> bool:
        """ç”¨ feedparser é©—è­‰è©² URL æ˜¯å¦ç‚ºå¯ç”¨ RSS feed"""
        try:
            feed = feedparser.parse(url)
            return bool(feed.entries)
        except Exception:
            return False

    def extract_feed_url(self, apple_url: str) -> Optional[str]:
        """å¾ Apple Podcast é é¢å…§åµŒ JSON ä¸­æ“·å– feedUrl"""
        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            res = requests.get(apple_url, headers=headers, timeout=10)
            match = re.search(r'"feedUrl"\s*:\s*"([^"]+)"', res.text)
            if match:
                return match.group(1)
        except Exception:
            pass
        return None

    def process_item(
        self,
        item: Dict[str, str],
        index: int,
        total: int
    ) -> Dict[str, Optional[str]]:
        apple_url: str = item.get("apple_url", "")
        print(f"[{index}/{total}] ğŸ” æŠ“å– RSS for {apple_url}")
        rss_url: Optional[str] = self.extract_feed_url(apple_url)
        time.sleep(self.delay)
        return {"apple_url": apple_url, "feed_url": rss_url}

    def enrich(self) -> None:
        with open(self.input_file, "r", encoding="utf-8") as f:
            podcast_list: List[Dict[str, str]] = json.load(f)

        total: int = len(podcast_list)
        enriched_list: List[Dict[str, Optional[str]]] = []

        with ThreadPoolExecutor(max_workers=self.threads) as executor:
            future_to_index = {
                executor.submit(self.process_item, item, idx + 1, total): idx
                for idx, item in enumerate(podcast_list)
            }
            for future in as_completed(future_to_index):
                result = future.result()
                enriched_list.append(result)

        with open(self.output_file, "w", encoding="utf-8") as f:
            json.dump(enriched_list, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²å¯«å…¥çµæœè‡³ {self.output_file}")


if __name__ == "__main__":
    # æ‰‹å‹•æŒ‡å®šè¦ä½¿ç”¨å“ªå€‹å€æ®µ

    input_file = f"apple_urls_part_{part_start}_{part_end}.json"
    output_file = f"podcast_with_rss.json"

    extractor = PodcastRSSExtractor(
        input_file=input_file,
        output_file=output_file,
        threads=10,
        delay=1.5,
    )
    extractor.enrich()
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®
INPUT_FILE = "podcast_with_rss.json"
OUTPUT_DIR = f"rss_outputs_{genre_id}"  # å–®ä¸€è¼¸å‡ºè³‡æ–™å¤¾
NUM_THREADS = 10  # åŒæ™‚è™•ç†åŸ·è¡Œç·’æ•¸

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)


def safe_filename(text: str) -> str:
    """æ¸…ç†æª”åï¼šç§»é™¤ç‰¹æ®Šç¬¦è™Ÿä¸¦é™åˆ¶é•·åº¦"""
    return re.sub(r"[^0-9A-Za-z_\- ]", "_", text)[:80].strip()


def process_podcast(podcast: Dict[str, str], index: int, total: int) -> Optional[str]:
    """è§£æå–®ä¸€ RSS ä¸¦å°‡çµæœå¯«å…¥å–®ä¸€è³‡æ–™å¤¾ï¼Œå›å‚³æª”æ¡ˆè·¯å¾‘"""
    feed_url = podcast.get("feed_url")
    apple_url = podcast.get("apple_url", "")
    if not feed_url:
        print(f"âŒ [ç¬¬{index}/{total}] æ²’æœ‰ feed_urlï¼Œè·³é")
        return None

    # è§£æ podcast_id
    match = re.search(r"id(\d+)", apple_url)
    podcast_id = match.group(1) if match else str(index)
    print(f"ğŸ” [{index}/{total}] è™•ç† RSS: {feed_url}")

    feed = feedparser.parse(feed_url)
    if not feed.entries:
        print(f"âš ï¸ [ç¬¬{index}] RSS ç„¡å…§å®¹ï¼Œè·³é")
        return None

    # æ“·å–é›†æ•¸è³‡æ–™
    episodes = []
    for entry in feed.entries:
        audio_url = entry.enclosures[0].href if entry.enclosures else None
        title = entry.get("title", "")
        episodes.append({
            "id": entry.get("id") or safe_filename(title),
            "title": title,
            "published": entry.get("published", ""),
            "description": entry.get("summary", ""),
            "audio_url": audio_url
        })

    # å¯«å…¥åŒä¸€è³‡æ–™å¤¾
    filename = f"RSS_{podcast_id}.json"
    output_path = os.path.join(OUTPUT_DIR, filename)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(episodes, f, ensure_ascii=False, indent=2)

    print(f"âœ… [{index}/{total}] å·²å¯«å…¥: {output_path} ({len(episodes)} é›†)")
    return output_path


def main():
    # è®€å– RSS åˆ—è¡¨
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        podcast_list: List[Dict[str, str]] = json.load(f)
    total = len(podcast_list)

    results = []
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        futures = {executor.submit(process_podcast, podcast, idx+1, total): idx
                   for idx, podcast in enumerate(podcast_list)}
        for future in as_completed(futures):
            path = future.result()
            if path:
                results.append(path)

    print(f"\næ‰€æœ‰ä»»å‹™å®Œæˆï¼Œå…±å¯«å…¥ {len(results)} å€‹æª”æ¡ˆåˆ° `{OUTPUT_DIR}`ã€‚")


if __name__ == "__main__":
    main()