"""
ç°¡åŒ–æ¸¬è©¦è…³æœ¬ï¼šåˆ‡æ–·ã€è²¼æ¨™ã€è½‰å‘é‡æµç¨‹
å°ˆé–€æ¸¬è©¦åŸºæœ¬åŠŸèƒ½ï¼Œä¸ä¾è³´ PostgreSQL
æ•´åˆéŒ¯èª¤è¨˜éŒ„åŠŸèƒ½
"""

import logging
import sys
from pathlib import Path
from typing import Dict, List, Any
import json
from datetime import datetime

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(str(Path(__file__).parent))

from vector_pipeline.core import (
    MongoDBProcessor, MongoDocument,
    TextChunker, TextChunk,
    VectorProcessor,
    MilvusWriter
)
from vector_pipeline.utils.text_cleaner import TextCleaner
from vector_pipeline.error_logger import ErrorLogger, ErrorHandler

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simple_pipeline_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class SimplePipelineTester:
    """ç°¡åŒ–ç®¡é“æ¸¬è©¦å™¨"""
    
    def __init__(self):
        # é…ç½®
        self.mongo_config = {
            'host': 'localhost',
            'port': 27017,
            'database': 'podwise',
            'collection': 'podcasts'
        }
        
        self.milvus_config = {
            'host': 'localhost',
            'port': 19530,
            'collection_name': 'podcast_chunks_simple_test',
            'dim': 1024
        }
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.text_cleaner = TextCleaner()
        self.mongo_processor = MongoDBProcessor(self.mongo_config)
        self.text_chunker = TextChunker(max_chunk_size=1024, overlap_size=100)
        self.vector_processor = VectorProcessor(model_name="BAAI/bge-m3")
        self.milvus_writer = MilvusWriter(self.milvus_config)
        
        # åˆå§‹åŒ–éŒ¯èª¤è¨˜éŒ„å™¨
        self.error_logger = ErrorLogger("error_logs")
        self.error_handler = ErrorHandler(self.error_logger)
        
        logger.info("ç°¡åŒ–æ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_simple_tags(self, text: str) -> List[str]:
        """ç°¡å–®çš„æ¨™ç±¤æå–"""
        tags = []
        text_lower = text.lower()
        
        # ç°¡å–®é—œéµå­—åŒ¹é…
        keyword_mapping = {
            'ai': 'AIäººå·¥æ™ºæ…§',
            'äººå·¥æ™ºæ…§': 'AIäººå·¥æ™ºæ…§',
            'ç§‘æŠ€': 'ç§‘æŠ€æŠ€è¡“',
            'æŠ€è¡“': 'ç§‘æŠ€æŠ€è¡“',
            'å•†æ¥­': 'å•†æ¥­ç®¡ç†',
            'ä¼æ¥­': 'å•†æ¥­ç®¡ç†',
            'ç®¡ç†': 'å•†æ¥­ç®¡ç†',
            'å‰µæ¥­': 'å‰µæ¥­å‰µæ–°',
            'å‰µæ–°': 'å‰µæ¥­å‰µæ–°',
            'æ•™è‚²': 'æ•™è‚²å­¸ç¿’',
            'å­¸ç¿’': 'æ•™è‚²å­¸ç¿’',
            'å¥åº·': 'å¥åº·ç”Ÿæ´»',
            'ç”Ÿæ´»': 'å¥åº·ç”Ÿæ´»',
            'æŠ•è³‡': 'æŠ•è³‡ç†è²¡',
            'ç†è²¡': 'æŠ•è³‡ç†è²¡',
            'å¨›æ¨‚': 'å¨›æ¨‚ä¼‘é–’',
            'ä¼‘é–’': 'å¨›æ¨‚ä¼‘é–’',
            'æ”¿æ²»': 'æ”¿æ²»ç¤¾æœƒ',
            'ç¤¾æœƒ': 'æ”¿æ²»ç¤¾æœƒ',
            'æ–‡åŒ–': 'æ–‡åŒ–è—è¡“',
            'è—è¡“': 'æ–‡åŒ–è—è¡“'
        }
        
        for keyword, tag in keyword_mapping.items():
            if keyword in text_lower:
                if tag not in tags:
                    tags.append(tag)
                if len(tags) >= 3:  # æœ€å¤š3å€‹æ¨™ç±¤
                    break
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ¨™ç±¤ï¼Œä½¿ç”¨é è¨­æ¨™ç±¤
        if not tags:
            tags = ['ä¸€èˆ¬å…§å®¹']
        
        return tags[:3]
    
    def process_single_document(self, doc: MongoDocument) -> List[Dict[str, Any]]:
        """è™•ç†å–®å€‹æ–‡ä»¶ï¼ˆæ•´åˆéŒ¯èª¤è™•ç†ï¼‰"""
        logger.info(f"è™•ç†æ–‡ä»¶: {doc.title}")
        
        try:
            # 1. æ¸…ç†æ–‡æœ¬
            try:
                cleaned_title = self.text_cleaner.clean_text(doc.title)
                cleaned_content = self.text_cleaner.clean_text(doc.content)
            except Exception as e:
                self.error_handler.handle_text_processing_error(
                    collection_id=getattr(doc, 'collection_id', 'unknown'),
                    rss_id=getattr(doc, 'rss_id', 'unknown'),
                    title=doc.title,
                    error=e,
                    stage="text_cleaning"
                )
                raise
            
            logger.info(f"æ¸…ç†å¾Œæ¨™é¡Œ: {cleaned_title}")
            logger.info(f"æ¸…ç†å¾Œå…§å®¹é•·åº¦: {len(cleaned_content)}")
            
            # 2. åˆ‡åˆ†æ–‡æœ¬
            try:
                chunks = self.text_chunker.split_text_into_chunks(cleaned_content, str(doc.id))
            except Exception as e:
                self.error_handler.handle_text_processing_error(
                    collection_id=getattr(doc, 'collection_id', 'unknown'),
                    rss_id=getattr(doc, 'rss_id', 'unknown'),
                    title=doc.title,
                    error=e,
                    stage="text_chunking"
                )
                raise
            
            logger.info(f"åˆ‡åˆ†å‡º {len(chunks)} å€‹æ–‡æœ¬å¡Š")
            
            processed_chunks = []
            
            # 3. è™•ç†æ¯å€‹æ–‡æœ¬å¡Š
            for i, chunk in enumerate(chunks):
                try:
                    logger.info(f"è™•ç†æ–‡æœ¬å¡Š {i+1}/{len(chunks)}")
                    
                    # æå–æ¨™ç±¤
                    tags = self.extract_simple_tags(chunk.chunk_text)
                    logger.info(f"æ¨™ç±¤: {tags}")
                    
                    # ç”Ÿæˆå‘é‡
                    try:
                        text_embedding = self.vector_processor.generate_embedding(chunk.chunk_text)
                    except Exception as e:
                        self.error_handler.handle_vectorization_error(
                            collection_id=getattr(doc, 'collection_id', 'unknown'),
                            rss_id=getattr(doc, 'rss_id', 'unknown'),
                            title=doc.title,
                            error=e,
                            stage="text_vectorization"
                        )
                        raise
                    
                    tag_embeddings = []
                    for tag in tags:
                        try:
                            tag_embedding = self.vector_processor.generate_embedding(tag)
                            tag_embeddings.append(tag_embedding)
                        except Exception as e:
                            self.error_handler.handle_vectorization_error(
                                collection_id=getattr(doc, 'collection_id', 'unknown'),
                                rss_id=getattr(doc, 'rss_id', 'unknown'),
                                title=doc.title,
                                error=e,
                                stage="tag_vectorization"
                            )
                            # ä½¿ç”¨æ–‡æœ¬å‘é‡ä½œç‚ºæ¨™ç±¤å‘é‡çš„å‚™ç”¨
                            tag_embeddings.append(text_embedding)
                    
                    # ç¢ºä¿æœ‰3å€‹æ¨™ç±¤å‘é‡
                    while len(tag_embeddings) < 3:
                        tag_embeddings.append(text_embedding)
                    
                    processed_chunk = {
                        'chunk_id': f"{doc.id}_chunk_{i}",
                        'chunk_index': i,
                        'chunk_text': chunk.chunk_text,
                        'chunk_length': len(chunk.chunk_text),
                        'episode_id': getattr(doc, 'episode_id', 999),
                        'podcast_id': getattr(doc, 'podcast_id', 999),
                        'episode_title': cleaned_title,
                        'podcast_name': getattr(doc, 'podcast_name', 'æœªçŸ¥æ’­å®¢'),
                        'author': getattr(doc, 'author', 'æœªçŸ¥ä½œè€…'),
                        'category': getattr(doc, 'category', 'ä¸€èˆ¬'),
                        'created_at': datetime.now().isoformat(),
                        'source_model': 'simple_test',
                        'language': 'zh-TW',
                        'embedding': text_embedding,
                        'tag_1': tag_embeddings[0],
                        'tag_2': tag_embeddings[1],
                        'tag_3': tag_embeddings[2],
                        'tags': tags
                    }
                    
                    processed_chunks.append(processed_chunk)
                    
                except Exception as e:
                    self.error_handler.handle_general_error(
                        collection_id=getattr(doc, 'collection_id', 'unknown'),
                        rss_id=getattr(doc, 'rss_id', 'unknown'),
                        title=doc.title,
                        error=e,
                        stage="chunk_processing"
                    )
                    logger.error(f"è™•ç†æ–‡æœ¬å¡Š {i+1} å¤±æ•—: {e}")
                    continue
            
            return processed_chunks
            
        except Exception as e:
            self.error_handler.handle_general_error(
                collection_id=getattr(doc, 'collection_id', 'unknown'),
                rss_id=getattr(doc, 'rss_id', 'unknown'),
                title=doc.title,
                error=e,
                stage="document_processing"
            )
            logger.error(f"è™•ç†æ–‡ä»¶å¤±æ•—: {e}")
            return []
    
    def test_with_sample_data(self) -> None:
        """ä½¿ç”¨æ¨£æœ¬è³‡æ–™æ¸¬è©¦"""
        logger.info("=== ä½¿ç”¨æ¨£æœ¬è³‡æ–™æ¸¬è©¦ ===")
        
        # æ¨£æœ¬è³‡æ–™
        sample_text = """
        äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ˜¯ç•¶ä»Šç§‘æŠ€ç™¼å±•çš„é‡è¦è¶¨å‹¢ã€‚åœ¨å•†æ¥­æ‡‰ç”¨ä¸­ï¼ŒAIæŠ€è¡“æ­£åœ¨æ”¹è®Šä¼æ¥­çš„é‹ç‡Ÿæ–¹å¼ï¼Œ
        å¾å®¢æˆ¶æœå‹™åˆ°ç”¢å“é–‹ç™¼ï¼Œå¾å¸‚å ´åˆ†æåˆ°æ±ºç­–åˆ¶å®šï¼ŒAIéƒ½ç™¼æ®è‘—è¶Šä¾†è¶Šé‡è¦çš„ä½œç”¨ã€‚
        
        åœ¨æ•™è‚²é ˜åŸŸï¼ŒAIæŠ€è¡“ç‚ºå­¸ç¿’è€…æä¾›äº†å€‹æ€§åŒ–çš„å­¸ç¿’é«”é©—ï¼Œæ™ºèƒ½å°å¸«ç³»çµ±å¯ä»¥æ ¹æ“šå­¸ç”Ÿçš„å­¸ç¿’é€²åº¦
        å’Œèˆˆè¶£èª¿æ•´æ•™å­¸å…§å®¹ï¼Œæé«˜å­¸ç¿’æ•ˆç‡ã€‚åŒæ™‚ï¼ŒAIä¹Ÿåœ¨å¹«åŠ©æ•™å¸«æ›´å¥½åœ°ç®¡ç†èª²å ‚å’Œè©•ä¼°å­¸ç”Ÿè¡¨ç¾ã€‚
        
        åœ¨å¥åº·é†«ç™‚æ–¹é¢ï¼ŒAIæŠ€è¡“çš„æ‡‰ç”¨æ›´æ˜¯å»£æ³›ï¼Œå¾é†«å­¸å½±åƒè¨ºæ–·åˆ°è—¥ç‰©ç ”ç™¼ï¼Œå¾æ‚£è€…è­·ç†åˆ°ç–¾ç—…é æ¸¬ï¼Œ
        AIéƒ½åœ¨ç‚ºé†«ç™‚è¡Œæ¥­å¸¶ä¾†é©å‘½æ€§çš„è®ŠåŒ–ã€‚é€™äº›æŠ€è¡“ä¸åƒ…æé«˜äº†é†«ç™‚æ•ˆç‡ï¼Œä¹Ÿç‚ºæ‚£è€…æä¾›äº†æ›´å¥½çš„é†«ç™‚æœå‹™ã€‚
        """
        
        # å‰µå»ºæ¨¡æ“¬æ–‡ä»¶
        mock_doc = MongoDocument(
            id="test_001",
            title="ğŸ§ AIç§‘æŠ€æ–°çŸ¥ï¼šäººå·¥æ™ºæ…§åœ¨å•†æ¥­èˆ‡æ•™è‚²ä¸­çš„æ‡‰ç”¨ ğŸ”¥",
            content=sample_text,
            episode_id=1001,
            podcast_id=2001,
            podcast_name="ç§‘æŠ€æ–°çŸ¥æ’­å®¢",
            author="ç§‘æŠ€å°ˆå®¶",
            category="ç§‘æŠ€",
            collection_id="test_collection",
            rss_id="test_rss_001"
        )
        
        # è™•ç†æ–‡ä»¶
        processed_chunks = self.process_single_document(mock_doc)
        
        # å„²å­˜åˆ° Milvus
        if processed_chunks:
            try:
                success_count = self.milvus_writer.write_chunks(processed_chunks)
                logger.info(f"æˆåŠŸå„²å­˜ {success_count} å€‹æ–‡æœ¬å¡Šåˆ° Milvus")
            except Exception as e:
                self.error_handler.handle_milvus_error(
                    collection_id="test_collection",
                    rss_id="test_rss_001",
                    title=mock_doc.title,
                    error=e,
                    stage="milvus_write"
                )
                logger.error(f"å„²å­˜åˆ° Milvus å¤±æ•—: {e}")
    
    def test_with_mongo_data(self, limit: int = 2) -> None:
        """ä½¿ç”¨ MongoDB çœŸå¯¦è³‡æ–™æ¸¬è©¦"""
        logger.info("=== ä½¿ç”¨ MongoDB çœŸå¯¦è³‡æ–™æ¸¬è©¦ ===")
        
        try:
            # ç²å–æ–‡ä»¶
            documents = self.mongo_processor.get_documents(limit=limit)
            logger.info(f"ç²å–åˆ° {len(documents)} å€‹æ–‡ä»¶")
            
            total_processed = 0
            
            for i, doc in enumerate(documents):
                logger.info(f"\nè™•ç†æ–‡ä»¶ {i+1}/{len(documents)}")
                
                try:
                    processed_chunks = self.process_single_document(doc)
                    total_processed += len(processed_chunks)
                    
                    # å„²å­˜åˆ° Milvus
                    if processed_chunks:
                        try:
                            success_count = self.milvus_writer.write_chunks(processed_chunks)
                            logger.info(f"æ–‡ä»¶ {i+1} æˆåŠŸå„²å­˜ {success_count} å€‹æ–‡æœ¬å¡Š")
                        except Exception as e:
                            self.error_handler.handle_milvus_error(
                                collection_id=getattr(doc, 'collection_id', 'unknown'),
                                rss_id=getattr(doc, 'rss_id', 'unknown'),
                                title=doc.title,
                                error=e,
                                stage="milvus_write"
                            )
                            logger.error(f"å„²å­˜æ–‡ä»¶ {i+1} åˆ° Milvus å¤±æ•—: {e}")
                    
                except Exception as e:
                    self.error_handler.handle_general_error(
                        collection_id=getattr(doc, 'collection_id', 'unknown'),
                        rss_id=getattr(doc, 'rss_id', 'unknown'),
                        title=doc.title,
                        error=e,
                        stage="document_processing"
                    )
                    logger.error(f"è™•ç†æ–‡ä»¶ {i+1} å¤±æ•—: {e}")
                    continue
            
            logger.info(f"ç¸½å…±è™•ç†äº† {total_processed} å€‹æ–‡æœ¬å¡Š")
            
        except Exception as e:
            self.error_handler.handle_general_error(
                collection_id="unknown",
                rss_id="unknown",
                title="unknown",
                error=e,
                stage="mongodb_connection"
            )
            logger.error(f"MongoDB æ¸¬è©¦å¤±æ•—: {e}")
    
    def run_test(self, use_mongo: bool = False) -> None:
        """åŸ·è¡Œæ¸¬è©¦"""
        logger.info("é–‹å§‹åŸ·è¡Œç°¡åŒ–ç®¡é“æ¸¬è©¦")
        
        try:
            if use_mongo:
                self.test_with_mongo_data()
            else:
                self.test_with_sample_data()
            
            logger.info("æ¸¬è©¦å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            # å„²å­˜éŒ¯èª¤å ±å‘Š
            if self.error_logger.errors:
                logger.info("=== éŒ¯èª¤å ±å‘Š ===")
                self.error_logger.print_summary()
                
                reports = self.error_handler.save_error_reports()
                logger.info(f"éŒ¯èª¤å ±å‘Šå·²å„²å­˜:")
                logger.info(f"  JSON: {reports['json_report']}")
                logger.info(f"  CSV: {reports['csv_report']}")
            else:
                logger.info("æ²’æœ‰éŒ¯èª¤è¨˜éŒ„")
            
            self.cleanup()
    
    def cleanup(self) -> None:
        """æ¸…ç†è³‡æº"""
        try:
            self.mongo_processor.close()
            self.milvus_writer.close()
            logger.info("è³‡æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"æ¸…ç†è³‡æºæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç°¡åŒ–ç®¡é“æ¸¬è©¦')
    parser.add_argument('--mongo', action='store_true', help='ä½¿ç”¨ MongoDB çœŸå¯¦è³‡æ–™')
    parser.add_argument('--sample', action='store_true', help='ä½¿ç”¨æ¨£æœ¬è³‡æ–™')
    
    args = parser.parse_args()
    
    tester = SimplePipelineTester()
    
    if args.mongo:
        tester.run_test(use_mongo=True)
    else:
        tester.run_test(use_mongo=False)


if __name__ == "__main__":
    main() 