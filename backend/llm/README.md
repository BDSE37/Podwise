# Podwise LLM æœå‹™

## æ¦‚è¿°

Podwise LLM æœå‹™æ˜¯ä¸€å€‹æ•´åˆå¤šç¨®èªè¨€æ¨¡å‹çš„çµ±ä¸€æœå‹™ï¼Œæ¡ç”¨ OOP æ¶æ§‹è¨­è¨ˆï¼Œæ”¯æ´ Qwen2.5-Taiwanã€Qwen3:8b ç­‰æ¨¡å‹ï¼Œä¸¦æ•´åˆ Langfuse è¿½è¹¤åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹è‰²

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
- **å¤šæ¨¡å‹æ”¯æ´** - æ”¯æ´ Qwen2.5-Taiwanã€Qwen3:8bã€DeepSeek ç­‰æ¨¡å‹
- **OOP æ¶æ§‹** - æ¡ç”¨ç‰©ä»¶å°å‘è¨­è¨ˆï¼Œæ˜“æ–¼æ“´å±•å’Œç¶­è­·
- **è‡ªå‹• Fallback** - ç•¶ä¸»è¦æ¨¡å‹å¤±æ•—æ™‚è‡ªå‹•åˆ‡æ›åˆ°å‚™ç”¨æ¨¡å‹
- **Langfuse è¿½è¹¤** - å®Œæ•´çš„è«‹æ±‚è¿½è¹¤å’Œç›£æ§
- **å‘é‡åµŒå…¥** - æ”¯æ´ BGE-M3 å‘é‡åµŒå…¥æ¨¡å‹

### ğŸ“Š æ¨¡å‹é…ç½®
- **Qwen2.5-Taiwan** (å„ªå…ˆç´š 1) - å°ç£å„ªåŒ–çš„ Qwen æ¨¡å‹
- **Qwen3:8b** (å„ªå…ˆç´š 2) - æ¨™æº– Qwen3 æ¨¡å‹
- **Qwen** (å„ªå…ˆç´š 3) - é€šç”¨ Qwen æ¨¡å‹ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
- **DeepSeek** (å„ªå…ˆç´š 4) - DeepSeek ç·¨ç¨‹æ¨¡å‹

## ç³»çµ±æ¶æ§‹

### ç›®éŒ„çµæ§‹
```
llm/
â”œâ”€â”€ main.py                    # çµ±ä¸€ä¸»ä»‹é¢ (FastAPI)
â”œâ”€â”€ core/                      # æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ ollama_llm.py          # Ollama æ•´åˆ
â”‚   â””â”€â”€ base_llm.py            # åŸºç¤ LLM é¡åˆ¥
â”œâ”€â”€ config/                    # é…ç½®æ¨¡çµ„
â”œâ”€â”€ requirements.txt           # ä¾è³´å¥—ä»¶
â””â”€â”€ Dockerfile                 # å®¹å™¨åŒ–é…ç½®
```

### é¡åˆ¥æ¶æ§‹
```
LLMService
â”œâ”€â”€ ModelConfig               # æ¨¡å‹é…ç½®
â”œâ”€â”€ GenerationRequest         # ç”Ÿæˆè«‹æ±‚
â”œâ”€â”€ GenerationResponse        # ç”Ÿæˆå›æ‡‰
â””â”€â”€ æ ¸å¿ƒæ–¹æ³•
    â”œâ”€â”€ generate_text()       # æ–‡å­—ç”Ÿæˆ
    â”œâ”€â”€ _select_best_model()  # æ¨¡å‹é¸æ“‡
    â”œâ”€â”€ _fallback_generation() # Fallback æ©Ÿåˆ¶
    â””â”€â”€ _calculate_confidence() # ä¿¡å¿ƒåº¦è¨ˆç®—
```

## API ç«¯é»

### å¥åº·æª¢æŸ¥
```http
GET /health
```

å›æ‡‰ï¼š
```json
{
  "status": "healthy",
  "models": [
    {
      "name": "qwen2.5-Taiwan",
      "model_id": "qwen2.5:7b",
      "enabled": true,
      "priority": 1
    }
  ],
  "embedding_models": ["bge-m3"]
}
```

### æ–‡å­—ç”Ÿæˆ
```http
POST /generate
Content-Type: application/json

{
  "prompt": "è«‹æ¨è–¦æŠ•è³‡ç†è²¡çš„ podcast",
  "model": "qwen2.5-Taiwan",
  "max_tokens": 2048,
  "temperature": 0.7,
  "system_prompt": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ podcast æ¨è–¦åŠ©æ‰‹",
  "user_id": "user123",
  "metadata": {
    "source": "llm_test"
  }
}
```

å›æ‡‰ï¼š
```json
{
  "text": "æ ¹æ“šæ‚¨çš„éœ€æ±‚ï¼Œæˆ‘æ¨è–¦ä»¥ä¸‹æŠ•è³‡ç†è²¡ podcast...",
  "model_used": "qwen2.5-Taiwan",
  "tokens_used": 150,
  "processing_time": 2.5,
  "confidence": 0.85,
  "trace_id": "trace_123"
}
```

### å‘é‡åµŒå…¥
```http
POST /embed
Content-Type: application/json

{
  "text": "æŠ•è³‡ç†è²¡ podcast",
  "model": "bge-m3"
}
```

å›æ‡‰ï¼š
```json
{
  "embedding": [[0.1, 0.2, 0.3, ...]]
}
```

### æ¨¡å‹åˆ—è¡¨
```http
GET /models
```

å›æ‡‰ï¼š
```json
{
  "llm_models": [
    {
      "name": "qwen2.5-Taiwan",
      "model_id": "qwen2.5:7b",
      "enabled": true,
      "priority": 1
    }
  ],
  "embedding_models": ["bge-m3"]
}
```

## é…ç½®èªªæ˜

### ç’°å¢ƒè®Šæ•¸
```bash
# Ollama é…ç½®
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# Langfuse è¿½è¹¤
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_HOST=http://localhost:3000

# å‘é‡æ¨¡å‹è·¯å¾‘
BGE_MODEL_PATH=/app/models/external/bge-m3

# æœå‹™é…ç½®
LLM_SERVICE_PORT=8004
LLM_SERVICE_HOST=0.0.0.0
```

### æ¨¡å‹é…ç½®
```python
# åœ¨ main.py ä¸­çš„ _load_model_configs æ–¹æ³•
self.models["qwen2.5-Taiwan"] = ModelConfig(
    model_name="Qwen2.5-Taiwan",
    model_id="qwen2.5:7b",
    host=os.getenv("OLLAMA_HOST", "localhost"),
    port=int(os.getenv("OLLAMA_PORT", "11434")),
    api_endpoint="/api/generate",
    max_tokens=2048,
    temperature=0.7,
    priority=1
)
```

## ä½¿ç”¨ç¯„ä¾‹

### Python å®¢æˆ¶ç«¯
```python
import httpx
import asyncio

async def test_llm_service():
    async with httpx.AsyncClient() as client:
        # å¥åº·æª¢æŸ¥
        response = await client.get("http://localhost:8004/health")
        print("å¥åº·ç‹€æ…‹:", response.json())
        
        # æ–‡å­—ç”Ÿæˆ
        response = await client.post(
            "http://localhost:8004/generate",
            json={
                "prompt": "æ¨è–¦æŠ•è³‡ç†è²¡çš„ podcast",
                "model": "qwen2.5-Taiwan",
                "max_tokens": 500
            }
        )
        print("ç”Ÿæˆçµæœ:", response.json())

# åŸ·è¡Œæ¸¬è©¦
asyncio.run(test_llm_service())
```

### JavaScript å®¢æˆ¶ç«¯
```javascript
// å¥åº·æª¢æŸ¥
const healthResponse = await fetch('http://localhost:8004/health');
const healthData = await healthResponse.json();
console.log('å¥åº·ç‹€æ…‹:', healthData);

// æ–‡å­—ç”Ÿæˆ
const generateResponse = await fetch('http://localhost:8004/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        prompt: 'æ¨è–¦æŠ•è³‡ç†è²¡çš„ podcast',
        model: 'qwen2.5-Taiwan',
        max_tokens: 500
    })
});
const generateData = await generateResponse.json();
console.log('ç”Ÿæˆçµæœ:', generateData);
```

## å•Ÿå‹•æŒ‡å—

### 1. å®‰è£ä¾è³´
```bash
cd backend/llm
pip install -r requirements.txt
```

### 2. å•Ÿå‹• Ollama æœå‹™
```bash
# ç¢ºä¿ Ollama å·²å®‰è£ä¸¦é‹è¡Œ
ollama serve

# æ‹‰å–æ¨¡å‹
ollama pull qwen2.5:7b
ollama pull qwen3:8b
```

### 3. å•Ÿå‹• LLM æœå‹™
```bash
python main.py
```

### 4. é©—è­‰æœå‹™
```bash
curl http://localhost:8004/health
```

## æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **æ¨¡å‹é€£æ¥å¤±æ•—**
   - æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹è¡Œ
   - ç¢ºèªæ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰
   - æª¢æŸ¥ç¶²è·¯é€£æ¥

2. **Langfuse è¿½è¹¤å¤±æ•—**
   - æª¢æŸ¥ Langfuse é…ç½®
   - ç¢ºèª API é‡‘é‘°æ˜¯å¦æ­£ç¢º

3. **å‘é‡æ¨¡å‹è¼‰å…¥å¤±æ•—**
   - æª¢æŸ¥æ¨¡å‹è·¯å¾‘æ˜¯å¦æ­£ç¢º
   - ç¢ºèªæ¨¡å‹æª”æ¡ˆæ˜¯å¦å®Œæ•´

### æ—¥èªŒæª¢æŸ¥
```bash
# æŸ¥çœ‹æœå‹™æ—¥èªŒ
tail -f logs/llm_service.log

# æŸ¥çœ‹ Ollama æ—¥èªŒ
ollama logs
```

## æ•´åˆæ¸¬è©¦

### èˆ‡ RAG Pipeline æ•´åˆ
```python
# åœ¨ RAG Pipeline ä¸­ä½¿ç”¨ LLM æœå‹™
import httpx

async def get_llm_response(prompt: str, model: str = "qwen2.5-Taiwan"):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8004/generate",
            json={
                "prompt": prompt,
                "model": model,
                "max_tokens": 2048
            }
        )
        return response.json()
```

### èˆ‡ TTS æœå‹™æ•´åˆ
```python
# ç”Ÿæˆæ–‡å­—å¾Œè½‰ç‚ºèªéŸ³
async def generate_and_speak(prompt: str):
    # 1. ç”Ÿæˆæ–‡å­—
    llm_response = await get_llm_response(prompt)
    text = llm_response["text"]
    
    # 2. è½‰ç‚ºèªéŸ³
    tts_response = await generate_tts(text)
    return tts_response
```

## æ•ˆèƒ½å„ªåŒ–

### 1. æ¨¡å‹å¿«å–
- ä½¿ç”¨ HTTP é€£æ¥æ± 
- å¯¦ä½œæ¨¡å‹å›æ‡‰å¿«å–
- å„ªåŒ–æ¨¡å‹è¼‰å…¥æ™‚é–“

### 2. ä¸¦è¡Œè™•ç†
- æ”¯æ´å¤šå€‹ä¸¦ç™¼è«‹æ±‚
- å¯¦ä½œè«‹æ±‚éšŠåˆ—
- å„ªåŒ–è³‡æºä½¿ç”¨

### 3. ç›£æ§æŒ‡æ¨™
- è«‹æ±‚å»¶é²ç›£æ§
- æ¨¡å‹ä½¿ç”¨ç‡çµ±è¨ˆ
- éŒ¯èª¤ç‡è¿½è¹¤

## æœªä¾†è¦åŠƒ

1. **æ¨¡å‹æ“´å±•**
   - æ”¯æ´æ›´å¤šèªè¨€æ¨¡å‹
   - å¯¦ä½œæ¨¡å‹è‡ªå‹•é¸æ“‡
   - æ”¯æ´æ¨¡å‹å¾®èª¿

2. **åŠŸèƒ½å¢å¼·**
   - æ”¯æ´ä¸²æµå›æ‡‰
   - å¯¦ä½œå°è©±è¨˜æ†¶
   - æ”¯æ´å¤šèªè¨€

3. **æ•ˆèƒ½æå‡**
   - å¯¦ä½œæ¨¡å‹é‡åŒ–
   - å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨
   - æ”¯æ´ GPU åŠ é€Ÿ

é€™å€‹ LLM æœå‹™ç¢ºä¿äº†èˆ‡å…¶ä»– Podwise æ¨¡çµ„çš„ç„¡ç¸«æ•´åˆï¼Œç‚ºæ•´å€‹ç³»çµ±æä¾›å¼·å¤§çš„èªè¨€æ¨¡å‹æ”¯æ´ã€‚ 