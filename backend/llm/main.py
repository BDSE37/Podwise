#!/usr/bin/env python3
"""
Podwise LLM ä¸»æ¨¡çµ„

æä¾›çµ±ä¸€çš„ LLM æœå‹™å…¥å£é»ï¼Œæ•´åˆæ‰€æœ‰èªè¨€æ¨¡å‹åŠŸèƒ½ï¼š
- å¤šæ¨¡å‹ç®¡ç† (Qwen2.5-Taiwan-7B-Instruct, Qwen3-8B)
- æ¨¡å‹è·¯ç”±å’Œè² è¼‰å‡è¡¡
- æ•ˆèƒ½ç›£æ§å’Œè¿½è¹¤
- éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
- é…ç½®ç®¡ç†

ç¬¦åˆ OOP åŸå‰‡å’Œ Google Clean Code æ¨™æº–
ä½œè€…: Podwise Team
ç‰ˆæœ¬: 2.0.0
"""

import logging
import os
import sys
import asyncio
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from pathlib import Path

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """LLM é…ç½®é¡åˆ¥"""
    enable_qwen_taiwan: bool = True
    enable_qwen3: bool = True
    enable_fallback: bool = True
    max_tokens: int = 2048
    temperature: float = 0.7
    timeout: int = 60
    retry_count: int = 3
    log_level: str = "INFO"


class LLMManager:
    """LLM ç®¡ç†å™¨ - çµ±ä¸€ç®¡ç†æ‰€æœ‰èªè¨€æ¨¡å‹æœå‹™"""
    
    def __init__(self, config: Optional[LLMConfig] = None):
        """åˆå§‹åŒ– LLM ç®¡ç†å™¨"""
        self.config = config or LLMConfig()
        self.models = {}
        self._initialize_models()
        logger.info("ğŸš€ LLM ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_models(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹"""
        try:
            # å°å…¥æ ¸å¿ƒæœå‹™
            from .core.ollama_llm import OllamaLLM
            from .core.base_llm import BaseLLM
            
            # åˆå§‹åŒ– Qwen2.5-Taiwan-7B-Instruct æ¨¡å‹
            if self.config.enable_qwen_taiwan:
                self.models['qwen_taiwan'] = OllamaLLM(
                    model_name="Qwen2.5-Taiwan-7B-Instruct",
                    host="localhost",
                    port=11434
                )
                logger.info("âœ… Qwen2.5-Taiwan-7B-Instruct æ¨¡å‹å·²åˆå§‹åŒ–")
            
            # åˆå§‹åŒ– Qwen3-8B æ¨¡å‹
            if self.config.enable_qwen3:
                self.models['qwen3'] = OllamaLLM(
                    model_name="Qwen3-8B",
                    host="localhost",
                    port=11434
                )
                logger.info("âœ… Qwen3-8B æ¨¡å‹å·²åˆå§‹åŒ–")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def get_model(self, model_name: str) -> Any:
        """ç²å–æŒ‡å®šæ¨¡å‹"""
        if model_name not in self.models:
            raise ValueError(f"æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨")
        return self.models[model_name]
    
    def get_qwen_taiwan(self) -> Any:
        """ç²å– Qwen2.5-Taiwan-7B-Instruct æ¨¡å‹"""
        return self.get_model('qwen_taiwan')
    
    def get_qwen3(self) -> Any:
        """ç²å– Qwen3-8B æ¨¡å‹"""
        return self.get_model('qwen3')
    
    async def generate_text(self, prompt: str, model_name: Optional[str] = None, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡å­—"""
        try:
            # é¸æ“‡æ¨¡å‹
            if model_name and model_name in self.models:
                model = self.get_model(model_name)
            else:
                # ä½¿ç”¨é è¨­æ¨¡å‹æˆ–è‡ªå‹•é¸æ“‡
                model = self.get_qwen_taiwan()
            
            # ç”Ÿæˆæ–‡å­—
            response = await model.generate(
                prompt=prompt,
                max_tokens=kwargs.get('max_tokens', self.config.max_tokens),
                temperature=kwargs.get('temperature', self.config.temperature),
                **kwargs
            )
            
            return {
                "success": True,
                "text": response,
                "model_used": model.model_name,
                "prompt": prompt
            }
            
        except Exception as e:
            logger.error(f"æ–‡å­—ç”Ÿæˆå¤±æ•—: {e}")
            
            # å˜—è©¦ fallback
            if self.config.enable_fallback and model_name != 'qwen3':
                try:
                    fallback_model = self.get_qwen3()
                    response = await fallback_model.generate(prompt, **kwargs)
                    return {
                        "success": True,
                        "text": response,
                        "model_used": fallback_model.model_name,
                        "fallback": True,
                        "original_error": str(e)
                    }
                except Exception as fallback_error:
                    logger.error(f"Fallback ä¹Ÿå¤±æ•—: {fallback_error}")
            
            return {
                "success": False,
                "error": str(e),
                "prompt": prompt
            }
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        health_status = {
            "status": "healthy",
            "models": {},
            "timestamp": str(Path(__file__).stat().st_mtime)
        }
        
        for model_name, model in self.models.items():
            try:
                # åŸºæœ¬å¯ç”¨æ€§æª¢æŸ¥
                if hasattr(model, 'health_check'):
                    model_health = model.health_check()
                else:
                    model_health = {"status": "available"}
                
                health_status["models"][model_name] = model_health
            except Exception as e:
                health_status["models"][model_name] = {
                    "status": "error",
                    "error": str(e)
                }
                health_status["status"] = "unhealthy"
        
        return health_status
    
    def get_service_info(self) -> Dict[str, Any]:
        """ç²å–æœå‹™è³‡è¨Š"""
        return {
            "module": "llm",
            "version": "2.0.0",
            "description": "Podwise èªè¨€æ¨¡å‹æœå‹™",
            "models": list(self.models.keys()),
            "config": {
                "enable_qwen_taiwan": self.config.enable_qwen_taiwan,
                "enable_qwen3": self.config.enable_qwen3,
                "enable_fallback": self.config.enable_fallback,
                "max_tokens": self.config.max_tokens,
                "temperature": self.config.temperature,
                "timeout": self.config.timeout,
                "retry_count": self.config.retry_count,
                "log_level": self.config.log_level
            }
        }


# å…¨åŸŸå¯¦ä¾‹
_llm_manager: Optional[LLMManager] = None


def get_llm_manager(config: Optional[LLMConfig] = None) -> LLMManager:
    """ç²å– LLM ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager(config)
    return _llm_manager


def initialize_llm(config: Optional[LLMConfig] = None) -> LLMManager:
    """åˆå§‹åŒ– LLM æ¨¡çµ„"""
    return get_llm_manager(config)


# ä¾¿æ·å‡½æ•¸
def get_qwen_taiwan():
    """ç²å– Qwen2.5-Taiwan-7B-Instruct æ¨¡å‹"""
    return get_llm_manager().get_qwen_taiwan()


def get_qwen3():
    """ç²å– Qwen3-8B æ¨¡å‹"""
    return get_llm_manager().get_qwen3()


async def generate_text(prompt: str, model_name: Optional[str] = None, **kwargs):
    """ç”Ÿæˆæ–‡å­—"""
    return await get_llm_manager().generate_text(prompt, model_name, **kwargs)


if __name__ == "__main__":
    # æ¸¬è©¦æ¨¡å¼
    async def test_llm():
        try:
            llm_manager = initialize_llm()
            print("âœ… LLM æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
            print(f"ğŸ“‹ æœå‹™è³‡è¨Š: {llm_manager.get_service_info()}")
            print(f"ğŸ¥ å¥åº·æª¢æŸ¥: {llm_manager.health_check()}")
            
            # æ¸¬è©¦æ–‡å­—ç”Ÿæˆ
            result = await llm_manager.generate_text("ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹è‡ªå·±")
            print(f"ğŸ¤– æ–‡å­—ç”Ÿæˆæ¸¬è©¦: {result}")
            
        except Exception as e:
            print(f"âŒ LLM æ¨¡çµ„åˆå§‹åŒ–å¤±æ•—: {e}")
            sys.exit(1)
    
    asyncio.run(test_llm()) 