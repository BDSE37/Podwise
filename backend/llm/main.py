#!/usr/bin/env python3
"""
Podwise LLM çµ±ä¸€æœå‹™æ¨¡çµ„

æ•´åˆæ‰€æœ‰èªè¨€æ¨¡å‹åŠŸèƒ½ï¼š
- Ollama æœ¬åœ°æ¨¡å‹ (Qwen2.5-Taiwan-7B-Instruct, Qwen3-8B)
- Hugging Face æ¨¡å‹
- æ¨¡å‹è·¯ç”±å’Œè² è¼‰å‡è¡¡
- æ•ˆèƒ½ç›£æ§å’Œè¿½è¹¤
- éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
- é…ç½®ç®¡ç†

ç¬¦åˆ OOP åŸå‰‡å’Œ Google Clean Code æ¨™æº–
ä½œè€…: Podwise Team
ç‰ˆæœ¬: 3.0.0
"""

import logging
import os
import sys
import asyncio
import aiohttp
import json
import time
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from pathlib import Path
from datetime import datetime
from contextlib import asynccontextmanager

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """LLM é…ç½®é¡åˆ¥"""
    # æ¨¡å‹å•Ÿç”¨é…ç½®
    enable_qwen_taiwan: bool = True
    enable_qwen3: bool = True
    enable_huggingface: bool = True
    enable_fallback: bool = True
    
    # ç”Ÿæˆåƒæ•¸
    max_tokens: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 40
    repeat_penalty: float = 1.1
    
    # é€£æ¥é…ç½®
    timeout: int = 60
    retry_count: int = 3
    
    # Ollama é…ç½®
    ollama_host: str = "http://localhost:11434"
    ollama_timeout: int = 120
    
    # Hugging Face é…ç½®
    hf_model_path: str = "./models"
    hf_device: str = "cpu"
    
    # æ—¥èªŒé…ç½®
    log_level: str = "INFO"


@dataclass
class GenerationRequest:
    """ç”Ÿæˆè«‹æ±‚è³‡æ–™çµæ§‹"""
    prompt: str
    system: Optional[str] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    repeat_penalty: Optional[float] = None
    stream: bool = False


@dataclass
class GenerationResponse:
    """ç”Ÿæˆå›æ‡‰è³‡æ–™çµæ§‹"""
    text: str
    model_used: str
    tokens_used: int
    processing_time: float
    success: bool = True
    error: Optional[str] = None


class OllamaLLM:
    """Ollama LLM æœå‹™é¡åˆ¥"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.http_session: Optional[aiohttp.ClientSession] = None
        self.available_models: List[str] = []
        self.model_mapping = {
            "qwen2.5:7b-taiwan": "qwen2.5-taiwan-7b-instruct:latest",
            "qwen3:8b": "qwen3-8b:latest",
            "qwen2.5-taiwan": "qwen2.5-taiwan-7b-instruct:latest",
            "qwen3": "qwen3-8b:latest",
            "qwen2.5-taiwan-7b-instruct": "qwen2.5-taiwan-7b-instruct:latest"
        }
        self.logger = logging.getLogger(f"{__name__}.OllamaLLM")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Ollama æœå‹™"""
        try:
            self.http_session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=self.config.ollama_timeout)
            )
            
            # æ¸¬è©¦é€£æ¥
            await self._test_connection()
            
            # æª¢æŸ¥å¯ç”¨æ¨¡å‹
            await self._check_available_models()
            
            self.logger.info(f"Ollama æœå‹™åˆå§‹åŒ–æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {self.available_models}")
            return True
            
        except Exception as e:
            self.logger.error(f"Ollama æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def _test_connection(self):
        """æ¸¬è©¦ Ollama é€£æ¥"""
        try:
            if self.http_session is None:
                raise Exception("HTTP session æœªåˆå§‹åŒ–")
            
            url = f"{self.config.ollama_host}/api/tags"
            async with self.http_session.get(url) as response:
                if response.status == 200:
                    self.logger.info("Ollama é€£æ¥æ­£å¸¸")
                else:
                    raise Exception(f"Ollama é€£æ¥ç•°å¸¸: {response.status}")
        except Exception as e:
            self.logger.error(f"Ollama é€£æ¥å¤±æ•—: {e}")
            raise
    
    async def _check_available_models(self):
        """æª¢æŸ¥å¯ç”¨æ¨¡å‹"""
        try:
            if self.http_session is None:
                self.available_models = []
                return
                
            url = f"{self.config.ollama_host}/api/tags"
            async with self.http_session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    self.available_models = [model['name'] for model in data.get('models', [])]
                else:
                    self.available_models = []
        except Exception as e:
            self.logger.error(f"æª¢æŸ¥å¯ç”¨æ¨¡å‹å¤±æ•—: {e}")
            self.available_models = []
    
    def _map_model_name(self, model_name: str) -> str:
        """æ˜ å°„æ¨¡å‹åç¨±"""
        return self.model_mapping.get(model_name, model_name)
    
    async def generate_text(self, request: GenerationRequest, model_name: str) -> GenerationResponse:
        """ç”Ÿæˆæ–‡å­—"""
        start_time = time.time()
        
        try:
            # æ˜ å°„æ¨¡å‹åç¨±
            mapped_model_name = self._map_model_name(model_name)
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            if mapped_model_name not in self.available_models:
                raise Exception(f"æ¨¡å‹ {mapped_model_name} ä¸å¯ç”¨ï¼Œå¯ç”¨æ¨¡å‹: {self.available_models}")
            
            # æº–å‚™è«‹æ±‚è³‡æ–™
            payload = {
                "model": mapped_model_name,
                "prompt": request.prompt,
                "stream": request.stream,
                "options": {
                    "num_predict": request.max_tokens or self.config.max_tokens,
                    "temperature": request.temperature or self.config.temperature,
                    "top_p": request.top_p or self.config.top_p,
                    "top_k": request.top_k or self.config.top_k,
                    "repeat_penalty": request.repeat_penalty or self.config.repeat_penalty
                }
            }
            
            if request.system:
                payload["system"] = request.system
            
            # ç™¼é€è«‹æ±‚
            if self.http_session is None:
                raise Exception("HTTP session æœªåˆå§‹åŒ–")
                
            url = f"{self.config.ollama_host}/api/generate"
            async with self.http_session.post(url, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    processing_time = time.time() - start_time
                    
                    return GenerationResponse(
                        text=data.get('response', ''),
                        model_used=mapped_model_name,
                        tokens_used=data.get('eval_count', 0),
                        processing_time=processing_time,
                        success=True
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"Ollama API éŒ¯èª¤: {response.status} - {error_text}")
                    
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"æ–‡å­—ç”Ÿæˆå¤±æ•—: {e}")
            
            return GenerationResponse(
                text="",
                model_used=model_name,
                tokens_used=0,
                processing_time=processing_time,
                success=False,
                error=str(e)
            )
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        try:
            if self.http_session is None:
                return {
                    "status": "error",
                    "error": "HTTP session æœªåˆå§‹åŒ–",
                    "available_models": [],
                    "connection": "failed"
                }
                
            url = f"{self.config.ollama_host}/api/tags"
            async with self.http_session.get(url) as response:
                return {
                    "status": "healthy" if response.status == 200 else "unhealthy",
                    "available_models": self.available_models,
                    "connection": "ok" if response.status == 200 else "failed"
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "available_models": [],
                "connection": "failed"
            }
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.http_session:
            await self.http_session.close()


class HuggingFaceLLM:
    """Hugging Face LLM æœå‹™é¡åˆ¥"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.models: Dict[str, Any] = {}
        self.logger = logging.getLogger(f"{__name__}.HuggingFaceLLM")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Hugging Face æœå‹™"""
        try:
            # é€™è£¡å¯ä»¥æ·»åŠ  Hugging Face æ¨¡å‹åˆå§‹åŒ–é‚è¼¯
            self.logger.info("Hugging Face æœå‹™åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            self.logger.error(f"Hugging Face æœå‹™åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def _load_models(self):
        """è¼‰å…¥æ¨¡å‹"""
        # å¯¦ç¾æ¨¡å‹è¼‰å…¥é‚è¼¯
        pass
    
    async def generate_text(self, request: GenerationRequest, model_name: str) -> GenerationResponse:
        """ç”Ÿæˆæ–‡å­—"""
        start_time = time.time()
        
        try:
            # é€™è£¡å¯¦ç¾ Hugging Face æ¨¡å‹æ¨ç†
            # ç›®å‰è¿”å›é è¨­å›æ‡‰
            processing_time = time.time() - start_time
            
            return GenerationResponse(
                text=f"[Hugging Face] {request.prompt} (æ¨¡å‹: {model_name})",
                model_used=model_name,
                tokens_used=len(request.prompt.split()),
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"Hugging Face æ–‡å­—ç”Ÿæˆå¤±æ•—: {e}")
            
            return GenerationResponse(
                text="",
                model_used=model_name,
                tokens_used=0,
                processing_time=processing_time,
                success=False,
                error=str(e)
            )
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        return {
            "status": "healthy",
            "available_models": list(self.models.keys()),
            "connection": "ok"
        }


class LLMManager:
    """LLM ç®¡ç†å™¨é¡åˆ¥"""
    
    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        self.ollama_llm: Optional[OllamaLLM] = None
        self.hf_llm: Optional[HuggingFaceLLM] = None
        self.logger = logging.getLogger(f"{__name__}.LLMManager")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– LLM ç®¡ç†å™¨"""
        try:
            # åˆå§‹åŒ– Ollama æœå‹™
            if self.config.enable_qwen_taiwan or self.config.enable_qwen3:
                self.ollama_llm = OllamaLLM(self.config)
                if not await self.ollama_llm.initialize():
                    self.logger.warning("Ollama æœå‹™åˆå§‹åŒ–å¤±æ•—")
            
            # åˆå§‹åŒ– Hugging Face æœå‹™
            if self.config.enable_huggingface:
                self.hf_llm = HuggingFaceLLM(self.config)
                if not await self.hf_llm.initialize():
                    self.logger.warning("Hugging Face æœå‹™åˆå§‹åŒ–å¤±æ•—")
            
            self.logger.info("ğŸš€ LLM ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"LLM ç®¡ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    
    async def generate_text(self, request: GenerationRequest, model_name: Optional[str] = None) -> GenerationResponse:
        """ç”Ÿæˆæ–‡å­—"""
        # å¦‚æœæ²’æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é è¨­æ¨¡å‹
        if not model_name:
            if self.config.enable_qwen_taiwan:
                model_name = "qwen2.5:7b-taiwan"
            elif self.config.enable_qwen3:
                model_name = "qwen3:8b"
            else:
                return GenerationResponse(
                    text="",
                    model_used="",
                    tokens_used=0,
                    processing_time=0,
                    success=False,
                    error="æ²’æœ‰å¯ç”¨çš„æ¨¡å‹"
                )
        
        # æ ¹æ“šæ¨¡å‹åç¨±é¸æ“‡æœå‹™
        if model_name in ["qwen2.5:7b-taiwan", "qwen3:8b", "qwen2.5-taiwan", "qwen3", "qwen2.5-taiwan-7b-instruct", "qwen2.5-taiwan-7b-instruct:latest"]:
            if self.ollama_llm:
                return await self.ollama_llm.generate_text(request, model_name)
            else:
                return GenerationResponse(
                    text="",
                    model_used=model_name,
                    tokens_used=0,
                    processing_time=0,
                    success=False,
                    error="Ollama æœå‹™ä¸å¯ç”¨"
                )
        else:
            # å˜—è©¦ Hugging Face æœå‹™
            if self.hf_llm:
                return await self.hf_llm.generate_text(request, model_name)
            else:
                return GenerationResponse(
                    text="",
                    model_used=model_name,
                    tokens_used=0,
                    processing_time=0,
                    success=False,
                    error="Hugging Face æœå‹™ä¸å¯ç”¨"
                )
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        health_status = {
            "status": "healthy",
            "services": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # æª¢æŸ¥ Ollama æœå‹™
        if self.ollama_llm:
            try:
                ollama_health = await self.ollama_llm.health_check()
                health_status["services"]["ollama"] = ollama_health
            except Exception as e:
                health_status["services"]["ollama"] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # æª¢æŸ¥ Hugging Face æœå‹™
        if self.hf_llm:
            try:
                hf_health = await self.hf_llm.health_check()
                health_status["services"]["huggingface"] = hf_health
            except Exception as e:
                health_status["services"]["huggingface"] = {
                    "status": "error",
                    "error": str(e)
                }
        
        return health_status
    
    async def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.ollama_llm:
            await self.ollama_llm.cleanup()


# å…¨å±€ LLM ç®¡ç†å™¨å¯¦ä¾‹
_llm_manager: Optional[LLMManager] = None


def get_llm_manager(config: Optional[LLMConfig] = None) -> LLMManager:
    """ç²å– LLM ç®¡ç†å™¨å¯¦ä¾‹"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager(config)
    return _llm_manager


async def initialize_llm(config: Optional[LLMConfig] = None) -> LLMManager:
    """åˆå§‹åŒ– LLM æœå‹™"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager(config)
        await _llm_manager.initialize()
    return _llm_manager


async def generate_text(prompt: str, model_name: Optional[str] = None, **kwargs) -> Dict[str, Any]:
    """ç”Ÿæˆæ–‡å­—çš„ä¾¿æ·å‡½æ•¸"""
    manager = get_llm_manager()
    request = GenerationRequest(
        prompt=prompt,
        system=kwargs.get('system'),
        max_tokens=kwargs.get('max_tokens'),
        temperature=kwargs.get('temperature'),
        top_p=kwargs.get('top_p'),
        top_k=kwargs.get('top_k'),
        repeat_penalty=kwargs.get('repeat_penalty')
    )
    
    response = await manager.generate_text(request, model_name)
    
    return {
        "success": response.success,
        "text": response.text,
        "model_used": response.model_used,
        "tokens_used": response.tokens_used,
        "processing_time": response.processing_time,
        "error": response.error
    }


if __name__ == "__main__":
    async def test_llm():
        """æ¸¬è©¦ LLM æœå‹™"""
        config = LLMConfig(
            enable_qwen_taiwan=True,
            enable_qwen3=True,
            enable_huggingface=False,
            ollama_host="http://localhost:11434"
        )
        
        manager = await initialize_llm(config)
        
        # æ¸¬è©¦æ–‡å­—ç”Ÿæˆ
        test_prompt = "è«‹ç”¨ç¹é«”ä¸­æ–‡ä»‹ç´¹å°ç£çš„ç§‘æŠ€ç™¼å±•"
        result = await generate_text(test_prompt, "qwen2.5:7b-taiwan")
        
        print(f"æ¸¬è©¦çµæœ: {result}")
        
        # å¥åº·æª¢æŸ¥
        health = await manager.health_check()
        print(f"å¥åº·æª¢æŸ¥: {health}")
    
    asyncio.run(test_llm())


# FastAPI æ‡‰ç”¨ç¨‹å¼
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, Any

# å…¨å±€ LLM ç®¡ç†å™¨
llm_manager: Optional[LLMManager] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸç®¡ç†"""
    global llm_manager
    # å•Ÿå‹•æ™‚åˆå§‹åŒ–
    try:
        config = LLMConfig(
            enable_qwen_taiwan=True,
            enable_qwen3=True,
            enable_huggingface=False,
            ollama_host=os.getenv("OLLAMA_HOST", "http://ollama:11434")
        )
        llm_manager = await initialize_llm(config)
        logger.info("LLM æœå‹™å•Ÿå‹•æˆåŠŸ")
    except Exception as e:
        logger.error(f"LLM æœå‹™å•Ÿå‹•å¤±æ•—: {e}")
        raise
    
    yield
    
    # é—œé–‰æ™‚æ¸…ç†
    if llm_manager:
        await llm_manager.cleanup()
        logger.info("LLM æœå‹™å·²é—œé–‰")

app = FastAPI(
    title="Podwise LLM Service",
    description="çµ±ä¸€çš„èªè¨€æ¨¡å‹æœå‹™",
    version="3.0.0",
    lifespan=lifespan
)

# CORS è¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è«‹æ±‚æ¨¡å‹
class TextGenerationRequest(BaseModel):
    prompt: str
    model_name: Optional[str] = None
    system: Optional[str] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    repeat_penalty: Optional[float] = None

# å›æ‡‰æ¨¡å‹
class TextGenerationResponse(BaseModel):
    success: bool
    text: str
    model_used: str
    tokens_used: int
    processing_time: float
    error: Optional[str] = None

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    global llm_manager
    if llm_manager is None:
        raise HTTPException(status_code=503, detail="LLM æœå‹™æœªåˆå§‹åŒ–")
    
    health = await llm_manager.health_check()
    return health

@app.post("/generate", response_model=TextGenerationResponse)
async def generate_text_endpoint(request: TextGenerationRequest):
    """æ–‡å­—ç”Ÿæˆç«¯é»"""
    global llm_manager
    if llm_manager is None:
        raise HTTPException(status_code=503, detail="LLM æœå‹™æœªåˆå§‹åŒ–")
    
    try:
        generation_request = GenerationRequest(
            prompt=request.prompt,
            system=request.system,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            top_k=request.top_k,
            repeat_penalty=request.repeat_penalty
        )
        
        response = await llm_manager.generate_text(generation_request, request.model_name)
        
        return TextGenerationResponse(
            success=response.success,
            text=response.text,
            model_used=response.model_used,
            tokens_used=response.tokens_used,
            processing_time=response.processing_time,
            error=response.error
        )
        
    except Exception as e:
        logger.error(f"æ–‡å­—ç”Ÿæˆç«¯é»éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models")
async def get_available_models():
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    global llm_manager
    if llm_manager is None:
        raise HTTPException(status_code=503, detail="LLM æœå‹™æœªåˆå§‹åŒ–")
    
    models = []
    if llm_manager.ollama_llm:
        models.extend(llm_manager.ollama_llm.available_models)
    if llm_manager.hf_llm:
        models.extend(list(llm_manager.hf_llm.models.keys()))
    
    return {
        "models": models,
        "mapping": llm_manager.ollama_llm.model_mapping if llm_manager.ollama_llm else {}
    }

@app.get("/")
async def root():
    """æ ¹ç«¯é»"""
    return {
        "service": "Podwise LLM Service",
        "version": "3.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "generate": "/generate",
            "models": "/models"
        }
    } 