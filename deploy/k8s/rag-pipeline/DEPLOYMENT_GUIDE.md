# Podwise RAG Pipeline K8s éƒ¨ç½²æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•åœ¨ Kubernetes é›†ç¾¤ä¸Šéƒ¨ç½² Podwise RAG Pipeline æœå‹™ï¼ŒåŒ…å«æ–°çš„ LLM å‚™æ´æ©Ÿåˆ¶ã€‚

## ğŸ“‹ å‰ç½®éœ€æ±‚

- Kubernetes é›†ç¾¤
- kubectl å·¥å…·
- Docker æˆ– Podman
- è¨ªå•å®¹å™¨è¨»å†Šè¡¨çš„æ¬Šé™
- OpenAI API Key (å¯é¸ï¼Œç”¨æ–¼å‚™æ´)

## ğŸš€ éƒ¨ç½²æ–¹å¼

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ K8s éƒ¨ç½²è…³æœ¬ (æ¨è–¦)

```bash
# 1. é€²å…¥éƒ¨ç½²ç›®éŒ„
cd deploy/k8s/rag-pipeline

# 2. è¨­ç½®ç’°å¢ƒè®Šæ•¸ (å¯é¸)
export OPENAI_API_KEY="your_openai_api_key_here"
export LANGFUSE_PUBLIC_KEY="your_langfuse_public_key"
export LANGFUSE_SECRET_KEY="your_langfuse_secret_key"

# 3. åŸ·è¡Œéƒ¨ç½²è…³æœ¬
chmod +x build-and-deploy-k8s.sh
./build-and-deploy-k8s.sh
```

### æ–¹å¼äºŒï¼šä½¿ç”¨ Worker2 éƒ¨ç½²è…³æœ¬

```bash
# 1. é€²å…¥éƒ¨ç½²ç›®éŒ„
cd deploy/k8s/rag-pipeline

# 2. è¨­ç½®ç’°å¢ƒè®Šæ•¸ (å¯é¸)
export OPENAI_API_KEY="your_openai_api_key_here"

# 3. åŸ·è¡Œéƒ¨ç½²è…³æœ¬
chmod +x deploy-rag-on-worker2.sh
./deploy-rag-on-worker2.sh
```

### æ–¹å¼ä¸‰ï¼šæ‰‹å‹•éƒ¨ç½²

```bash
# 1. å‰µå»ºå‘½åç©ºé–“
kubectl create namespace podwise

# 2. å‰µå»º Secrets (å¯é¸)
kubectl create secret generic openai-secrets \
    --from-literal=OPENAI_API_KEY="your_openai_api_key" \
    --namespace=podwise

# 3. éƒ¨ç½²æœå‹™
kubectl apply -f rag-pipeline-deployment.yaml

# 4. æª¢æŸ¥éƒ¨ç½²ç‹€æ…‹
kubectl rollout status deployment/rag-pipeline-service -n podwise
```

## ğŸ¤– LLM å‚™æ´æ©Ÿåˆ¶

### æ¨¡å‹å„ªå…ˆç´šé †åº

1. **Qwen2.5-Taiwan** (ç¬¬ä¸€å„ªå…ˆ)
   - å°ç£å„ªåŒ–ç‰ˆæœ¬
   - é‡å°ç¹é«”ä¸­æ–‡å„ªåŒ–
   - æ¨¡å‹åç¨±: `weiren119/Qwen2.5-Taiwan-8B-Instruct`

2. **Qwen3:8b** (ç¬¬äºŒå„ªå…ˆ)
   - æ¨™æº– Qwen3 æ¨¡å‹
   - ä¸»è¦å‚™ç”¨æ¨¡å‹
   - æ¨¡å‹åç¨±: `Qwen/Qwen2.5-8B-Instruct`

3. **OpenAI GPT-3.5** (å‚™æ´)
   - éœ€è¦ OpenAI API Key
   - ç•¶å‰é¢æ¨¡å‹ä¸å¯ç”¨æ™‚å•Ÿç”¨
   - æ¨¡å‹åç¨±: `gpt-3.5-turbo`

4. **OpenAI GPT-4** (æœ€å¾Œå‚™æ´)
   - æœ€é«˜å“è³ªå‚™æ´
   - æˆæœ¬è¼ƒé«˜
   - æ¨¡å‹åç¨±: `gpt-4`

### é…ç½® OpenAI å‚™æ´

1. **è¨­ç½®ç’°å¢ƒè®Šæ•¸**:
   ```bash
   export OPENAI_API_KEY="your_openai_api_key_here"
   ```

2. **å‰µå»º K8s Secret**:
   ```bash
   kubectl create secret generic openai-secrets \
       --from-literal=OPENAI_API_KEY="your_openai_api_key" \
       --namespace=podwise
   ```

3. **é©—è­‰é…ç½®**:
   ```bash
   curl http://worker2:30806/api/v1/llm-status
   ```

## âœ… é©—è­‰éƒ¨ç½²

### æª¢æŸ¥æœå‹™ç‹€æ…‹

```bash
# æª¢æŸ¥ Pod ç‹€æ…‹
kubectl get pods -n podwise -l app=rag-pipeline-service

# æª¢æŸ¥æœå‹™ç‹€æ…‹
kubectl get svc -n podwise -l app=rag-pipeline-service

# æª¢æŸ¥ç¯€é»åˆ†é…
kubectl get pods -n podwise -l app=rag-pipeline-service -o wide
```

### æ¸¬è©¦æœå‹™é€£ç·š

```bash
# ç²å–æœå‹™ç«¯é»
NODE_PORT=$(kubectl get svc rag-pipeline-service -n podwise -o jsonpath='{.spec.ports[0].nodePort}')

# æ¸¬è©¦å¥åº·æª¢æŸ¥
curl http://worker2:$NODE_PORT/health

# æ¸¬è©¦ LLM ç‹€æ…‹
curl http://worker2:$NODE_PORT/api/v1/llm-status

# æ¸¬è©¦ API æ–‡æª”
curl http://worker2:$NODE_PORT/docs
```

### æ¸¬è©¦ LLM å‚™æ´æ©Ÿåˆ¶

```bash
# é€²å…¥ Pod åŸ·è¡Œæ¸¬è©¦
kubectl exec -it deployment/rag-pipeline-service -n podwise -- python3 -c "
from backend.rag_pipeline.test_llm_fallback import LLMFallbackTest
import asyncio

async def test():
    test_suite = LLMFallbackTest()
    results = test_suite.run_all_tests()
    print(f'æ¸¬è©¦çµæœ: {results[\"summary\"]}')

asyncio.run(test())
"
```

## âš™ï¸ é…ç½®èªªæ˜

### ç’°å¢ƒè®Šæ•¸

| è®Šæ•¸åç¨± | èªªæ˜ | é è¨­å€¼ |
|---------|------|--------|
| `PYTHONPATH` | Python è·¯å¾‘ | `/app` |
| `PYTHONUNBUFFERED` | Python è¼¸å‡ºç·©è¡ | `1` |
| `APP_ENV` | æ‡‰ç”¨ç’°å¢ƒ | `production` |
| `DEBUG` | é™¤éŒ¯æ¨¡å¼ | `false` |
| `LOG_LEVEL` | æ—¥èªŒç­‰ç´š | `INFO` |
| `OLLAMA_HOST` | Ollama æœå‹™åœ°å€ | `http://worker1:11434` |
| `OLLAMA_MODEL` | Ollama æ¨¡å‹åç¨± | `qwen2.5:8b` |
| `OPENAI_API_KEY` | OpenAI API Key | (å¯é¸) |

### è³‡æºé™åˆ¶

é è¨­è³‡æºé…ç½®ï¼š

- **CPU**: 2-4 cores
- **è¨˜æ†¶é«”**: 4-8 GiB
- **ç¯€é»é¸æ“‡å™¨**: worker2

### æŒä¹…åŒ–å­˜å„²

æœå‹™ä½¿ç”¨ä»¥ä¸‹ PVCï¼š

- `rag-data-pvc`: æ•¸æ“šå­˜å„²
- `rag-models-pvc`: æ¨¡å‹å­˜å„²
- `rag-cache-pvc`: å¿«å–å­˜å„²

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **Pod ç„¡æ³•å•Ÿå‹•**
   ```bash
   # æª¢æŸ¥ Pod äº‹ä»¶
   kubectl describe pod <pod-name> -n podwise
   
   # æŸ¥çœ‹ Pod æ—¥èªŒ
   kubectl logs <pod-name> -n podwise
   ```

2. **LLM æœå‹™é€£ç·šå¤±æ•—**
   ```bash
   # æª¢æŸ¥ Ollama æœå‹™
   curl http://worker1:11434/api/tags
   
   # æª¢æŸ¥æ¨¡å‹å¯ç”¨æ€§
   kubectl exec -it deployment/rag-pipeline-service -n podwise -- python3 -c "
   from backend.rag_pipeline.core.qwen3_llm_manager import get_qwen3_llm_manager
   manager = get_qwen3_llm_manager()
   print(f'å¯ç”¨æ¨¡å‹: {manager.get_available_models()}')
   "
   ```

3. **OpenAI å‚™æ´ä¸å·¥ä½œ**
   ```bash
   # æª¢æŸ¥ OpenAI Secret
   kubectl get secret openai-secrets -n podwise -o yaml
   
   # æ¸¬è©¦ OpenAI é…ç½®
   kubectl exec -it deployment/rag-pipeline-service -n podwise -- python3 -c "
   from backend.rag_pipeline.config.integrated_config import get_config
   config = get_config()
   print(f'OpenAI é…ç½®: {bool(config.api.openai_api_key)}')
   "
   ```

4. **CrewAI æ¨¡çµ„ç¼ºå¤±**
   ```bash
   # æª¢æŸ¥ CrewAI å®‰è£
   kubectl exec -it deployment/rag-pipeline-service -n podwise -- python3 -c "
   import crewai
   print(f'CrewAI ç‰ˆæœ¬: {crewai.__version__}')
   "
   ```

### æ—¥èªŒæŸ¥çœ‹

```bash
# æŸ¥çœ‹å¯¦æ™‚æ—¥èªŒ
kubectl logs -f deployment/rag-pipeline-service -n podwise

# æŸ¥çœ‹ç‰¹å®šæ™‚é–“çš„æ—¥èªŒ
kubectl logs deployment/rag-pipeline-service -n podwise --since=1h

# æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒ
kubectl logs deployment/rag-pipeline-service -n podwise | grep ERROR
```

## ğŸ”„ ç¶­è­·æ“ä½œ

### æ›´æ–°éƒ¨ç½²

```bash
# æ›´æ–°æ˜ åƒ
kubectl set image deployment/rag-pipeline-service rag-pipeline-service=192.168.32.38:5000/podwise-rag-pipeline:latest -n podwise

# é‡å•Ÿéƒ¨ç½²
kubectl rollout restart deployment/rag-pipeline-service -n podwise

# æŸ¥çœ‹æ›´æ–°ç‹€æ…‹
kubectl rollout status deployment/rag-pipeline-service -n podwise
```

### æ“´å±•æœå‹™

```bash
# æ“´å±•å‰¯æœ¬æ•¸
kubectl scale deployment/rag-pipeline-service --replicas=3 -n podwise

# æŸ¥çœ‹æ“´å±•ç‹€æ…‹
kubectl get pods -n podwise -l app=rag-pipeline-service
```

### å‚™ä»½å’Œæ¢å¾©

```bash
# å‚™ä»½é…ç½®
kubectl get deployment rag-pipeline-service -n podwise -o yaml > backup.yaml

# æ¢å¾©é…ç½®
kubectl apply -f backup.yaml
```

## ğŸ“Š ç›£æ§

### å¥åº·æª¢æŸ¥

```bash
# è‡ªå‹•å¥åº·æª¢æŸ¥
kubectl get pods -n podwise -l app=rag-pipeline-service

# æ‰‹å‹•å¥åº·æª¢æŸ¥
curl http://worker2:30806/health
```

### æ€§èƒ½ç›£æ§

```bash
# æŸ¥çœ‹è³‡æºä½¿ç”¨æƒ…æ³
kubectl top pods -n podwise

# æŸ¥çœ‹ç¯€é»è³‡æº
kubectl top nodes
```

### LLM ç›£æ§

```bash
# æŸ¥çœ‹ LLM ç‹€æ…‹
curl http://worker2:30806/api/v1/llm-status

# æŸ¥çœ‹æ¨¡å‹æŒ‡æ¨™
kubectl exec -it deployment/rag-pipeline-service -n podwise -- python3 -c "
from backend.rag_pipeline.core.qwen3_llm_manager import get_qwen3_llm_manager
manager = get_qwen3_llm_manager()
print(manager.get_metrics_summary())
"
```

## ğŸ”’ å®‰å…¨æ³¨æ„äº‹é …

1. **API Key ä¿è­·**: ç¢ºä¿ OpenAI API Key å®‰å…¨å­˜å„²
2. **ç¶²è·¯å®‰å…¨**: é™åˆ¶æœå‹™è¨ªå•æ¬Šé™
3. **è³‡æºé™åˆ¶**: è¨­ç½®é©ç•¶çš„è³‡æºé™åˆ¶é˜²æ­¢è³‡æºæ¿«ç”¨
4. **æ—¥èªŒç®¡ç†**: å®šæœŸæ¸…ç†æ—¥èªŒæª”æ¡ˆ

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

éƒ¨ç½²æˆåŠŸå¾Œæ‡‰è©²çœ‹åˆ°ï¼š

- âœ… Pod ç‹€æ…‹ç‚º `Running`
- âœ… å¥åº·æª¢æŸ¥ç«¯é»å›æ‡‰æ­£å¸¸
- âœ… API æŸ¥è©¢ç«¯é»æ­£å¸¸å·¥ä½œ
- âœ… LLM å‚™æ´æ©Ÿåˆ¶æ­£å¸¸é‹ä½œ
- âœ… å°ç£æ¨¡å‹ç‚ºç¬¬ä¸€å„ªå…ˆ
- âœ… OpenAI å‚™æ´é…ç½®æ­£ç¢º (å¦‚æœè¨­ç½®)
- âœ… podri-chat èƒ½æˆåŠŸç™¼é€æŸ¥è©¢åˆ° RAG Pipeline

## ğŸ”„ é‡æ–°éƒ¨ç½²

å¦‚æœéœ€è¦é‡æ–°éƒ¨ç½²ï¼š

```bash
# åˆªé™¤ç¾æœ‰ Pod
kubectl delete pod -n podwise -l app=rag-pipeline-service --force --grace-period=0

# é‡æ–°åŸ·è¡Œå»ºç½®è…³æœ¬
./build-and-deploy-k8s.sh
``` 